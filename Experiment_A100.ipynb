{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "runtime_attributes": {
        "runtime_version": "2025.10"
      }
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Single Test"
      ],
      "metadata": {
        "id": "y-6XyOu-y4bJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers accelerate pillow torch\n",
        "!pip install -q flash-attn --no-build-isolation"
      ],
      "metadata": {
        "id": "pCg-P6EKcHuQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "051b9554-0f94-4b22-cb42-b8a336e7549e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/8.4 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/8.4 MB\u001b[0m \u001b[31m107.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m147.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m109.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for flash-attn (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llmcompressor datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I6VrjHIVYFyZ",
        "outputId": "3c263a78-69c1-4fb5-89ae-0d07de9e59fc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llmcompressor\n",
            "  Downloading llmcompressor-0.8.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Collecting loguru<=0.7.3,>=0.7.2 (from llmcompressor)\n",
            "  Downloading loguru-0.7.3-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: pyyaml<=6.0.3,>=6.0.1 in /usr/local/lib/python3.12/dist-packages (from llmcompressor) (6.0.3)\n",
            "Requirement already satisfied: numpy<=2.3.3,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from llmcompressor) (2.0.2)\n",
            "Requirement already satisfied: requests<=2.32.5,>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from llmcompressor) (2.32.4)\n",
            "Requirement already satisfied: tqdm<=4.67.1,>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from llmcompressor) (4.67.1)\n",
            "Requirement already satisfied: torch<=2.8.0,>=2.7.0 in /usr/local/lib/python3.12/dist-packages (from llmcompressor) (2.8.0+cu126)\n",
            "Collecting transformers<=4.56.2,>=4.53.0 (from llmcompressor)\n",
            "  Downloading transformers-4.56.2-py3-none-any.whl.metadata (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.1/40.1 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: accelerate<=1.10.1,>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from llmcompressor) (1.10.1)\n",
            "Requirement already satisfied: nvidia-ml-py<=13.580.82,>=12.560.30 in /usr/local/lib/python3.12/dist-packages (from llmcompressor) (12.575.51)\n",
            "Requirement already satisfied: pillow<=11.3.0,>=10.4.0 in /usr/local/lib/python3.12/dist-packages (from llmcompressor) (11.3.0)\n",
            "Collecting compressed-tensors==0.12.2 (from llmcompressor)\n",
            "  Downloading compressed_tensors-0.12.2-py3-none-any.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.12/dist-packages (from compressed-tensors==0.12.2->llmcompressor) (2.11.10)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.35.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate<=1.10.1,>=1.6.0->llmcompressor) (5.9.5)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate<=1.10.1,>=1.6.0->llmcompressor) (0.6.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.10)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<=2.32.5,>=2.32.2->llmcompressor) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<=2.32.5,>=2.32.2->llmcompressor) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<=2.32.5,>=2.32.2->llmcompressor) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<=2.32.5,>=2.32.2->llmcompressor) (2025.10.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (3.4.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<=4.56.2,>=4.53.0->llmcompressor) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<=4.56.2,>=4.53.0->llmcompressor) (0.22.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0->compressed-tensors==0.12.2->llmcompressor) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0->compressed-tensors==0.12.2->llmcompressor) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0->compressed-tensors==0.12.2->llmcompressor) (0.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<=2.8.0,>=2.7.0->llmcompressor) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<=2.8.0,>=2.7.0->llmcompressor) (3.0.3)\n",
            "Downloading llmcompressor-0.8.1-py3-none-any.whl (273 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m273.2/273.2 kB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading compressed_tensors-0.12.2-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.0/183.0 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading loguru-0.7.3-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformers-4.56.2-py3-none-any.whl (11.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m57.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: loguru, transformers, compressed-tensors, llmcompressor\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.57.0\n",
            "    Uninstalling transformers-4.57.0:\n",
            "      Successfully uninstalled transformers-4.57.0\n",
            "Successfully installed compressed-tensors-0.12.2 llmcompressor-0.8.1 loguru-0.7.3 transformers-4.56.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U bitsandbytes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_tuG642YIRs",
        "outputId": "53935963-acf9-47ac-c46a-401e06417b4f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: torch<3,>=2.3 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.8.0+cu126)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (25.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.3->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.3)\n",
            "Downloading bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl (59.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m51.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bitsandbytes\n",
            "Successfully installed bitsandbytes-0.48.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6RPnK-KkbI1D",
        "outputId": "91ad536e-8b74-4e0c-e5ab-ea781bdc3e65"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.12.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python test_baseline.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZV_YISLiWAr0",
        "outputId": "0abef367-578e-47ee-fbb4-8ad14aa6a005"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-06 03:34:35.760722: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-06 03:34:35.779437: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764992075.801849    1257 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764992075.809413    1257 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764992075.836373    1257 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764992075.836427    1257 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764992075.836431    1257 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764992075.836436    1257 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-06 03:34:35.844242: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Loading sample image...\n",
            "Image size: (1000, 667)\n",
            "\n",
            "================================================================================\n",
            "Baseline_without_kv_cache Test for LLaVA\n",
            "================================================================================\n",
            "processor_config.json: 100% 173/173 [00:00<00:00, 1.57MB/s]\n",
            "chat_template.json: 100% 701/701 [00:00<00:00, 7.03MB/s]\n",
            "chat_template.jinja: 100% 674/674 [00:00<00:00, 6.59MB/s]\n",
            "preprocessor_config.json: 100% 505/505 [00:00<00:00, 5.69MB/s]\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "tokenizer_config.json: 1.45kB [00:00, 8.54MB/s]\n",
            "tokenizer.model: 100% 500k/500k [00:02<00:00, 227kB/s]\n",
            "tokenizer.json: 3.62MB [00:00, 220MB/s]\n",
            "added_tokens.json: 100% 41.0/41.0 [00:00<00:00, 486kB/s]\n",
            "special_tokens_map.json: 100% 552/552 [00:00<00:00, 5.94MB/s]\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "config.json: 100% 950/950 [00:00<00:00, 10.8MB/s]\n",
            "model.safetensors.index.json: 70.1kB [00:00, 154MB/s]\n",
            "Fetching 3 files:   0% 0/3 [00:00<?, ?it/s]\n",
            "model-00003-of-00003.safetensors:   0% 0.00/4.18G [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:   0% 0.00/4.96G [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:   0% 0.00/4.99G [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:   0% 546k/4.18G [00:01<3:21:41, 345kB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:   0% 599k/4.96G [00:02<4:37:59, 297kB/s]\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:   0% 1.74M/4.18G [00:02<1:21:03, 858kB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:   2% 68.8M/4.18G [00:03<02:16, 30.1MB/s] \u001b[A\n",
            "model-00003-of-00003.safetensors:   6% 270M/4.18G [00:03<00:30, 130MB/s]  \u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:   1% 67.6M/4.96G [00:03<03:41, 22.1MB/s]\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  13% 538M/4.18G [00:03<00:12, 291MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:   4% 202M/4.96G [00:03<01:01, 77.5MB/s] \u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  19% 807M/4.18G [00:04<00:08, 403MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  24% 1.01G/4.18G [00:04<00:06, 491MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:   7% 336M/4.96G [00:04<00:43, 106MB/s] \u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  29% 1.21G/4.18G [00:06<00:13, 227MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:   8% 403M/4.96G [00:08<01:43, 44.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  12% 604M/4.96G [00:09<00:59, 72.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:   0% 23.8k/4.99G [00:10<592:18:24, 2.34kB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  16% 805M/4.96G [00:10<00:38, 108MB/s] \u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  31% 1.28G/4.18G [00:10<00:35, 81.3MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  18% 872M/4.96G [00:10<00:35, 115MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  22% 1.07G/4.96G [00:12<00:29, 132MB/s]\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  32% 1.34G/4.18G [00:12<00:43, 65.6MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  37% 1.54G/4.18G [00:13<00:28, 92.1MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  42% 1.75G/4.18G [00:14<00:20, 121MB/s] \u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:   0% 37.2k/4.99G [00:14<518:38:22, 2.67kB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  45% 1.88G/4.18G [00:14<00:16, 141MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  48% 2.01G/4.18G [00:15<00:12, 168MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:   0% 97.0k/4.99G [00:15<150:46:27, 9.20kB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  50% 2.08G/4.18G [00:15<00:11, 190MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  51% 2.15G/4.18G [00:15<00:10, 187MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:   0% 115k/4.99G [00:15<123:29:52, 11.2kB/s] \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  23% 1.14G/4.96G [00:16<01:04, 59.1MB/s]\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  53% 2.22G/4.18G [00:16<00:12, 153MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  26% 1.28G/4.96G [00:16<00:46, 79.2MB/s]\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  55% 2.28G/4.18G [00:17<00:14, 131MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  27% 1.34G/4.96G [00:17<00:42, 84.4MB/s]\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  58% 2.42G/4.18G [00:17<00:10, 161MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:   1% 67.2M/4.99G [00:17<08:25, 9.75MB/s]   \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  28% 1.41G/4.96G [00:18<00:40, 86.6MB/s]\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  59% 2.48G/4.18G [00:18<00:11, 143MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  61% 2.55G/4.18G [00:18<00:11, 145MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  31% 1.54G/4.96G [00:19<00:35, 95.3MB/s]\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  63% 2.62G/4.18G [00:19<00:11, 141MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  64% 2.68G/4.18G [00:20<00:12, 118MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  32% 1.61G/4.96G [00:21<00:47, 70.8MB/s]\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  66% 2.75G/4.18G [00:21<00:17, 81.4MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  35% 1.75G/4.96G [00:22<00:40, 79.2MB/s]\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  69% 2.89G/4.18G [00:22<00:12, 101MB/s] \u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  37% 1.81G/4.96G [00:22<00:32, 97.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:   3% 134M/4.99G [00:22<06:52, 11.8MB/s] \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  41% 2.01G/4.96G [00:23<00:18, 156MB/s] \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:   3% 137M/4.99G [00:25<08:57, 9.03MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  43% 2.15G/4.96G [00:26<00:36, 76.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:   3% 145M/4.99G [00:27<10:20, 7.81MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  45% 2.21G/4.96G [00:27<00:34, 79.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:   4% 209M/4.99G [00:27<04:43, 16.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  47% 2.35G/4.96G [00:27<00:23, 109MB/s] \u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  70% 2.90G/4.18G [00:27<00:40, 31.1MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:   5% 270M/4.99G [00:28<02:55, 27.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  49% 2.42G/4.96G [00:28<00:22, 112MB/s]\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  71% 2.97G/4.18G [00:28<00:31, 38.0MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  73% 3.04G/4.18G [00:29<00:22, 49.5MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  74% 3.11G/4.18G [00:29<00:16, 63.9MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  76% 3.17G/4.18G [00:29<00:12, 80.7MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  51% 2.55G/4.96G [00:30<00:24, 97.4MB/s]\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  78% 3.24G/4.18G [00:30<00:12, 76.3MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  79% 3.31G/4.18G [00:30<00:09, 96.3MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:   6% 300M/4.99G [00:31<03:55, 19.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  54% 2.68G/4.96G [00:31<00:21, 105MB/s] \u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  81% 3.37G/4.18G [00:31<00:07, 111MB/s] \u001b[A\n",
            "model-00003-of-00003.safetensors:  82% 3.44G/4.18G [00:31<00:05, 134MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:   7% 367M/4.99G [00:31<02:36, 29.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  84% 3.51G/4.18G [00:32<00:05, 115MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  86% 3.57G/4.18G [00:32<00:04, 136MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  55% 2.75G/4.96G [00:33<00:28, 76.5MB/s]\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  87% 3.64G/4.18G [00:33<00:03, 138MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  57% 2.82G/4.96G [00:33<00:27, 77.5MB/s]\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  89% 3.71G/4.18G [00:34<00:04, 102MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  58% 2.89G/4.96G [00:34<00:23, 86.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:   9% 434M/4.99G [00:35<03:02, 25.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  90% 3.77G/4.18G [00:36<00:06, 65.4MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  61% 3.02G/4.96G [00:36<00:23, 81.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  62% 3.09G/4.96G [00:36<00:22, 85.0MB/s]\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  92% 3.84G/4.18G [00:37<00:05, 61.2MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  95% 3.97G/4.18G [00:37<00:01, 103MB/s] \u001b[A\n",
            "model-00003-of-00003.safetensors:  97% 4.04G/4.18G [00:37<00:01, 121MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors: 100% 4.18G/4.18G [00:38<00:00, 154MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors: 100% 4.18G/4.18G [00:38<00:00, 109MB/s]\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  64% 3.15G/4.96G [00:38<00:29, 60.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  65% 3.22G/4.96G [00:39<00:23, 73.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  68% 3.35G/4.96G [00:40<00:21, 75.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  70% 3.49G/4.96G [00:41<00:15, 96.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  72% 3.56G/4.96G [00:42<00:13, 107MB/s] \u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  73% 3.62G/4.96G [00:42<00:11, 119MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  76% 3.76G/4.96G [00:42<00:07, 161MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  77% 3.82G/4.96G [00:43<00:06, 167MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  80% 3.96G/4.96G [00:43<00:04, 218MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  81% 4.03G/4.96G [00:43<00:04, 217MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  11% 539M/4.99G [00:43<04:50, 15.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  11% 573M/4.99G [00:44<03:54, 18.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  12% 615M/4.99G [00:44<02:53, 25.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  14% 676M/4.99G [00:44<01:54, 37.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  15% 743M/4.99G [00:45<01:23, 51.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  83% 4.09G/4.96G [00:45<00:08, 103MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  16% 810M/4.99G [00:45<01:05, 63.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  19% 944M/4.99G [00:46<00:36, 110MB/s] \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  84% 4.16G/4.96G [00:46<00:08, 92.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  85% 4.23G/4.96G [00:46<00:06, 115MB/s] \u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  87% 4.29G/4.96G [00:47<00:05, 127MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  20% 978M/4.99G [00:47<00:54, 73.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  20% 1.01G/4.99G [00:47<00:55, 71.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  89% 4.43G/4.96G [00:48<00:04, 129MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  22% 1.07G/4.99G [00:48<00:50, 77.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  23% 1.14G/4.99G [00:49<00:56, 68.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  24% 1.21G/4.99G [00:50<00:45, 83.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  26% 1.27G/4.99G [00:50<00:37, 99.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  91% 4.49G/4.96G [00:50<00:07, 60.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  92% 4.56G/4.96G [00:51<00:05, 73.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  93% 4.62G/4.96G [00:51<00:03, 95.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  27% 1.34G/4.99G [00:52<00:55, 65.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  28% 1.41G/4.99G [00:52<00:46, 76.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  95% 4.69G/4.96G [00:53<00:04, 57.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  30% 1.48G/4.99G [00:55<01:11, 49.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  96% 4.76G/4.96G [00:56<00:04, 45.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  97% 4.82G/4.96G [00:56<00:02, 54.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  31% 1.54G/4.99G [00:56<01:08, 50.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  32% 1.61G/4.99G [00:57<00:50, 66.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  34% 1.68G/4.99G [00:57<00:38, 85.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  35% 1.74G/4.99G [00:57<00:30, 108MB/s] \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  38% 1.88G/4.99G [00:57<00:19, 161MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  39% 1.95G/4.99G [00:58<00:18, 163MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  40% 2.01G/4.99G [00:58<00:16, 179MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  43% 2.15G/4.99G [00:58<00:11, 249MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  46% 2.28G/4.99G [00:59<00:08, 303MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  47% 2.35G/4.99G [00:59<00:09, 267MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  50% 2.49G/4.99G [00:59<00:07, 352MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  51% 2.56G/4.99G [00:59<00:06, 381MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  99% 4.89G/4.96G [01:00<00:01, 36.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  53% 2.62G/4.99G [01:00<00:11, 202MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  54% 2.69G/4.99G [01:00<00:10, 211MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  55% 2.76G/4.99G [01:01<00:10, 217MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  57% 2.83G/4.99G [01:01<00:09, 227MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors: 100% 4.96G/4.96G [01:01<00:00, 80.3MB/s]\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  58% 2.89G/4.99G [01:02<00:11, 180MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  59% 2.96G/4.99G [01:02<00:12, 168MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  61% 3.03G/4.99G [01:02<00:12, 158MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  62% 3.09G/4.99G [01:03<00:11, 167MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  63% 3.15G/4.99G [01:03<00:09, 193MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  65% 3.22G/4.99G [01:03<00:10, 173MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  66% 3.29G/4.99G [01:04<00:08, 202MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  69% 3.42G/4.99G [01:04<00:04, 315MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  70% 3.49G/4.99G [01:04<00:04, 338MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  73% 3.62G/4.99G [01:04<00:02, 458MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  74% 3.69G/4.99G [01:04<00:02, 470MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  75% 3.76G/4.99G [01:05<00:03, 358MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  77% 3.83G/4.99G [01:05<00:03, 307MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  78% 3.89G/4.99G [01:05<00:03, 277MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  79% 3.96G/4.99G [01:06<00:05, 197MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  81% 4.03G/4.99G [01:06<00:06, 157MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  82% 4.09G/4.99G [01:07<00:05, 174MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  83% 4.16G/4.99G [01:07<00:04, 186MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  85% 4.23G/4.99G [01:08<00:04, 164MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  86% 4.29G/4.99G [01:08<00:04, 174MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  87% 4.36G/4.99G [01:08<00:03, 175MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  89% 4.43G/4.99G [01:09<00:03, 161MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  91% 4.56G/4.99G [01:09<00:01, 228MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  93% 4.63G/4.99G [01:09<00:01, 236MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  94% 4.70G/4.99G [01:10<00:01, 239MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  95% 4.73G/4.99G [01:10<00:01, 240MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  96% 4.79G/4.99G [01:10<00:00, 243MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors: 100% 4.99G/4.99G [01:10<00:00, 70.7MB/s]\n",
            "Fetching 3 files: 100% 3/3 [01:11<00:00, 23.70s/it]\n",
            "Loading checkpoint shards: 100% 3/3 [00:04<00:00,  1.44s/it]\n",
            "generation_config.json: 100% 141/141 [00:00<00:00, 1.59MB/s]\n",
            "Model loaded.\n",
            "Running 3 inference passes...\n",
            " Run 1/3: 8.78 tokens/sec, 13.52 GB\n",
            " Run 2/3: 28.59 tokens/sec, 13.52 GB\n",
            " Run 3/3: 28.79 tokens/sec, 13.52 GB\n",
            "\n",
            "kv_cache_results Results:\n",
            "  Average Latency: 6.119 seconds\n",
            "  Average Throughput: 22.05 tokens/sec\n",
            "  Average Memory: 13.52 GB\n",
            "\n",
            "Sample Output:\n",
            "USER:  \n",
            "Describe this image in detail.\n",
            "ASSISTANT: The image features a pier extending out into a large body of water, likely a lake. The pier is made of wood and has a bench situated on it, providing ...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python test_flash_attn.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJZATiT0eCXB",
        "outputId": "7db8e94b-0f62-438a-cb62-edb0d256a708"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-06 03:36:39.627135: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-06 03:36:39.644323: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764992199.665528    1953 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764992199.671850    1953 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764992199.687893    1953 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764992199.687919    1953 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764992199.687923    1953 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764992199.687927    1953 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-06 03:36:39.692650: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Loading sample image...\n",
            "Image size: (1000, 667)\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "TEST 2: With Flash Attention 2\n",
            "--------------------------------------------------------------------------------\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Loading checkpoint shards: 100% 3/3 [00:04<00:00,  1.60s/it]\n",
            "Running 3 inference passes...\n",
            " Run 1/3: 17.81 tokens/sec, 13.52 GB\n",
            " Run 2/3: 21.67 tokens/sec, 13.52 GB\n",
            " Run 3/3: 21.75 tokens/sec, 13.52 GB\n",
            "\n",
            "Flash Attention 2 Results:\n",
            "  Average Latency: 4.943 seconds\n",
            "  Average Throughput: 20.41 tokens/sec\n",
            "  Average Memory: 13.52 GB\n",
            "\n",
            "Sample Output:\n",
            "USER:  \n",
            "Describe this image in detail.\n",
            "ASSISTANT: The image features a pier extending out into a large body of water, likely a lake. The pier is made of wood and has a bench situated on it, providing ...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python test_bitsandbytes.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vW2O-BceeFvK",
        "outputId": "1d32a5b4-959a-4cb2-d835-c9d488795031"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-06 03:37:19.532306: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-06 03:37:19.549891: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764992239.571091    2202 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764992239.577486    2202 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764992239.593845    2202 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764992239.593886    2202 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764992239.593889    2202 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764992239.593891    2202 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-06 03:37:19.598707: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Loading sample image...\n",
            "Image size: (1000, 667)\n",
            "\n",
            "================================================================================\n",
            "bitsandbytes Quantization 4 bits Test for LLaVA\n",
            "================================================================================\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "Loading checkpoint shards: 100% 3/3 [00:17<00:00,  5.82s/it]\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "\n",
            "AWQ bitsandbytes quant 4 bit complete. Loading quantized model...\n",
            "\n",
            "Running 3 inference passes...\n",
            " Run 1/3: 14.25 tokens/sec, 4.69 GB\n",
            " Run 2/3: 15.80 tokens/sec, 4.69 GB\n",
            " Run 3/3: 15.57 tokens/sec, 4.69 GB\n",
            "\n",
            "bitsandbytes Results:\n",
            "  Avg Latency: 6.591 seconds\n",
            "  Avg Throughput: 15.20 tokens/sec\n",
            "  Avg Memory: 4.69 GB\n",
            "\n",
            "Sample Output:\n",
            "USER:  \n",
            "Describe this image in detail.\n",
            "ASSISTANT: The image features a wooden pier extending out over a body of water. The pier is situated in a serene, natural setting with a mountain in the backgrou...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python test_kv.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aik6Mkopek8e",
        "outputId": "4ab3e3bd-af51-4dc8-9f5d-6ae2d480ba1f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-06 03:38:34.563233: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-06 03:38:34.580577: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764992314.601689    2620 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764992314.608064    2620 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764992314.624199    2620 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764992314.624226    2620 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764992314.624229    2620 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764992314.624232    2620 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-06 03:38:34.629058: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Loading sample image...\n",
            "Image size: (1000, 667)\n",
            "\n",
            "================================================================================\n",
            "KV_cache Test for LLaVA\n",
            "================================================================================\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Loading checkpoint shards: 100% 3/3 [00:04<00:00,  1.44s/it]\n",
            "Model loaded.\n",
            "Running 3 inference passes...\n",
            " Run 1/3: 151.07 tokens/sec, 13.55 GB\n",
            " Run 2/3: 164.37 tokens/sec, 13.55 GB\n",
            " Run 3/3: 168.80 tokens/sec, 13.55 GB\n",
            "\n",
            "kv_cache Results:\n",
            "  Average Latency: 4.309 seconds\n",
            "  Average Throughput: 161.41 tokens/sec\n",
            "  Average Memory: 13.55 GB\n",
            "\n",
            "Sample Output:\n",
            "USER:  \n",
            "Describe this image in detail.\n",
            "ASSISTANT: The image features a pier with a wooden walkway extending out into the water. The pier is situated in front of a beautiful mountain range, creating a ...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python test_kv_prune_quant.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H55M7Pi8yexw",
        "outputId": "cc2a666c-eb9b-4094-d3fc-56873a7ac96b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-06 03:39:13.007741: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-06 03:39:13.026248: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764992353.047711    2859 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764992353.054151    2859 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764992353.070747    2859 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764992353.070775    2859 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764992353.070778    2859 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764992353.070781    2859 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-06 03:39:13.075573: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Loading sample image...\n",
            "Image size: (1000, 667)\n",
            "\n",
            "================================================================================\n",
            "Prun + Quantization KV_cache Test for LLaVA\n",
            "================================================================================\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Loading checkpoint shards: 100% 3/3 [00:04<00:00,  1.43s/it]\n",
            "Model loaded.\n",
            "Running 3 inference passes...\n",
            " Run 1/3: 162.10 tokens/sec, 14.29 GB\n",
            " Run 2/3: 165.74 tokens/sec, 14.29 GB\n",
            " Run 3/3: 164.81 tokens/sec, 14.29 GB\n",
            "\n",
            "Prun + Quant_kv_cache Results:\n",
            "  Average Latency: 4.227 seconds\n",
            "  Average Throughput: 164.22 tokens/sec\n",
            "  Average Memory: 14.29 GB\n",
            "\n",
            "Sample Output:\n",
            "USER:  \n",
            "Describe this image in detail.\n",
            "ASSISTANT: The image features a pier or dock extending over a body of water, likely a lake or a river. The scene is set in a mountainous area, with a mountain in...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python test_kv_prune.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YPBK2sapy0zb",
        "outputId": "e0cbf2aa-8439-40f2-961b-d81faff11b81"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-06 03:39:51.679973: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-06 03:39:51.696737: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764992391.717845    3077 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764992391.724267    3077 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764992391.740306    3077 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764992391.740332    3077 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764992391.740335    3077 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764992391.740338    3077 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-06 03:39:51.745164: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Loading sample image...\n",
            "Image size: (1000, 667)\n",
            "\n",
            "================================================================================\n",
            "Prun KV_cache Test for LLaVA\n",
            "================================================================================\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Loading checkpoint shards: 100% 3/3 [00:04<00:00,  1.43s/it]\n",
            "Model loaded.\n",
            "Running 3 inference passes...\n",
            " Run 1/3: 303.07 tokens/sec, 14.11 GB\n",
            " Run 2/3: 313.01 tokens/sec, 14.11 GB\n",
            " Run 3/3: 313.56 tokens/sec, 14.11 GB\n",
            "\n",
            "Prun_kv_cacheresults Results:\n",
            "  Average Latency: 2.076 seconds\n",
            "  Average Throughput: 309.88 tokens/sec\n",
            "  Average Memory: 14.11 GB\n",
            "\n",
            "Sample Output:\n",
            "USER:  \n",
            "Describe this image in detail.\n",
            "ASSISTANT:The image shows a pier extending out into the water. The pier is situated on the water's edge, and it is a long pier that extends out into the water. T...\n",
            "\n",
            "Loading sample image...\n",
            "Image size: (850, 567)\n",
            "\n",
            "================================================================================\n",
            "Prun KV_cache Test for LLaVA\n",
            "================================================================================\n",
            "Loading checkpoint shards: 100% 3/3 [00:04<00:00,  1.44s/it]\n",
            "Model loaded.\n",
            "Running 3 inference passes...\n",
            " Run 1/3: 289.98 tokens/sec, 14.11 GB\n",
            " Run 2/3: 284.96 tokens/sec, 14.11 GB\n",
            " Run 3/3: 289.03 tokens/sec, 14.11 GB\n",
            "\n",
            "Prun_kv_cacheresults Results:\n",
            "  Average Latency: 2.247 seconds\n",
            "  Average Throughput: 287.99 tokens/sec\n",
            "  Average Memory: 14.11 GB\n",
            "\n",
            "Sample Output:\n",
            "USER:  \n",
            "Describe this image in detail.\n",
            "ASSISTANT:The image shows a close-up of a cat's face, with its eyes, nose, and mouth clearly visible.\n",
            ": The cat's face is the main focus of the image, with its e...\n",
            "\n",
            "Loading sample image...\n",
            "Image size: (850, 567)\n",
            "\n",
            "================================================================================\n",
            "Prun KV_cache Test for LLaVA\n",
            "================================================================================\n",
            "Loading checkpoint shards: 100% 3/3 [00:04<00:00,  1.44s/it]\n",
            "Model loaded.\n",
            "Running 3 inference passes...\n",
            " Run 1/3: 288.67 tokens/sec, 14.12 GB\n",
            " Run 2/3: 287.95 tokens/sec, 14.12 GB\n",
            " Run 3/3: 288.47 tokens/sec, 14.12 GB\n",
            "\n",
            "Prun_kv_cacheresults Results:\n",
            "  Average Latency: 2.244 seconds\n",
            "  Average Throughput: 288.37 tokens/sec\n",
            "  Average Memory: 14.12 GB\n",
            "\n",
            "Sample Output:\n",
            "USER:  \n",
            "Describe this image in detail.\n",
            "ASSISTANT:The image shows a close-up of a cat's face, with its eyes, nose, and mouth clearly visible.\n",
            ": The cat's face is the main focus of the image, with its e...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python test_kv_quant.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afVotDivy6tQ",
        "outputId": "a279f9e8-3b82-4f51-8209-dfa3ac517e9b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-06 03:41:00.787214: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-06 03:41:00.804867: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764992460.826304    3424 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764992460.832805    3424 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764992460.849089    3424 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764992460.849118    3424 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764992460.849121    3424 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764992460.849124    3424 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-06 03:41:00.853928: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Loading sample image...\n",
            "Image size: (1000, 667)\n",
            "\n",
            "================================================================================\n",
            "Quantization KV_cache Test for LLaVA\n",
            "================================================================================\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Loading checkpoint shards: 100% 3/3 [00:04<00:00,  1.42s/it]\n",
            "Model loaded.\n",
            "Running 3 inference passes...\n",
            " Run 1/3: 160.99 tokens/sec, 14.28 GB\n",
            " Run 2/3: 166.23 tokens/sec, 14.28 GB\n",
            " Run 3/3: 165.62 tokens/sec, 14.28 GB\n",
            "\n",
            "Quant_kv_cache Results:\n",
            "  Average Latency: 4.225 seconds\n",
            "  Average Throughput: 164.28 tokens/sec\n",
            "  Average Memory: 14.28 GB\n",
            "\n",
            "Sample Output:\n",
            "USER:  \n",
            "Describe this image in detail.\n",
            "ASSISTANT: The image features a pier with a wooden walkway extending out into the water. The pier is situated in front of a beautiful mountain range, creating a ...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python int8_tensor_prune_quant.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5r5OpOrz5glA",
        "outputId": "de2bb96a-78a8-4255-ae4b-5667c6902bf6"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-06 03:41:38.235859: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-06 03:41:38.253482: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764992498.274797    3636 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764992498.281243    3636 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764992498.297613    3636 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764992498.297653    3636 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764992498.297656    3636 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764992498.297659    3636 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-06 03:41:38.302570: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "quant only\n",
            "Loading sample image...\n",
            "Image size: (1000, 667)\n",
            "\n",
            "================================================================================\n",
            "Method 1: INT8 + Tensor-wise KV Pruning + KV Quantization\n",
            "================================================================================\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "Loading checkpoint shards: 100% 3/3 [00:16<00:00,  5.57s/it]\n",
            "INT8 model loaded (bitsandbytes).\n",
            "Running 3 inference passes...\n",
            " Run 1/3: 43.23 tokens/sec, 7.97 GB\n",
            " Run 2/3: 45.11 tokens/sec, 7.97 GB\n",
            " Run 3/3: 45.19 tokens/sec, 7.97 GB\n",
            "\n",
            "Method 1 Results (INT8 + Tensor-wise Prune + KV Quant):\n",
            "  Average Latency: 15.599 seconds\n",
            "  Average Throughput: 44.51 tokens/sec\n",
            "  Average Memory: 7.97 GB\n",
            "\n",
            "Sample Output:\n",
            "USER:  \n",
            "Describe this image in detail.\n",
            "ASSISTANT: The image features a pier with a wooden walkway extending out into the water. The pier is situated near a beautiful mountain range, creating a picture...\n",
            "\n",
            "Pruning only\n",
            "Loading sample image...\n",
            "Image size: (1000, 667)\n",
            "\n",
            "================================================================================\n",
            "Method 1: INT8 + Tensor-wise KV Pruning + KV Quantization\n",
            "================================================================================\n",
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "Loading checkpoint shards: 100% 3/3 [00:16<00:00,  5.56s/it]\n",
            "INT8 model loaded (bitsandbytes).\n",
            "Running 3 inference passes...\n",
            " Run 1/3: 43.48 tokens/sec, 7.83 GB\n",
            " Run 2/3: 43.04 tokens/sec, 7.83 GB\n",
            " Run 3/3: 43.52 tokens/sec, 7.83 GB\n",
            "\n",
            "Method 1 Results (INT8 + Tensor-wise Prune + KV Quant):\n",
            "  Average Latency: 16.011 seconds\n",
            "  Average Throughput: 43.35 tokens/sec\n",
            "  Average Memory: 7.83 GB\n",
            "\n",
            "Sample Output:\n",
            "USER:  \n",
            "Describe this image in detail.\n",
            "ASSISTANT: The image shows a pier extending over a body of water. The pier is situated on the shore of the body of water, extending out over the water. \n",
            "\n",
            "\n",
            "In the...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python int4_tensor_prune_quant.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8meb3O5M6RGK",
        "outputId": "c2972615-eacc-443d-bca0-66deb9755466"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-06 03:44:21.575983: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-06 03:44:21.593904: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764992661.615082    4419 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764992661.621582    4419 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764992661.637914    4419 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764992661.637943    4419 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764992661.637946    4419 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764992661.637949    4419 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-06 03:44:21.642705: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "quant only\n",
            "Loading sample image...\n",
            "Image size: (1000, 667)\n",
            "\n",
            "================================================================================\n",
            "Method 1: INT4 + Tensor-wise KV Pruning + KV Quantization\n",
            "================================================================================\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "Loading checkpoint shards: 100% 3/3 [00:16<00:00,  5.64s/it]\n",
            "INT4 model loaded (bitsandbytes).\n",
            "Running 3 inference passes...\n",
            " Run 1/3: 101.45 tokens/sec, 5.20 GB\n",
            " Run 2/3: 102.80 tokens/sec, 5.20 GB\n",
            " Run 3/3: 104.42 tokens/sec, 5.20 GB\n",
            "\n",
            "Method 1 Results (INT4 + Tensor-wise Prune + KV Quant):\n",
            "  Average Latency: 6.746 seconds\n",
            "  Average Throughput: 102.89 tokens/sec\n",
            "  Average Memory: 5.20 GB\n",
            "\n",
            "Sample Output:\n",
            "USER:  \n",
            "Describe this image in detail.\n",
            "ASSISTANT: The image features a wooden pier extending out over a large body of water, possibly a lake or a river. The pier is situated in a serene, natural envir...\n",
            "\n",
            "Pruning only\n",
            "Loading sample image...\n",
            "Image size: (1000, 667)\n",
            "\n",
            "================================================================================\n",
            "Method 1: INT4 + Tensor-wise KV Pruning + KV Quantization\n",
            "================================================================================\n",
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "Loading checkpoint shards: 100% 3/3 [00:16<00:00,  5.57s/it]\n",
            "INT4 model loaded (bitsandbytes).\n",
            "Running 3 inference passes...\n",
            " Run 1/3: 103.17 tokens/sec, 5.05 GB\n",
            " Run 2/3: 104.88 tokens/sec, 5.05 GB\n",
            " Run 3/3: 104.44 tokens/sec, 5.05 GB\n",
            "\n",
            "Method 1 Results (INT4 + Tensor-wise Prune + KV Quant):\n",
            "  Average Latency: 6.663 seconds\n",
            "  Average Throughput: 104.16 tokens/sec\n",
            "  Average Memory: 5.05 GB\n",
            "\n",
            "Sample Output:\n",
            "USER:  \n",
            "Describe this image in detail.\n",
            "ASSISTANT: The image shows a pier, which is a wooden pier with a pier, and a pier, and a pier, and a pier, and a pier, and a pier, and a pier, and a wooden pier ...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python int4_channel_prune_quant.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iCWqqDdq7zj4",
        "outputId": "dd13361a-de90-4952-fbbe-80de7fb0d241"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-06 03:46:12.835649: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-06 03:46:12.853903: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764992772.875398    5006 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764992772.881947    5006 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764992772.898588    5006 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764992772.898628    5006 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764992772.898631    5006 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764992772.898639    5006 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-06 03:46:12.903497: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "enable quant\n",
            "Loading sample image...\n",
            "Image size: (1000, 667)\n",
            "\n",
            "================================================================================\n",
            "Method 2: INT4 + Channel-wise KV Pruning + KV Quantization\n",
            "================================================================================\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "Loading checkpoint shards: 100% 3/3 [00:16<00:00,  5.63s/it]\n",
            "INT4 model loaded (bitsandbytes).\n",
            "Running 3 inference passes for Method 2...\n",
            " Run 1/3: 104.21 tokens/sec, 5.21 GB\n",
            " Run 2/3: 104.76 tokens/sec, 5.21 GB\n",
            " Run 3/3: 104.74 tokens/sec, 5.21 GB\n",
            "\n",
            "Method 2 Results (INT4 + Channel-wise Prune + KV Quant):\n",
            "  Average Latency: 6.637 seconds\n",
            "  Average Throughput: 104.57 tokens/sec\n",
            "  Average Memory: 5.21 GB\n",
            "\n",
            "Sample Output:\n",
            "USER:  \n",
            "Describe this image in detail.\n",
            "ASSISTANT:\n",
            "\n",
            "* The image features a pier on a lake, with a boat visible in the foreground.\n",
            "\n",
            "* The pier is situated next to a forest, adding a serene and natural a...\n",
            "\n",
            "disable quant\n",
            "Loading sample image...\n",
            "Image size: (1000, 667)\n",
            "\n",
            "================================================================================\n",
            "Method 2: INT4 + Channel-wise KV Pruning + KV Quantization\n",
            "================================================================================\n",
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "Loading checkpoint shards: 100% 3/3 [00:16<00:00,  5.61s/it]\n",
            "INT4 model loaded (bitsandbytes).\n",
            "Running 3 inference passes for Method 2...\n",
            " Run 1/3: 105.86 tokens/sec, 5.05 GB\n",
            " Run 2/3: 104.79 tokens/sec, 5.05 GB\n",
            " Run 3/3: 104.65 tokens/sec, 5.05 GB\n",
            "\n",
            "Method 2 Results (INT4 + Channel-wise Prune + KV Quant):\n",
            "  Average Latency: 6.603 seconds\n",
            "  Average Throughput: 105.10 tokens/sec\n",
            "  Average Memory: 5.05 GB\n",
            "\n",
            "Sample Output:\n",
            "USER:  \n",
            "Describe this image in detail.\n",
            "ASSISTANT:\n",
            "\n",
            "* The image features a pier on a lake, with a boat visible in the scene.\n",
            "\n",
            "* The pier is situated next to a forest, adding a serene and natural atmosp...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python channel_prune_quant.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E6WzfMF_9c2U",
        "outputId": "7bd565c8-5b52-48c0-bfc2-222ae2126ddf"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-06 03:48:03.959874: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-06 03:48:03.977331: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764992883.998687    5596 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764992884.005142    5596 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764992884.021544    5596 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764992884.021576    5596 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764992884.021578    5596 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764992884.021581    5596 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-06 03:48:04.026519: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "enable quant\n",
            "Loading sample image...\n",
            "Image size: (1000, 667)\n",
            "\n",
            "================================================================================\n",
            "Method 2: Channel-wise KV Pruning + KV Quantization\n",
            "================================================================================\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Loading checkpoint shards: 100% 3/3 [00:04<00:00,  1.44s/it]\n",
            "LLava model loaded .\n",
            "Running 3 inference passes for Method 2...\n",
            " Run 1/3: 196.67 tokens/sec, 14.28 GB\n",
            " Run 2/3: 198.47 tokens/sec, 14.28 GB\n",
            " Run 3/3: 194.99 tokens/sec, 14.28 GB\n",
            "\n",
            "Method 2 Results (Channel-wise Prune + KV Quant):\n",
            "  Average Latency: 3.427 seconds\n",
            "  Average Throughput: 196.71 tokens/sec\n",
            "  Average Memory: 14.28 GB\n",
            "\n",
            "Sample Output:\n",
            "USER:  \n",
            "Describe this image in detail.\n",
            "ASSISTANT: The image features a pier or dock extending out into the water. The pier is situated near a mountainous area, adding a picturesque backdrop to the sce...\n",
            "\n",
            "disable quant\n",
            "Loading sample image...\n",
            "Image size: (1000, 667)\n",
            "\n",
            "================================================================================\n",
            "Method 2: Channel-wise KV Pruning + KV Quantization\n",
            "================================================================================\n",
            "Loading checkpoint shards: 100% 3/3 [00:04<00:00,  1.45s/it]\n",
            "LLava model loaded .\n",
            "Running 3 inference passes for Method 2...\n",
            " Run 1/3: 195.66 tokens/sec, 14.12 GB\n",
            " Run 2/3: 197.93 tokens/sec, 14.12 GB\n",
            " Run 3/3: 194.18 tokens/sec, 14.12 GB\n",
            "\n",
            "Method 2 Results (Channel-wise Prune + KV Quant):\n",
            "  Average Latency: 3.440 seconds\n",
            "  Average Throughput: 195.93 tokens/sec\n",
            "  Average Memory: 14.12 GB\n",
            "\n",
            "Sample Output:\n",
            "USER:  \n",
            "Describe this image in detail.\n",
            "ASSISTANT: The image features a pier or dock extending out into the water. The pier is situated near a mountainous area, adding a picturesque backdrop to the sce...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Batch Test"
      ],
      "metadata": {
        "id": "BOpk631vetP2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python test_baseline_batch.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6AVaFYccexo2",
        "outputId": "98b161d4-bf40-4c76-fa26-bba6a8364c62"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-06 03:49:01.252564: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-06 03:49:01.269883: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764992941.291247    5917 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764992941.297736    5917 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764992941.314196    5917 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764992941.314223    5917 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764992941.314226    5917 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764992941.314229    5917 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-06 03:49:01.319172: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Loading sample image...\n",
            "Image size: (1000, 667)\n",
            "\n",
            "================================================================================\n",
            "Baseline_without_kv_cache Test for LLaVA\n",
            "================================================================================\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Loading checkpoint shards: 100% 3/3 [00:04<00:00,  1.45s/it]\n",
            "Model loaded.\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "\n",
            "==============================\n",
            "  Batch Size Stress Test (true batch forward)\n",
            "==============================\n",
            "\n",
            "--- Batch Size = 4 (1 forward pass with 4 images) ---\n",
            " ==> BS=4: latency=4.536s, throughput=611.93 tok/s, max mem=14.57 GB\n",
            "\n",
            "--- Batch Size = 8 (1 forward pass with 8 images) ---\n",
            " ==> BS=8: latency=4.119s, throughput=1347.98 tok/s, max mem=15.98 GB\n",
            "\n",
            "--- Batch Size = 16 (1 forward pass with 16 images) ---\n",
            " ==> BS=16: latency=4.242s, throughput=2617.79 tok/s, max mem=18.79 GB\n",
            "\n",
            "--- Batch Size = 32 (1 forward pass with 32 images) ---\n",
            " ==> BS=32: latency=5.953s, throughput=3730.35 tok/s, max mem=24.41 GB\n",
            "\n",
            "Baseline Batch Results:\n",
            "  BS=4: Latency=4.536s, Throughput=611.93 tok/s, Memory=14.57 GB\n",
            "  BS=8: Latency=4.119s, Throughput=1347.98 tok/s, Memory=15.98 GB\n",
            "  BS=16: Latency=4.242s, Throughput=2617.79 tok/s, Memory=18.79 GB\n",
            "  BS=32: Latency=5.953s, Throughput=3730.35 tok/s, Memory=24.41 GB\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python test_flash_attn_batch.py"
      ],
      "metadata": {
        "id": "VjY4U_Ug5m-9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0781de8-5711-4b6e-ac6e-ff5c5813948a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-06 03:49:47.239177: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-06 03:49:47.256699: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764992987.278468    6165 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764992987.285065    6165 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764992987.301481    6165 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764992987.301521    6165 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764992987.301524    6165 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764992987.301526    6165 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-06 03:49:47.306480: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Loading sample image...\n",
            "Image size: (1000, 667)\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "TEST 2: With Flash Attention 2 (Batch Test)\n",
            "--------------------------------------------------------------------------------\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Loading checkpoint shards: 100% 3/3 [00:04<00:00,  1.45s/it]\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "\n",
            "==============================\n",
            "  Batch Size Stress Test (true batch forward)\n",
            "==============================\n",
            "\n",
            "--- Batch Size = 4 (1 forward pass with 4 images) ---\n",
            " ==> BS=4: latency=5.255s, throughput=528.29 tok/s, max mem=14.57 GB\n",
            "\n",
            "--- Batch Size = 8 (1 forward pass with 8 images) ---\n",
            " ==> BS=8: latency=4.773s, throughput=1163.15 tok/s, max mem=15.98 GB\n",
            "\n",
            "--- Batch Size = 16 (1 forward pass with 16 images) ---\n",
            " ==> BS=16: latency=5.009s, throughput=2216.79 tok/s, max mem=18.79 GB\n",
            "\n",
            "--- Batch Size = 32 (1 forward pass with 32 images) ---\n",
            " ==> BS=32: latency=5.983s, throughput=3711.91 tok/s, max mem=24.41 GB\n",
            "\n",
            "Flash Attention 2 Batch Results:\n",
            "  BS=4: Latency=5.255s, Throughput=528.29 tok/s, Memory=14.57 GB\n",
            "  BS=8: Latency=4.773s, Throughput=1163.15 tok/s, Memory=15.98 GB\n",
            "  BS=16: Latency=5.009s, Throughput=2216.79 tok/s, Memory=18.79 GB\n",
            "  BS=32: Latency=5.983s, Throughput=3711.91 tok/s, Memory=24.41 GB\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python test_kv_batch.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ed8pCHFDwlXy",
        "outputId": "5372e3c9-5f8b-4038-c3ac-e8a31f6561ec"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-06 03:50:35.317058: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-06 03:50:35.334743: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764993035.356307    6425 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764993035.362844    6425 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764993035.379529    6425 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764993035.379566    6425 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764993035.379569    6425 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764993035.379572    6425 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-06 03:50:35.384534: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\n",
            "Using kv cache:\n",
            "Loading sample image...\n",
            "Image size: (1000, 667)\n",
            "\n",
            "================================================================================\n",
            "Baseline KV_cache Batch Test for LLaVA\n",
            "================================================================================\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Loading checkpoint shards: 100% 3/3 [00:04<00:00,  1.44s/it]\n",
            "Model loaded.\n",
            "\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "\n",
            "==============================\n",
            "  Batch Size Stress Test (tensorwise)\n",
            "==============================\n",
            "\n",
            "--- Batch Size = 4 ---\n",
            " ==> BS=4, latency=4.427s, throughput=625.29 tok/s, max_mem=14.71 GB\n",
            "\n",
            "--- Batch Size = 8 ---\n",
            " ==> BS=8, latency=4.326s, throughput=1279.64 tok/s, max_mem=16.24 GB\n",
            "\n",
            "--- Batch Size = 16 ---\n",
            " ==> BS=16, latency=4.310s, throughput=2568.95 tok/s, max_mem=19.32 GB\n",
            "\n",
            "--- Batch Size = 32 ---\n",
            " ==> BS=32, latency=5.070s, throughput=4367.99 tok/s, max_mem=25.48 GB\n",
            "\n",
            "Baseline KV Cache Batch Results:\n",
            "  BS=4: Latency=4.427s, Throughput=625.29 tok/s, Memory=14.71 GB\n",
            "  BS=8: Latency=4.326s, Throughput=1279.64 tok/s, Memory=16.24 GB\n",
            "  BS=16: Latency=4.310s, Throughput=2568.95 tok/s, Memory=19.32 GB\n",
            "  BS=32: Latency=5.070s, Throughput=4367.99 tok/s, Memory=25.48 GB\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python test_kv_prune_batch.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G98BTrrIxdAg",
        "outputId": "dc85e14a-9395-4f7d-d8e0-ed14aeb66943"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-06 03:51:24.791715: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-06 03:51:24.809292: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764993084.830653    6683 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764993084.837050    6683 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764993084.853342    6683 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764993084.853370    6683 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764993084.853373    6683 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764993084.853376    6683 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-06 03:51:24.858257: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\n",
            "Using quantile for Pruning:\n",
            "Loading sample image...\n",
            "Image size: (1000, 667)\n",
            "\n",
            "================================================================================\n",
            "Tensor-wise PRUNING Only KV_cache Batch Test for LLaVA\n",
            "================================================================================\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Loading checkpoint shards: 100% 3/3 [00:04<00:00,  1.44s/it]\n",
            "Model loaded.\n",
            "\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "\n",
            "==============================\n",
            "  Batch Size Stress Test (tensorwise)\n",
            "==============================\n",
            "\n",
            "--- Batch Size = 4 ---\n",
            " ==> BS=4, latency=4.252s, throughput=649.16 tok/s, max_mem=17.04 GB\n",
            "\n",
            "--- Batch Size = 8 ---\n",
            "[ERROR] BS=8 failed: quantile() input tensor is too large\n",
            "\n",
            "--- Batch Size = 16 ---\n",
            "[ERROR] BS=16 failed: quantile() input tensor is too large\n",
            "\n",
            "--- Batch Size = 32 ---\n",
            "[ERROR] BS=32 failed: quantile() input tensor is too large\n",
            "\n",
            "Tensor-wise Pruning KV Cache Batch Results:\n",
            "  BS=4: Latency=4.252s, Throughput=649.16 tok/s, Memory=17.04 GB\n",
            "\n",
            "\n",
            "Using Topk for Pruning:\n",
            "Loading sample image...\n",
            "Image size: (1000, 667)\n",
            "\n",
            "================================================================================\n",
            "Tensor-wise PRUNING Only KV_cache Batch Test for LLaVA\n",
            "================================================================================\n",
            "Loading checkpoint shards: 100% 3/3 [00:04<00:00,  1.45s/it]\n",
            "Model loaded.\n",
            "\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "\n",
            "==============================\n",
            "  Batch Size Stress Test (tensorwise)\n",
            "==============================\n",
            "\n",
            "--- Batch Size = 4 ---\n",
            " ==> BS=4, latency=4.304s, throughput=641.23 tok/s, max_mem=17.04 GB\n",
            "\n",
            "--- Batch Size = 8 ---\n",
            " ==> BS=8, latency=4.383s, throughput=1263.04 tok/s, max_mem=20.93 GB\n",
            "\n",
            "--- Batch Size = 16 ---\n",
            " ==> BS=16, latency=4.434s, throughput=2497.06 tok/s, max_mem=28.59 GB\n",
            "\n",
            "--- Batch Size = 32 ---\n",
            " ==> BS=32, latency=5.844s, throughput=3789.27 tok/s, max_mem=44.06 GB\n",
            "\n",
            "Tensor-wise Pruning KV Cache Batch Results:\n",
            "  BS=4: Latency=4.304s, Throughput=641.23 tok/s, Memory=17.04 GB\n",
            "  BS=8: Latency=4.383s, Throughput=1263.04 tok/s, Memory=20.93 GB\n",
            "  BS=16: Latency=4.434s, Throughput=2497.06 tok/s, Memory=28.59 GB\n",
            "  BS=32: Latency=5.844s, Throughput=3789.27 tok/s, Memory=44.06 GB\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python test_kv_prune_quant_batch.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i6DnoWQxxux2",
        "outputId": "77d5fde9-9a55-4ed6-862f-18316cc19f29"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-06 03:52:39.746322: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-06 03:52:39.764295: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764993159.785572    7053 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764993159.792043    7053 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764993159.808841    7053 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764993159.808875    7053 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764993159.808878    7053 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764993159.808880    7053 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-06 03:52:39.813753: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\n",
            "Using quantile for Pruning:\n",
            "Loading sample image...\n",
            "Image size: (1000, 667)\n",
            "\n",
            "================================================================================\n",
            "Tensor-wise KV Pruning + Quantization Batch Test for LLaVA\n",
            "================================================================================\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Loading checkpoint shards: 100% 3/3 [00:04<00:00,  1.45s/it]\n",
            "Model loaded.\n",
            "\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "\n",
            "==============================\n",
            "  Batch Size Stress Test (tensorwise)\n",
            "==============================\n",
            "\n",
            "--- Batch Size = 4 ---\n",
            " ==> BS=4, latency=4.305s, throughput=642.96 tok/s, max_mem=17.66 GB\n",
            "\n",
            "--- Batch Size = 8 ---\n",
            "[ERROR] BS=8 failed: quantile() input tensor is too large\n",
            "\n",
            "--- Batch Size = 16 ---\n",
            "[ERROR] BS=16 failed: quantile() input tensor is too large\n",
            "\n",
            "--- Batch Size = 32 ---\n",
            "[ERROR] BS=32 failed: quantile() input tensor is too large\n",
            "\n",
            "Tensor-wise Prune + Quant KV Batch Results:\n",
            "  BS=4: Latency=4.305s, Throughput=642.96 tok/s, Memory=17.66 GB\n",
            "\n",
            "\n",
            "Using Topk for Pruning:\n",
            "Loading sample image...\n",
            "Image size: (1000, 667)\n",
            "\n",
            "================================================================================\n",
            "Tensor-wise KV Pruning + Quantization Batch Test for LLaVA\n",
            "================================================================================\n",
            "Loading checkpoint shards: 100% 3/3 [00:04<00:00,  1.45s/it]\n",
            "Model loaded.\n",
            "\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "\n",
            "==============================\n",
            "  Batch Size Stress Test (tensorwise)\n",
            "==============================\n",
            "\n",
            "--- Batch Size = 4 ---\n",
            " ==> BS=4, latency=4.302s, throughput=643.43 tok/s, max_mem=17.66 GB\n",
            "\n",
            "--- Batch Size = 8 ---\n",
            " ==> BS=8, latency=4.336s, throughput=1276.72 tok/s, max_mem=22.15 GB\n",
            "\n",
            "--- Batch Size = 16 ---\n",
            " ==> BS=16, latency=4.333s, throughput=2555.52 tok/s, max_mem=31.05 GB\n",
            "\n",
            "--- Batch Size = 32 ---\n",
            " ==> BS=32, latency=6.044s, throughput=3664.06 tok/s, max_mem=48.98 GB\n",
            "\n",
            "Tensor-wise Prune + Quant KV Batch Results:\n",
            "  BS=4: Latency=4.302s, Throughput=643.43 tok/s, Memory=17.66 GB\n",
            "  BS=8: Latency=4.336s, Throughput=1276.72 tok/s, Memory=22.15 GB\n",
            "  BS=16: Latency=4.333s, Throughput=2555.52 tok/s, Memory=31.05 GB\n",
            "  BS=32: Latency=6.044s, Throughput=3664.06 tok/s, Memory=48.98 GB\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python test_kv_quant_batch.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRQqn2xh_EgX",
        "outputId": "471bf2ad-a929-4f9f-962c-5d3992dba5fc"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-06 03:53:54.971295: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-06 03:53:54.989060: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764993235.010990    7425 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764993235.017547    7425 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764993235.034181    7425 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764993235.034218    7425 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764993235.034221    7425 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764993235.034224    7425 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-06 03:53:55.039241: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\n",
            "Using quantile for Pruning:\n",
            "Loading sample image...\n",
            "Image size: (1000, 667)\n",
            "\n",
            "================================================================================\n",
            "Tensor-wise Quantization-Only KV_cache Batch Test for LLaVA\n",
            "================================================================================\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Loading checkpoint shards: 100% 3/3 [00:04<00:00,  1.44s/it]\n",
            "Model loaded.\n",
            "\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "\n",
            "==============================\n",
            "  Batch Size Stress Test (tensorwise)\n",
            "==============================\n",
            "\n",
            "--- Batch Size = 4 ---\n",
            " ==> BS=4, latency=4.411s, throughput=627.53 tok/s, max_mem=17.65 GB\n",
            "\n",
            "--- Batch Size = 8 ---\n",
            " ==> BS=8, latency=4.309s, throughput=1284.88 tok/s, max_mem=22.09 GB\n",
            "\n",
            "--- Batch Size = 16 ---\n",
            " ==> BS=16, latency=4.369s, throughput=2533.93 tok/s, max_mem=30.91 GB\n",
            "\n",
            "--- Batch Size = 32 ---\n",
            " ==> BS=32, latency=6.920s, throughput=3200.09 tok/s, max_mem=48.69 GB\n",
            "\n",
            "Tensor-wise KV Quantization Batch Results:\n",
            "  BS=4: Latency=4.411s, Throughput=627.53 tok/s, Memory=17.65 GB\n",
            "  BS=8: Latency=4.309s, Throughput=1284.88 tok/s, Memory=22.09 GB\n",
            "  BS=16: Latency=4.369s, Throughput=2533.93 tok/s, Memory=30.91 GB\n",
            "  BS=32: Latency=6.920s, Throughput=3200.09 tok/s, Memory=48.69 GB\n",
            "\n",
            "\n",
            "Using Topk for Pruning:\n",
            "Loading sample image...\n",
            "Image size: (1000, 667)\n",
            "\n",
            "================================================================================\n",
            "Tensor-wise Quantization-Only KV_cache Batch Test for LLaVA\n",
            "================================================================================\n",
            "Loading checkpoint shards: 100% 3/3 [00:04<00:00,  1.46s/it]\n",
            "Model loaded.\n",
            "\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "\n",
            "==============================\n",
            "  Batch Size Stress Test (tensorwise)\n",
            "==============================\n",
            "\n",
            "--- Batch Size = 4 ---\n",
            " ==> BS=4, latency=4.358s, throughput=635.15 tok/s, max_mem=17.65 GB\n",
            "\n",
            "--- Batch Size = 8 ---\n",
            " ==> BS=8, latency=4.295s, throughput=1288.86 tok/s, max_mem=22.09 GB\n",
            "\n",
            "--- Batch Size = 16 ---\n",
            " ==> BS=16, latency=4.424s, throughput=2502.79 tok/s, max_mem=30.91 GB\n",
            "\n",
            "--- Batch Size = 32 ---\n",
            " ==> BS=32, latency=6.904s, throughput=3207.55 tok/s, max_mem=48.69 GB\n",
            "\n",
            "Tensor-wise KV Quantization Batch Results:\n",
            "  BS=4: Latency=4.358s, Throughput=635.15 tok/s, Memory=17.65 GB\n",
            "  BS=8: Latency=4.295s, Throughput=1288.86 tok/s, Memory=22.09 GB\n",
            "  BS=16: Latency=4.424s, Throughput=2502.79 tok/s, Memory=30.91 GB\n",
            "  BS=32: Latency=6.904s, Throughput=3207.55 tok/s, Memory=48.69 GB\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python int8_tensor_prune_quant_batch.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m9otAnG4An_M",
        "outputId": "e5f4c6d2-030a-4c67-b95a-6a281fb88ab5"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-06 03:55:25.060787: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-06 03:55:25.078650: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764993325.100060    7855 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764993325.106530    7855 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764993325.123219    7855 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764993325.123253    7855 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764993325.123256    7855 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764993325.123259    7855 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-06 03:55:25.128137: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\n",
            "Using quantile for Pruning:\n",
            "Loading sample image...\n",
            "Image size: (1000, 667)\n",
            "\n",
            "================================================================================\n",
            "Tensor-wise Quantization-Only KV_cache Batch Test for LLaVA\n",
            "================================================================================\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "Loading checkpoint shards: 100% 3/3 [00:16<00:00,  5.60s/it]\n",
            "Model loaded.\n",
            "\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "\n",
            "==============================\n",
            "  Batch Size Stress Test (tensorwise)\n",
            "==============================\n",
            "\n",
            "--- Batch Size = 4 ---\n",
            " ==> BS=4, latency=17.265s, throughput=160.33 tok/s, max_mem=11.34 GB\n",
            "\n",
            "--- Batch Size = 8 ---\n",
            " ==> BS=8, latency=17.146s, throughput=322.88 tok/s, max_mem=15.78 GB\n",
            "\n",
            "--- Batch Size = 16 ---\n",
            " ==> BS=16, latency=15.784s, throughput=689.30 tok/s, max_mem=24.56 GB\n",
            "\n",
            "--- Batch Size = 32 ---\n",
            " ==> BS=32, latency=18.189s, throughput=1198.08 tok/s, max_mem=42.16 GB\n",
            "\n",
            "Tensor-wise KV Quantization Batch Results:\n",
            "  BS=4: Latency=17.265s, Throughput=160.33 tok/s, Memory=11.34 GB\n",
            "  BS=8: Latency=17.146s, Throughput=322.88 tok/s, Memory=15.78 GB\n",
            "  BS=16: Latency=15.784s, Throughput=689.30 tok/s, Memory=24.56 GB\n",
            "  BS=32: Latency=18.189s, Throughput=1198.08 tok/s, Memory=42.16 GB\n",
            "\n",
            "\n",
            "Using Topk for Pruning:\n",
            "Loading sample image...\n",
            "Image size: (1000, 667)\n",
            "\n",
            "================================================================================\n",
            "Tensor-wise Quantization-Only KV_cache Batch Test for LLaVA\n",
            "================================================================================\n",
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "Loading checkpoint shards: 100% 3/3 [00:16<00:00,  5.61s/it]\n",
            "Model loaded.\n",
            "\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "\n",
            "==============================\n",
            "  Batch Size Stress Test (tensorwise)\n",
            "==============================\n",
            "\n",
            "--- Batch Size = 4 ---\n",
            " ==> BS=4, latency=17.187s, throughput=161.05 tok/s, max_mem=11.35 GB\n",
            "\n",
            "--- Batch Size = 8 ---\n",
            " ==> BS=8, latency=17.145s, throughput=322.89 tok/s, max_mem=15.79 GB\n",
            "\n",
            "--- Batch Size = 16 ---\n",
            " ==> BS=16, latency=15.860s, throughput=686.00 tok/s, max_mem=24.56 GB\n",
            "\n",
            "--- Batch Size = 32 ---\n",
            " ==> BS=32, latency=18.140s, throughput=1201.35 tok/s, max_mem=42.17 GB\n",
            "\n",
            "Tensor-wise KV Quantization Batch Results:\n",
            "  BS=4: Latency=17.187s, Throughput=161.05 tok/s, Memory=11.35 GB\n",
            "  BS=8: Latency=17.145s, Throughput=322.89 tok/s, Memory=15.79 GB\n",
            "  BS=16: Latency=15.860s, Throughput=686.00 tok/s, Memory=24.56 GB\n",
            "  BS=32: Latency=18.140s, Throughput=1201.35 tok/s, Memory=42.17 GB\n",
            "\n",
            "\n",
            "Disable quantization for KV cache:\n",
            "Loading sample image...\n",
            "Image size: (1000, 667)\n",
            "\n",
            "================================================================================\n",
            "Tensor-wise Quantization-Only KV_cache Batch Test for LLaVA\n",
            "================================================================================\n",
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "Loading checkpoint shards: 100% 3/3 [00:16<00:00,  5.63s/it]\n",
            "Model loaded.\n",
            "\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "\n",
            "==============================\n",
            "  Batch Size Stress Test (tensorwise)\n",
            "==============================\n",
            "\n",
            "--- Batch Size = 4 ---\n",
            " ==> BS=4, latency=17.179s, throughput=161.13 tok/s, max_mem=8.40 GB\n",
            "\n",
            "--- Batch Size = 8 ---\n",
            " ==> BS=8, latency=17.091s, throughput=323.91 tok/s, max_mem=9.94 GB\n",
            "\n",
            "--- Batch Size = 16 ---\n",
            " ==> BS=16, latency=17.951s, throughput=616.79 tok/s, max_mem=13.02 GB\n",
            "\n",
            "--- Batch Size = 32 ---\n",
            " ==> BS=32, latency=19.508s, throughput=1131.84 tok/s, max_mem=19.14 GB\n",
            "\n",
            "Tensor-wise KV Quantization Batch Results:\n",
            "  BS=4: Latency=17.179s, Throughput=161.13 tok/s, Memory=8.40 GB\n",
            "  BS=8: Latency=17.091s, Throughput=323.91 tok/s, Memory=9.94 GB\n",
            "  BS=16: Latency=17.951s, Throughput=616.79 tok/s, Memory=13.02 GB\n",
            "  BS=32: Latency=19.508s, Throughput=1131.84 tok/s, Memory=19.14 GB\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python int4_tensor_prune_quant_batch.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dO0cJsJcBIGb",
        "outputId": "60c2534e-e88d-4682-9676-8461f5e4e0c1"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-06 04:00:48.387904: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-06 04:00:48.405920: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764993648.427758    9320 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764993648.434210    9320 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764993648.450916    9320 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764993648.450946    9320 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764993648.450948    9320 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764993648.450951    9320 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-06 04:00:48.455874: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\n",
            "Using quantile for Pruning:\n",
            "================================================================================\n",
            "Method 1: INT4 + Tensor-wise KV Pruning + KV Quantization (Batch Test)\n",
            "================================================================================\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "Loading checkpoint shards: 100% 3/3 [00:17<00:00,  5.68s/it]\n",
            "INT4 model loaded (bitsandbytes).\n",
            "\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "\n",
            "==============================\n",
            "  Batch Size Stress Test (tensorwise)\n",
            "==============================\n",
            "\n",
            "--- Batch Size = 4 ---\n",
            " ==> BS=4, latency=10.154s, throughput=272.60 tok/s, max_mem=8.79 GB\n",
            "\n",
            "--- Batch Size = 8 ---\n",
            "[ERROR] BS=8 failed: quantile() input tensor is too large\n",
            "\n",
            "--- Batch Size = 16 ---\n",
            "[ERROR] BS=16 failed: quantile() input tensor is too large\n",
            "\n",
            "--- Batch Size = 32 ---\n",
            "[ERROR] BS=32 failed: quantile() input tensor is too large\n",
            "\n",
            "Method 1 Batch Results (INT4 + Tensor-wise Prune + KV Quant):\n",
            "  BS=4: Latency=10.154s, Throughput=272.60 tok/s, Memory=8.79 GB\n",
            "\n",
            "\n",
            "Using Topk for Pruning:\n",
            "================================================================================\n",
            "Method 1: INT4 + Tensor-wise KV Pruning + KV Quantization (Batch Test)\n",
            "================================================================================\n",
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "Loading checkpoint shards: 100% 3/3 [00:17<00:00,  5.70s/it]\n",
            "INT4 model loaded (bitsandbytes).\n",
            "\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "\n",
            "==============================\n",
            "  Batch Size Stress Test (tensorwise)\n",
            "==============================\n",
            "\n",
            "--- Batch Size = 4 ---\n",
            " ==> BS=4, latency=9.435s, throughput=293.36 tok/s, max_mem=8.78 GB\n",
            "\n",
            "--- Batch Size = 8 ---\n",
            " ==> BS=8, latency=10.209s, throughput=542.26 tok/s, max_mem=13.24 GB\n",
            "\n",
            "--- Batch Size = 16 ---\n",
            " ==> BS=16, latency=10.912s, throughput=1014.68 tok/s, max_mem=22.13 GB\n",
            "\n",
            "--- Batch Size = 32 ---\n",
            " ==> BS=32, latency=12.875s, throughput=1719.97 tok/s, max_mem=39.98 GB\n",
            "\n",
            "Method 1 Batch Results (INT4 + Tensor-wise Prune + KV Quant):\n",
            "  BS=4: Latency=9.435s, Throughput=293.36 tok/s, Memory=8.78 GB\n",
            "  BS=8: Latency=10.209s, Throughput=542.26 tok/s, Memory=13.24 GB\n",
            "  BS=16: Latency=10.912s, Throughput=1014.68 tok/s, Memory=22.13 GB\n",
            "  BS=32: Latency=12.875s, Throughput=1719.97 tok/s, Memory=39.98 GB\n",
            "\n",
            "\n",
            "Disable quantization for KV cache:\n",
            "================================================================================\n",
            "Method 1: INT4 + Tensor-wise KV Pruning + KV Quantization (Batch Test)\n",
            "================================================================================\n",
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "Loading checkpoint shards: 100% 3/3 [00:16<00:00,  5.46s/it]\n",
            "INT4 model loaded (bitsandbytes).\n",
            "\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "\n",
            "==============================\n",
            "  Batch Size Stress Test (tensorwise)\n",
            "==============================\n",
            "\n",
            "--- Batch Size = 4 ---\n",
            " ==> BS=4, latency=9.544s, throughput=290.04 tok/s, max_mem=8.17 GB\n",
            "\n",
            "--- Batch Size = 8 ---\n",
            " ==> BS=8, latency=10.092s, throughput=548.55 tok/s, max_mem=12.00 GB\n",
            "\n",
            "--- Batch Size = 16 ---\n",
            " ==> BS=16, latency=10.923s, throughput=1013.68 tok/s, max_mem=19.67 GB\n",
            "\n",
            "--- Batch Size = 32 ---\n",
            " ==> BS=32, latency=12.705s, throughput=1742.96 tok/s, max_mem=35.06 GB\n",
            "\n",
            "Method 1 Batch Results (INT4 + Tensor-wise Prune + KV Quant):\n",
            "  BS=4: Latency=9.544s, Throughput=290.04 tok/s, Memory=8.17 GB\n",
            "  BS=8: Latency=10.092s, Throughput=548.55 tok/s, Memory=12.00 GB\n",
            "  BS=16: Latency=10.923s, Throughput=1013.68 tok/s, Memory=19.67 GB\n",
            "  BS=32: Latency=12.705s, Throughput=1742.96 tok/s, Memory=35.06 GB\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python int4_channel_prune_quant_batch.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mXPS6YqdBrP4",
        "outputId": "b12ace53-8a53-4ca8-e817-fd4e9771748b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-06 04:05:35.238142: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-06 04:05:35.256183: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764993935.278644   10649 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764993935.285360   10649 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764993935.302235   10649 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764993935.302272   10649 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764993935.302275   10649 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764993935.302277   10649 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-06 04:05:35.307408: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\n",
            "Using unstructure for Pruning:\n",
            "================================================================================\n",
            "Method 2: INT4 + Channel-wise KV Pruning + KV Quantization (Batch Test)\n",
            "================================================================================\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "Loading checkpoint shards: 100% 3/3 [00:17<00:00,  5.67s/it]\n",
            "INT4 model loaded (bitsandbytes).\n",
            "\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "\n",
            "==============================\n",
            "  Batch Size Stress Test (channelwise)\n",
            "==============================\n",
            "\n",
            "--- Batch Size = 4 ---\n",
            " ==> BS=4, latency=5.519s, throughput=471.06 tok/s, max_mem=8.70 GB\n",
            "\n",
            "--- Batch Size = 8 ---\n",
            " ==> BS=8, latency=5.809s, throughput=895.10 tok/s, max_mem=13.09 GB\n",
            "\n",
            "--- Batch Size = 16 ---\n",
            " ==> BS=16, latency=6.294s, throughput=1652.38 tok/s, max_mem=21.81 GB\n",
            "\n",
            "--- Batch Size = 32 ---\n",
            " ==> BS=32, latency=7.315s, throughput=2843.46 tok/s, max_mem=39.28 GB\n",
            "\n",
            "Method 2 Batch Results (INT4 + Channel-wise Prune + KV Quant):\n",
            "  BS=4: Latency=5.519s, Throughput=471.06 tok/s, Memory=8.70 GB\n",
            "  BS=8: Latency=5.809s, Throughput=895.10 tok/s, Memory=13.09 GB\n",
            "  BS=16: Latency=6.294s, Throughput=1652.38 tok/s, Memory=21.81 GB\n",
            "  BS=32: Latency=7.315s, Throughput=2843.46 tok/s, Memory=39.28 GB\n",
            "\n",
            "\n",
            "disable quantization for Pruning:\n",
            "================================================================================\n",
            "Method 2: INT4 + Channel-wise KV Pruning + KV Quantization (Batch Test)\n",
            "================================================================================\n",
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "Loading checkpoint shards: 100% 3/3 [00:17<00:00,  5.71s/it]\n",
            "INT4 model loaded (bitsandbytes).\n",
            "\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "\n",
            "==============================\n",
            "  Batch Size Stress Test (channelwise)\n",
            "==============================\n",
            "\n",
            "--- Batch Size = 4 ---\n",
            " ==> BS=4, latency=5.489s, throughput=473.70 tok/s, max_mem=8.08 GB\n",
            "\n",
            "--- Batch Size = 8 ---\n",
            " ==> BS=8, latency=5.848s, throughput=889.22 tok/s, max_mem=11.84 GB\n",
            "\n",
            "--- Batch Size = 16 ---\n",
            " ==> BS=16, latency=6.301s, throughput=1650.48 tok/s, max_mem=19.34 GB\n",
            "\n",
            "--- Batch Size = 32 ---\n",
            " ==> BS=32, latency=7.312s, throughput=2844.45 tok/s, max_mem=34.36 GB\n",
            "\n",
            "Method 2 Batch Results (INT4 + Channel-wise Prune + KV Quant):\n",
            "  BS=4: Latency=5.489s, Throughput=473.70 tok/s, Memory=8.08 GB\n",
            "  BS=8: Latency=5.848s, Throughput=889.22 tok/s, Memory=11.84 GB\n",
            "  BS=16: Latency=6.301s, Throughput=1650.48 tok/s, Memory=19.34 GB\n",
            "  BS=32: Latency=7.312s, Throughput=2844.45 tok/s, Memory=34.36 GB\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python int8_channel_prune_quant_batch.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LEvKrie0E-e2",
        "outputId": "8e0e4357-1568-48e3-eb56-af862e231cf3"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-06 04:08:37.114661: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-06 04:08:37.132460: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764994117.154011   11533 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764994117.160459   11533 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764994117.176957   11533 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764994117.176991   11533 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764994117.176994   11533 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764994117.176997   11533 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-06 04:08:37.181855: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\n",
            "Using unstructure for Pruning:\n",
            "================================================================================\n",
            "Method 2: INT8 + Channel-wise KV Pruning + KV Quantization (Batch Test)\n",
            "================================================================================\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "Loading checkpoint shards: 100% 3/3 [00:16<00:00,  5.65s/it]\n",
            "INT8 model loaded (bitsandbytes).\n",
            "\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "\n",
            "==============================\n",
            "  Batch Size Stress Test (channelwise)\n",
            "==============================\n",
            "\n",
            "--- Batch Size = 4 ---\n",
            " ==> BS=4, latency=5.437s, throughput=459.09 tok/s, max_mem=11.22 GB\n",
            "\n",
            "--- Batch Size = 8 ---\n",
            " ==> BS=8, latency=17.328s, throughput=319.49 tok/s, max_mem=15.85 GB\n",
            "\n",
            "--- Batch Size = 16 ---\n",
            " ==> BS=16, latency=18.259s, throughput=606.38 tok/s, max_mem=24.74 GB\n",
            "\n",
            "--- Batch Size = 32 ---\n",
            " ==> BS=32, latency=20.685s, throughput=1070.56 tok/s, max_mem=42.67 GB\n",
            "\n",
            "Method 2 Batch Results (INT8 + Channel-wise Prune + KV Quant):\n",
            "  BS=4: Latency=5.437s, Throughput=459.09 tok/s, Memory=11.22 GB\n",
            "  BS=8: Latency=17.328s, Throughput=319.49 tok/s, Memory=15.85 GB\n",
            "  BS=16: Latency=18.259s, Throughput=606.38 tok/s, Memory=24.74 GB\n",
            "  BS=32: Latency=20.685s, Throughput=1070.56 tok/s, Memory=42.67 GB\n",
            "\n",
            "\n",
            "disable quantization for Pruning:\n",
            "================================================================================\n",
            "Method 2: INT8 + Channel-wise KV Pruning + KV Quantization (Batch Test)\n",
            "================================================================================\n",
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "Loading checkpoint shards: 100% 3/3 [00:16<00:00,  5.65s/it]\n",
            "INT8 model loaded (bitsandbytes).\n",
            "\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "\n",
            "==============================\n",
            "  Batch Size Stress Test (channelwise)\n",
            "==============================\n",
            "\n",
            "--- Batch Size = 4 ---\n",
            " ==> BS=4, latency=5.415s, throughput=460.91 tok/s, max_mem=10.61 GB\n",
            "\n",
            "--- Batch Size = 8 ---\n",
            " ==> BS=8, latency=17.357s, throughput=318.96 tok/s, max_mem=14.61 GB\n",
            "\n",
            "--- Batch Size = 16 ---\n",
            " ==> BS=16, latency=18.185s, throughput=608.85 tok/s, max_mem=22.28 GB\n",
            "\n",
            "--- Batch Size = 32 ---\n",
            " ==> BS=32, latency=20.599s, throughput=1075.00 tok/s, max_mem=37.75 GB\n",
            "\n",
            "Method 2 Batch Results (INT8 + Channel-wise Prune + KV Quant):\n",
            "  BS=4: Latency=5.415s, Throughput=460.91 tok/s, Memory=10.61 GB\n",
            "  BS=8: Latency=17.357s, Throughput=318.96 tok/s, Memory=14.61 GB\n",
            "  BS=16: Latency=18.185s, Throughput=608.85 tok/s, Memory=22.28 GB\n",
            "  BS=32: Latency=20.599s, Throughput=1075.00 tok/s, Memory=37.75 GB\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python channel_prune_quant_batch.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UEiVgKw3FtwW",
        "outputId": "a97ce8fe-9c79-4950-d95e-80500631ba44"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-06 04:12:05.876139: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-06 04:12:05.893776: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764994325.915758   12534 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764994325.922244   12534 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764994325.939039   12534 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764994325.939082   12534 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764994325.939084   12534 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764994325.939087   12534 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-06 04:12:05.944203: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\n",
            "Using unstructure for Pruning:\n",
            "Loading sample image...\n",
            "Image size: (1000, 667)\n",
            "\n",
            "================================================================================\n",
            "Method 2: Channel-wise KV Pruning + KV Quantization (Batch Test)\n",
            "================================================================================\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Loading checkpoint shards: 100% 3/3 [00:04<00:00,  1.46s/it]\n",
            "LLava fp16 model loaded.\n",
            "\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "\n",
            "==============================\n",
            "  Batch Size Stress Test (channelwise)\n",
            "==============================\n",
            "\n",
            "--- Batch Size = 4 ---\n",
            " ==> BS=4, latency=1.447s, throughput=1724.47 tok/s, max_mem=17.53 GB\n",
            "\n",
            "--- Batch Size = 8 ---\n",
            " ==> BS=8, latency=4.485s, throughput=1234.43 tok/s, max_mem=22.16 GB\n",
            "\n",
            "--- Batch Size = 16 ---\n",
            " ==> BS=16, latency=1.464s, throughput=6818.94 tok/s, max_mem=30.51 GB\n",
            "\n",
            "--- Batch Size = 32 ---\n",
            " ==> BS=32, latency=6.405s, throughput=3457.34 tok/s, max_mem=48.98 GB\n",
            "\n",
            "Method 2 Batch Results (Channel-wise Prune + KV Quant):\n",
            "  BS=4: Latency=1.447s, Throughput=1724.47 tok/s, Memory=17.53 GB\n",
            "  BS=8: Latency=4.485s, Throughput=1234.43 tok/s, Memory=22.16 GB\n",
            "  BS=16: Latency=1.464s, Throughput=6818.94 tok/s, Memory=30.51 GB\n",
            "  BS=32: Latency=6.405s, Throughput=3457.34 tok/s, Memory=48.98 GB\n",
            "\n",
            "\n",
            "disable quantization for Pruning:\n",
            "Loading sample image...\n",
            "Image size: (1000, 667)\n",
            "\n",
            "================================================================================\n",
            "Method 2: Channel-wise KV Pruning + KV Quantization (Batch Test)\n",
            "================================================================================\n",
            "Loading checkpoint shards: 100% 3/3 [00:04<00:00,  1.45s/it]\n",
            "LLava fp16 model loaded.\n",
            "\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "\n",
            "==============================\n",
            "  Batch Size Stress Test (channelwise)\n",
            "==============================\n",
            "\n",
            "--- Batch Size = 4 ---\n",
            " ==> BS=4, latency=4.395s, throughput=629.85 tok/s, max_mem=17.04 GB\n",
            "\n",
            "--- Batch Size = 8 ---\n",
            " ==> BS=8, latency=4.441s, throughput=1246.59 tok/s, max_mem=20.93 GB\n",
            "\n",
            "--- Batch Size = 16 ---\n",
            " ==> BS=16, latency=4.527s, throughput=2445.70 tok/s, max_mem=28.59 GB\n",
            "\n",
            "--- Batch Size = 32 ---\n",
            " ==> BS=32, latency=6.226s, throughput=3556.78 tok/s, max_mem=44.06 GB\n",
            "\n",
            "Method 2 Batch Results (Channel-wise Prune + KV Quant):\n",
            "  BS=4: Latency=4.395s, Throughput=629.85 tok/s, Memory=17.04 GB\n",
            "  BS=8: Latency=4.441s, Throughput=1246.59 tok/s, Memory=20.93 GB\n",
            "  BS=16: Latency=4.527s, Throughput=2445.70 tok/s, Memory=28.59 GB\n",
            "  BS=32: Latency=6.226s, Throughput=3556.78 tok/s, Memory=44.06 GB\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python test_bitsandbytes_batch.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lK48zSaRS74Q",
        "outputId": "068f6c5e-ddfe-447a-f7dc-149ad48ababe"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-06 04:13:29.895095: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-06 04:13:29.913078: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764994409.935584   12969 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764994409.942162   12969 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764994409.958851   12969 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764994409.958889   12969 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764994409.958892   12969 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764994409.958895   12969 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-06 04:13:29.963918: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Loading sample image...\n",
            "Image size: (1000, 667)\n",
            "\n",
            "================================================================================\n",
            "8bit batch Test for LLaVA\n",
            "================================================================================\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "Loading checkpoint shards: 100% 3/3 [00:16<00:00,  5.65s/it]\n",
            "Model loaded.\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "\n",
            "==============================\n",
            "  Batch Size Stress Test (true batch forward)\n",
            "==============================\n",
            "\n",
            "--- Batch Size = 4 (1 forward pass with 4 images) ---\n",
            " ==> BS=4: latency=18.070s, throughput=153.63 tok/s, max mem=8.32 GB\n",
            "\n",
            "--- Batch Size = 8 (1 forward pass with 8 images) ---\n",
            " ==> BS=8: latency=17.228s, throughput=322.26 tok/s, max mem=9.78 GB\n",
            "\n",
            "--- Batch Size = 16 (1 forward pass with 16 images) ---\n",
            " ==> BS=16: latency=18.635s, throughput=595.86 tok/s, max mem=12.71 GB\n",
            "\n",
            "--- Batch Size = 32 (1 forward pass with 32 images) ---\n",
            " ==> BS=32: latency=21.037s, throughput=1055.67 tok/s, max mem=18.56 GB\n",
            "\"8bit Batch Results:\n",
            "  BS=4: Latency=18.070s, Throughput=153.63 tok/s, Memory=8.32 GB\n",
            "  BS=8: Latency=17.228s, Throughput=322.26 tok/s, Memory=9.78 GB\n",
            "  BS=16: Latency=18.635s, Throughput=595.86 tok/s, Memory=12.71 GB\n",
            "  BS=32: Latency=21.037s, Throughput=1055.67 tok/s, Memory=18.56 GB\n"
          ]
        }
      ]
    }
  ]
}