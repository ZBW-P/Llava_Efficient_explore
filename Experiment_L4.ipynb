{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "runtime_attributes": {
        "runtime_version": "2025.10"
      }
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Single Test"
      ],
      "metadata": {
        "id": "y-6XyOu-y4bJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers accelerate pillow torch\n",
        "!pip install -q flash-attn --no-build-isolation"
      ],
      "metadata": {
        "id": "pCg-P6EKcHuQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0808657-b36a-4dbc-ea97-80727cb5409b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/8.4 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m261.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m162.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for flash-attn (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llmcompressor datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I6VrjHIVYFyZ",
        "outputId": "6fd199d4-60b3-4d8b-bf24-f09f181c84cd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llmcompressor\n",
            "  Downloading llmcompressor-0.8.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Collecting loguru<=0.7.3,>=0.7.2 (from llmcompressor)\n",
            "  Downloading loguru-0.7.3-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: pyyaml<=6.0.3,>=6.0.1 in /usr/local/lib/python3.12/dist-packages (from llmcompressor) (6.0.3)\n",
            "Requirement already satisfied: numpy<=2.3.3,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from llmcompressor) (2.0.2)\n",
            "Requirement already satisfied: requests<=2.32.5,>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from llmcompressor) (2.32.4)\n",
            "Requirement already satisfied: tqdm<=4.67.1,>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from llmcompressor) (4.67.1)\n",
            "Requirement already satisfied: torch<=2.8.0,>=2.7.0 in /usr/local/lib/python3.12/dist-packages (from llmcompressor) (2.8.0+cu126)\n",
            "Collecting transformers<=4.56.2,>=4.53.0 (from llmcompressor)\n",
            "  Downloading transformers-4.56.2-py3-none-any.whl.metadata (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.1/40.1 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: accelerate<=1.10.1,>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from llmcompressor) (1.10.1)\n",
            "Requirement already satisfied: nvidia-ml-py<=13.580.82,>=12.560.30 in /usr/local/lib/python3.12/dist-packages (from llmcompressor) (12.575.51)\n",
            "Requirement already satisfied: pillow<=11.3.0,>=10.4.0 in /usr/local/lib/python3.12/dist-packages (from llmcompressor) (11.3.0)\n",
            "Collecting compressed-tensors==0.12.2 (from llmcompressor)\n",
            "  Downloading compressed_tensors-0.12.2-py3-none-any.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.12/dist-packages (from compressed-tensors==0.12.2->llmcompressor) (2.11.10)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.35.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate<=1.10.1,>=1.6.0->llmcompressor) (5.9.5)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate<=1.10.1,>=1.6.0->llmcompressor) (0.6.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.10)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<=2.32.5,>=2.32.2->llmcompressor) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<=2.32.5,>=2.32.2->llmcompressor) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<=2.32.5,>=2.32.2->llmcompressor) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<=2.32.5,>=2.32.2->llmcompressor) (2025.10.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (3.4.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<=4.56.2,>=4.53.0->llmcompressor) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<=4.56.2,>=4.53.0->llmcompressor) (0.22.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0->compressed-tensors==0.12.2->llmcompressor) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0->compressed-tensors==0.12.2->llmcompressor) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0->compressed-tensors==0.12.2->llmcompressor) (0.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<=2.8.0,>=2.7.0->llmcompressor) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<=2.8.0,>=2.7.0->llmcompressor) (3.0.3)\n",
            "Downloading llmcompressor-0.8.1-py3-none-any.whl (273 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m273.2/273.2 kB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading compressed_tensors-0.12.2-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.0/183.0 kB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading loguru-0.7.3-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformers-4.56.2-py3-none-any.whl (11.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m133.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: loguru, transformers, compressed-tensors, llmcompressor\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.57.0\n",
            "    Uninstalling transformers-4.57.0:\n",
            "      Successfully uninstalled transformers-4.57.0\n",
            "Successfully installed compressed-tensors-0.12.2 llmcompressor-0.8.1 loguru-0.7.3 transformers-4.56.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U bitsandbytes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_tuG642YIRs",
        "outputId": "da0cf6aa-9781-4116-848e-002cfe6b1d0a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: torch<3,>=2.3 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.8.0+cu126)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (25.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.3->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.3)\n",
            "Downloading bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl (59.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m54.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bitsandbytes\n",
            "Successfully installed bitsandbytes-0.48.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6RPnK-KkbI1D",
        "outputId": "fda610f2-bf9d-4e32-fde7-3f06065ba785"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.12.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python test_baseline.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZV_YISLiWAr0",
        "outputId": "264b1e49-7761-40d6-898b-9df5ce79c14b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-06 02:22:08.737190: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-06 02:22:08.754706: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764987728.775909    1030 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764987728.782287    1030 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764987728.798382    1030 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764987728.798412    1030 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764987728.798415    1030 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764987728.798418    1030 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-06 02:22:08.803213: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Loading sample image...\n",
            "Image size: (1000, 667)\n",
            "\n",
            "================================================================================\n",
            "Baseline_without_kv_cache Test for LLaVA\n",
            "================================================================================\n",
            "processor_config.json: 100% 173/173 [00:00<00:00, 1.05MB/s]\n",
            "chat_template.json: 100% 701/701 [00:00<00:00, 5.42MB/s]\n",
            "chat_template.jinja: 100% 674/674 [00:00<00:00, 3.46MB/s]\n",
            "preprocessor_config.json: 100% 505/505 [00:00<00:00, 3.28MB/s]\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "tokenizer_config.json: 1.45kB [00:00, 8.36MB/s]\n",
            "tokenizer.model: 100% 500k/500k [00:01<00:00, 338kB/s]\n",
            "tokenizer.json: 3.62MB [00:00, 200MB/s]\n",
            "added_tokens.json: 100% 41.0/41.0 [00:00<00:00, 285kB/s]\n",
            "special_tokens_map.json: 100% 552/552 [00:00<00:00, 4.77MB/s]\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "config.json: 100% 950/950 [00:00<00:00, 11.4MB/s]\n",
            "model.safetensors.index.json: 70.1kB [00:00, 185MB/s]\n",
            "Fetching 3 files:   0% 0/3 [00:00<?, ?it/s]\n",
            "model-00002-of-00003.safetensors:   0% 0.00/4.96G [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:   0% 0.00/4.99G [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:   0% 0.00/4.18G [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:   0% 1.35M/4.96G [00:01<1:38:13, 841kB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:   0% 1.95M/4.96G [00:01<1:11:52, 1.15MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:   1% 69.0M/4.96G [00:02<01:32, 52.8MB/s]  \u001b[A\n",
            "model-00002-of-00003.safetensors:   4% 203M/4.96G [00:02<00:43, 109MB/s]  \u001b[A\n",
            "model-00002-of-00003.safetensors:  11% 538M/4.96G [00:04<00:24, 183MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:   0% 11.6k/4.99G [00:05<614:08:50, 2.26kB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  14% 672M/4.96G [00:05<00:31, 137MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  18% 873M/4.96G [00:06<00:23, 176MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  19% 940M/4.96G [00:10<00:52, 76.0MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:   0% 546k/4.18G [00:12<26:52:35, 43.2kB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  22% 1.07G/4.96G [00:12<00:58, 66.2MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:   0% 25.1k/4.99G [00:12<718:28:03, 1.93kB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  26% 1.28G/4.96G [00:12<00:34, 107MB/s] \u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:   2% 67.6M/4.18G [00:12<09:15, 7.40MB/s]  \u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  28% 1.41G/4.96G [00:13<00:25, 138MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  32% 1.61G/4.96G [00:13<00:17, 191MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:   1% 28.8M/4.99G [00:13<25:38, 3.23MB/s]    \u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  37% 1.81G/4.96G [00:14<00:14, 219MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:   3% 135M/4.18G [00:14<04:27, 15.1MB/s] \u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  38% 1.88G/4.96G [00:14<00:16, 191MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:   5% 202M/4.18G [00:14<02:45, 24.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:   2% 92.9M/4.99G [00:14<07:12, 11.3MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  41% 2.01G/4.96G [00:15<00:13, 226MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:   6% 269M/4.18G [00:15<01:54, 34.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:   8% 336M/4.18G [00:16<01:24, 45.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  13% 537M/4.18G [00:17<00:39, 91.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  43% 2.15G/4.96G [00:17<00:22, 126MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  16% 671M/4.18G [00:17<00:26, 132MB/s] \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  19% 807M/4.18G [00:18<00:25, 133MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  26% 1.07G/4.18G [00:18<00:14, 216MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  45% 2.21G/4.96G [00:19<00:32, 83.9MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  29% 1.21G/4.18G [00:19<00:14, 205MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:   3% 135M/4.99G [00:19<07:56, 10.2MB/s] \u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  46% 2.28G/4.96G [00:19<00:28, 93.9MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  31% 1.28G/4.18G [00:20<00:20, 144MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  32% 1.34G/4.18G [00:23<00:36, 77.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:   3% 168M/4.99G [00:23<08:22, 9.59MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  47% 2.35G/4.96G [00:23<00:55, 47.1MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  49% 2.42G/4.96G [00:23<00:43, 58.4MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:   5% 236M/4.99G [00:24<04:43, 16.8MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  50% 2.48G/4.96G [00:24<00:38, 64.3MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  53% 2.62G/4.96G [00:24<00:23, 102MB/s] \u001b[A\n",
            "model-00002-of-00003.safetensors:  54% 2.68G/4.96G [00:26<00:31, 71.8MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:   5% 274M/4.99G [00:27<04:54, 16.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:   8% 408M/4.99G [00:27<02:06, 36.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  34% 1.41G/4.18G [00:27<01:03, 43.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  56% 2.75G/4.96G [00:28<00:33, 66.2MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  58% 2.89G/4.96G [00:28<00:22, 90.7MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:   9% 469M/4.99G [00:28<02:00, 37.6MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  62% 3.09G/4.96G [00:29<00:12, 145MB/s] \u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  35% 1.48G/4.18G [00:29<01:07, 40.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  64% 3.16G/4.96G [00:29<00:13, 139MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  37% 1.54G/4.18G [00:29<00:54, 48.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  40% 1.68G/4.18G [00:30<00:34, 71.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  43% 1.81G/4.18G [00:31<00:24, 97.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  65% 3.22G/4.96G [00:31<00:19, 90.2MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  47% 1.95G/4.18G [00:31<00:17, 129MB/s] \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  51% 2.15G/4.18G [00:31<00:10, 185MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  66% 3.29G/4.96G [00:33<00:26, 63.0MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  55% 2.28G/4.18G [00:34<00:19, 97.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  68% 3.36G/4.96G [00:35<00:31, 50.1MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  58% 2.41G/4.18G [00:36<00:18, 96.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  61% 2.55G/4.18G [00:36<00:12, 126MB/s] \u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  69% 3.42G/4.96G [00:37<00:29, 51.2MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  11% 537M/4.99G [00:37<04:14, 17.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  63% 2.62G/4.18G [00:38<00:19, 81.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  13% 637M/4.99G [00:39<03:09, 22.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  66% 2.75G/4.18G [00:39<00:14, 96.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  70% 3.49G/4.96G [00:39<00:37, 39.5MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  72% 3.56G/4.96G [00:40<00:26, 52.0MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  68% 2.82G/4.18G [00:40<00:14, 95.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  73% 3.62G/4.96G [00:40<00:21, 61.7MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  69% 2.89G/4.18G [00:41<00:13, 98.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  74% 3.69G/4.96G [00:41<00:17, 71.9MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  71% 2.95G/4.18G [00:41<00:10, 114MB/s] \u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  76% 3.76G/4.96G [00:41<00:13, 89.1MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  72% 3.02G/4.18G [00:42<00:10, 112MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  74% 3.09G/4.18G [00:42<00:10, 101MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  79% 3.89G/4.96G [00:43<00:12, 84.4MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  80% 3.96G/4.96G [00:43<00:09, 106MB/s] \u001b[A\n",
            "model-00002-of-00003.safetensors:  81% 4.03G/4.96G [00:43<00:06, 134MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  77% 3.22G/4.18G [00:43<00:07, 125MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  79% 3.29G/4.18G [00:43<00:05, 148MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  83% 4.09G/4.96G [00:43<00:06, 130MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  84% 4.16G/4.96G [00:44<00:05, 157MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  85% 4.23G/4.96G [00:44<00:03, 184MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  80% 3.36G/4.18G [00:44<00:06, 128MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  14% 706M/4.99G [00:44<03:40, 19.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  82% 3.42G/4.18G [00:44<00:05, 138MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  15% 774M/4.99G [00:45<02:53, 24.3MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  87% 4.29G/4.96G [00:46<00:09, 67.6MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  84% 3.49G/4.18G [00:47<00:11, 61.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  17% 841M/4.99G [00:48<02:55, 23.7MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  88% 4.36G/4.96G [00:50<00:14, 41.4MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  18% 908M/4.99G [00:50<02:27, 27.7MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  89% 4.43G/4.96G [00:50<00:10, 52.5MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  20% 975M/4.99G [00:50<01:49, 36.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  85% 3.56G/4.18G [00:50<00:15, 40.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  91% 4.49G/4.96G [00:50<00:07, 64.7MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  88% 3.69G/4.18G [00:51<00:07, 63.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  92% 4.56G/4.96G [00:51<00:05, 74.6MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  90% 3.76G/4.18G [00:51<00:05, 76.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  93% 4.63G/4.96G [00:51<00:03, 89.1MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  92% 3.83G/4.18G [00:52<00:04, 85.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  95% 4.69G/4.96G [00:52<00:02, 106MB/s] \u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  93% 3.89G/4.18G [00:52<00:02, 110MB/s] \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  21% 1.04G/4.99G [00:52<01:52, 35.0MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  96% 4.76G/4.96G [00:53<00:01, 98.6MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  95% 3.96G/4.18G [00:53<00:02, 100MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  96% 4.03G/4.18G [00:53<00:01, 130MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  98% 4.09G/4.18G [00:53<00:00, 162MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors:  97% 4.82G/4.96G [00:54<00:01, 83.1MB/s]\u001b[A\n",
            "model-00002-of-00003.safetensors:  99% 4.89G/4.96G [00:54<00:00, 89.0MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  21% 1.07G/4.99G [00:54<02:24, 27.1MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00003.safetensors: 100% 4.96G/4.96G [00:55<00:00, 89.6MB/s]\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  22% 1.07G/4.99G [00:55<02:30, 26.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors: 100% 4.18G/4.18G [00:55<00:00, 74.9MB/s]\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  23% 1.14G/4.99G [00:56<01:43, 37.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  24% 1.21G/4.99G [00:56<01:17, 48.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  26% 1.27G/4.99G [00:56<00:54, 68.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  27% 1.34G/4.99G [00:57<00:38, 94.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  28% 1.41G/4.99G [00:57<00:30, 116MB/s] \u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  30% 1.48G/4.99G [00:58<00:33, 103MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  34% 1.68G/4.99G [00:58<00:15, 220MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  35% 1.74G/4.99G [00:58<00:14, 221MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  38% 1.88G/4.99G [00:59<00:15, 196MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  39% 1.95G/4.99G [01:01<00:35, 86.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  40% 2.01G/4.99G [01:02<00:38, 78.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  42% 2.08G/4.99G [01:03<00:30, 97.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  43% 2.15G/4.99G [01:03<00:25, 113MB/s] \u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  44% 2.22G/4.99G [01:03<00:19, 142MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  46% 2.28G/4.99G [01:04<00:19, 137MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  48% 2.42G/4.99G [01:04<00:12, 213MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  51% 2.56G/4.99G [01:04<00:09, 266MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  53% 2.62G/4.99G [01:05<00:11, 203MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  54% 2.69G/4.99G [01:05<00:13, 170MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  55% 2.76G/4.99G [01:06<00:14, 150MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  57% 2.83G/4.99G [01:06<00:13, 164MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  58% 2.89G/4.99G [01:07<00:11, 185MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  59% 2.96G/4.99G [01:07<00:09, 225MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  61% 3.03G/4.99G [01:07<00:07, 261MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  62% 3.09G/4.99G [01:07<00:06, 294MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  63% 3.16G/4.99G [01:08<00:12, 144MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  65% 3.23G/4.99G [01:09<00:14, 124MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  66% 3.30G/4.99G [01:09<00:14, 120MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  67% 3.36G/4.99G [01:10<00:11, 140MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  69% 3.43G/4.99G [01:10<00:09, 160MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  70% 3.50G/4.99G [01:10<00:08, 182MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  71% 3.56G/4.99G [01:10<00:07, 201MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  74% 3.70G/4.99G [01:11<00:04, 273MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  75% 3.76G/4.99G [01:11<00:05, 220MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  79% 3.96G/4.99G [01:11<00:02, 385MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  82% 4.09G/4.99G [01:12<00:02, 411MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  86% 4.29G/4.99G [01:12<00:01, 481MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  89% 4.43G/4.99G [01:12<00:01, 443MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  91% 4.56G/4.99G [01:13<00:01, 406MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  93% 4.63G/4.99G [01:13<00:00, 380MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  95% 4.76G/4.99G [01:13<00:00, 402MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  97% 4.86G/4.99G [01:13<00:00, 399MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors: 100% 4.99G/4.99G [01:14<00:00, 67.0MB/s]\n",
            "Fetching 3 files: 100% 3/3 [01:15<00:00, 25.04s/it]\n",
            "Loading checkpoint shards: 100% 3/3 [00:04<00:00,  1.48s/it]\n",
            "generation_config.json: 100% 141/141 [00:00<00:00, 1.24MB/s]\n",
            "Model loaded.\n",
            "Running 3 inference passes...\n",
            " Run 1/3: 12.08 tokens/sec, 13.52 GB\n",
            " Run 2/3: 16.57 tokens/sec, 13.52 GB\n",
            " Run 3/3: 16.55 tokens/sec, 13.52 GB\n",
            "\n",
            "kv_cache_results Results:\n",
            "  Average Latency: 6.785 seconds\n",
            "  Average Throughput: 15.07 tokens/sec\n",
            "  Average Memory: 13.52 GB\n",
            "\n",
            "Sample Output:\n",
            "USER:  \n",
            "Describe this image in detail.\n",
            "ASSISTANT: The image features a pier extending out into a large body of water, likely a lake. The pier is made of wood and has a bench situated on it, providing ...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python test_flash_attn.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJZATiT0eCXB",
        "outputId": "e4048371-a43c-42c2-897f-aac738402ed8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-06 02:25:06.790038: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-06 02:25:06.807269: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764987906.828304    1876 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764987906.834662    1876 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764987906.850706    1876 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764987906.850734    1876 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764987906.850737    1876 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764987906.850740    1876 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-06 02:25:06.855571: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Loading sample image...\n",
            "Image size: (1000, 667)\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "TEST 2: With Flash Attention 2\n",
            "--------------------------------------------------------------------------------\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Loading checkpoint shards: 100% 3/3 [00:04<00:00,  1.55s/it]\n",
            "Running 3 inference passes...\n",
            " Run 1/3: 12.67 tokens/sec, 13.52 GB\n",
            " Run 2/3: 14.11 tokens/sec, 13.52 GB\n",
            " Run 3/3: 14.12 tokens/sec, 13.52 GB\n",
            "\n",
            "Flash Attention 2 Results:\n",
            "  Average Latency: 7.354 seconds\n",
            "  Average Throughput: 13.63 tokens/sec\n",
            "  Average Memory: 13.52 GB\n",
            "\n",
            "Sample Output:\n",
            "USER:  \n",
            "Describe this image in detail.\n",
            "ASSISTANT: The image features a pier extending out into a large body of water, likely a lake. The pier is made of wood and has a bench situated on it, providing ...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python test_bitsandbytes.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vW2O-BceeFvK",
        "outputId": "fd85a7d6-000a-4a29-fa74-92fc4023a54a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-06 02:26:00.671665: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-06 02:26:00.688963: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764987960.709983    2158 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764987960.716352    2158 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764987960.732366    2158 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764987960.732392    2158 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764987960.732395    2158 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764987960.732398    2158 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-06 02:26:00.737126: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Loading sample image...\n",
            "Image size: (1000, 667)\n",
            "\n",
            "================================================================================\n",
            "bitsandbytes Quantization 4 bits Test for LLaVA\n",
            "================================================================================\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "Loading checkpoint shards: 100% 3/3 [00:16<00:00,  5.58s/it]\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "\n",
            "AWQ bitsandbytes quant 4 bit complete. Loading quantized model...\n",
            "\n",
            "Running 3 inference passes...\n",
            " Run 1/3: 13.18 tokens/sec, 4.69 GB\n",
            " Run 2/3: 14.34 tokens/sec, 4.69 GB\n",
            " Run 3/3: 14.23 tokens/sec, 4.69 GB\n",
            "\n",
            "bitsandbytes Results:\n",
            "  Avg Latency: 7.196 seconds\n",
            "  Avg Throughput: 13.92 tokens/sec\n",
            "  Avg Memory: 4.69 GB\n",
            "\n",
            "Sample Output:\n",
            "USER:  \n",
            "Describe this image in detail.\n",
            "ASSISTANT: The image features a wooden pier extending out over a body of water. The pier is situated in a serene, natural setting with a mountain in the backgrou...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python test_kv.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aik6Mkopek8e",
        "outputId": "ee7b8561-9173-4fbb-bb87-323f27bf4322"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-06 02:31:48.996823: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-06 02:31:49.014555: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764988309.036035    3695 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764988309.042477    3695 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764988309.058848    3695 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764988309.058885    3695 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764988309.058888    3695 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764988309.058891    3695 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-06 02:31:49.063710: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Loading sample image...\n",
            "Image size: (1000, 667)\n",
            "\n",
            "================================================================================\n",
            "KV_cache Test for LLaVA\n",
            "================================================================================\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Loading checkpoint shards: 100% 3/3 [00:04<00:00,  1.37s/it]\n",
            "Model loaded.\n",
            "Running 3 inference passes...\n",
            " Run 1/3: 105.69 tokens/sec, 13.55 GB\n",
            " Run 2/3: 108.71 tokens/sec, 13.55 GB\n",
            " Run 3/3: 108.67 tokens/sec, 13.55 GB\n",
            "\n",
            "kv_cache Results:\n",
            "  Average Latency: 6.446 seconds\n",
            "  Average Throughput: 107.69 tokens/sec\n",
            "  Average Memory: 13.55 GB\n",
            "\n",
            "Sample Output:\n",
            "USER:  \n",
            "Describe this image in detail.\n",
            "ASSISTANT: The image features a pier with a wooden walkway extending out into the water. The pier is situated near a mountain, providing a picturesque backdrop. ...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python test_kv_prune_quant.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H55M7Pi8yexw",
        "outputId": "3fc2b539-5c30-4c02-927b-73b4bb7d3b22"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-06 02:32:32.976916: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-06 02:32:32.993864: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764988353.014764    3935 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764988353.021170    3935 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764988353.037238    3935 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764988353.037267    3935 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764988353.037270    3935 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764988353.037273    3935 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-06 02:32:33.042031: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Loading sample image...\n",
            "Image size: (1000, 667)\n",
            "\n",
            "================================================================================\n",
            "Prun + Quantization KV_cache Test for LLaVA\n",
            "================================================================================\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Loading checkpoint shards: 100% 3/3 [00:04<00:00,  1.38s/it]\n",
            "Model loaded.\n",
            "Running 3 inference passes...\n",
            " Run 1/3: 108.01 tokens/sec, 14.29 GB\n",
            " Run 2/3: 108.62 tokens/sec, 14.29 GB\n",
            " Run 3/3: 108.35 tokens/sec, 14.29 GB\n",
            "\n",
            "Prun + Quant_kv_cache Results:\n",
            "  Average Latency: 6.406 seconds\n",
            "  Average Throughput: 108.33 tokens/sec\n",
            "  Average Memory: 14.29 GB\n",
            "\n",
            "Sample Output:\n",
            "USER:  \n",
            "Describe this image in detail.\n",
            "ASSISTANT: The image features a pier or dock extending over a body of water, likely a lake or a large pond. The pier or dock is situated on the water's edge, pro...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python test_kv_prune.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YPBK2sapy0zb",
        "outputId": "38587d3f-5fa5-4bef-80e8-1c85b317747f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-06 02:33:17.137988: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-06 02:33:17.154771: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764988397.176076    4165 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764988397.182714    4165 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764988397.198470    4165 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764988397.198496    4165 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764988397.198499    4165 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764988397.198502    4165 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-06 02:33:17.203278: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Loading sample image...\n",
            "Image size: (1000, 667)\n",
            "\n",
            "================================================================================\n",
            "Prun KV_cache Test for LLaVA\n",
            "================================================================================\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Loading checkpoint shards: 100% 3/3 [00:04<00:00,  1.38s/it]\n",
            "Model loaded.\n",
            "Running 3 inference passes...\n",
            " Run 1/3: 202.05 tokens/sec, 14.11 GB\n",
            " Run 2/3: 205.37 tokens/sec, 14.11 GB\n",
            " Run 3/3: 205.57 tokens/sec, 14.11 GB\n",
            "\n",
            "Prun_kv_cacheresults Results:\n",
            "  Average Latency: 3.147 seconds\n",
            "  Average Throughput: 204.33 tokens/sec\n",
            "  Average Memory: 14.11 GB\n",
            "\n",
            "Sample Output:\n",
            "USER:  \n",
            "Describe this image in detail.\n",
            "ASSISTANT:The image shows a pier extending out into the water. The pier is situated on the water's edge, and it is a long pier that extends out into the water. T...\n",
            "\n",
            "Loading sample image...\n",
            "Image size: (850, 567)\n",
            "\n",
            "================================================================================\n",
            "Prun KV_cache Test for LLaVA\n",
            "================================================================================\n",
            "Loading checkpoint shards: 100% 3/3 [00:04<00:00,  1.39s/it]\n",
            "Model loaded.\n",
            "Running 3 inference passes...\n",
            " Run 1/3: 191.44 tokens/sec, 14.11 GB\n",
            " Run 2/3: 191.16 tokens/sec, 14.11 GB\n",
            " Run 3/3: 191.27 tokens/sec, 14.11 GB\n",
            "\n",
            "Prun_kv_cacheresults Results:\n",
            "  Average Latency: 3.382 seconds\n",
            "  Average Throughput: 191.29 tokens/sec\n",
            "  Average Memory: 14.11 GB\n",
            "\n",
            "Sample Output:\n",
            "USER:  \n",
            "Describe this image in detail.\n",
            "ASSISTANT:The image shows a close-up of a cat's face, with its eyes, nose, and mouth clearly visible.\n",
            ": The cat's face is the main focus of the image, with its e...\n",
            "\n",
            "Loading sample image...\n",
            "Image size: (850, 567)\n",
            "\n",
            "================================================================================\n",
            "Prun KV_cache Test for LLaVA\n",
            "================================================================================\n",
            "Loading checkpoint shards: 100% 3/3 [00:04<00:00,  1.39s/it]\n",
            "Model loaded.\n",
            "Running 3 inference passes...\n",
            " Run 1/3: 190.95 tokens/sec, 14.12 GB\n",
            " Run 2/3: 190.65 tokens/sec, 14.12 GB\n",
            " Run 3/3: 190.31 tokens/sec, 14.12 GB\n",
            "\n",
            "Prun_kv_cacheresults Results:\n",
            "  Average Latency: 3.394 seconds\n",
            "  Average Throughput: 190.64 tokens/sec\n",
            "  Average Memory: 14.12 GB\n",
            "\n",
            "Sample Output:\n",
            "USER:  \n",
            "Describe this image in detail.\n",
            "ASSISTANT:The image shows a close-up of a cat's face, with its eyes, nose, and mouth clearly visible.\n",
            ": The cat's face is the main focus of the image, with its e...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python test_kv_quant.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afVotDivy6tQ",
        "outputId": "2f1917a9-437f-410a-e1f9-443d9ac59995"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-06 02:36:38.220792: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-06 02:36:38.238150: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764988598.259228    5053 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764988598.265622    5053 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764988598.281863    5053 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764988598.281892    5053 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764988598.281895    5053 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764988598.281898    5053 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-06 02:36:38.286766: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Loading sample image...\n",
            "Image size: (1000, 667)\n",
            "\n",
            "================================================================================\n",
            "Quantization KV_cache Test for LLaVA\n",
            "================================================================================\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Loading checkpoint shards: 100% 3/3 [00:04<00:00,  1.38s/it]\n",
            "Model loaded.\n",
            "Running 3 inference passes...\n",
            " Run 1/3: 106.99 tokens/sec, 14.28 GB\n",
            " Run 2/3: 108.50 tokens/sec, 14.28 GB\n",
            " Run 3/3: 108.56 tokens/sec, 14.28 GB\n",
            "\n",
            "Quant_kv_cache Results:\n",
            "  Average Latency: 6.425 seconds\n",
            "  Average Throughput: 108.02 tokens/sec\n",
            "  Average Memory: 14.28 GB\n",
            "\n",
            "Sample Output:\n",
            "USER:  \n",
            "Describe this image in detail.\n",
            "ASSISTANT: The image features a pier with a wooden walkway extending out into the water. The pier is situated in front of a beautiful mountain range, creating a ...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python int8_tensor_prune_quant.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5r5OpOrz5glA",
        "outputId": "ef37387d-61a1-490f-a403-a7e57fcb9e38"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-06 02:37:22.104337: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-06 02:37:22.121091: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764988642.142268    5288 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764988642.148667    5288 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764988642.164316    5288 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764988642.164343    5288 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764988642.164347    5288 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764988642.164351    5288 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-06 02:37:22.169033: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "quant only\n",
            "Loading sample image...\n",
            "Image size: (1000, 667)\n",
            "\n",
            "================================================================================\n",
            "Method 1: INT8 + Tensor-wise KV Pruning + KV Quantization\n",
            "================================================================================\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "Loading checkpoint shards: 100% 3/3 [00:16<00:00,  5.43s/it]\n",
            "INT8 model loaded (bitsandbytes).\n",
            "Running 3 inference passes...\n",
            " Run 1/3: 44.81 tokens/sec, 7.97 GB\n",
            " Run 2/3: 45.12 tokens/sec, 7.97 GB\n",
            " Run 3/3: 45.53 tokens/sec, 7.97 GB\n",
            "\n",
            "Method 1 Results (INT8 + Tensor-wise Prune + KV Quant):\n",
            "  Average Latency: 15.370 seconds\n",
            "  Average Throughput: 45.15 tokens/sec\n",
            "  Average Memory: 7.97 GB\n",
            "\n",
            "Sample Output:\n",
            "USER:  \n",
            "Describe this image in detail.\n",
            "ASSISTANT: The image features a pier with a wooden walkway extending out into the water. The pier is situated in front of a beautiful mountain range, creating a ...\n",
            "\n",
            "Pruning only\n",
            "Loading sample image...\n",
            "Image size: (1000, 667)\n",
            "\n",
            "================================================================================\n",
            "Method 1: INT8 + Tensor-wise KV Pruning + KV Quantization\n",
            "================================================================================\n",
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "Loading checkpoint shards: 100% 3/3 [00:16<00:00,  5.43s/it]\n",
            "INT8 model loaded (bitsandbytes).\n",
            "Running 3 inference passes...\n",
            " Run 1/3: 83.52 tokens/sec, 7.80 GB\n",
            " Run 2/3: 83.16 tokens/sec, 7.80 GB\n",
            " Run 3/3: 82.88 tokens/sec, 7.80 GB\n",
            "\n",
            "Method 1 Results (INT8 + Tensor-wise Prune + KV Quant):\n",
            "  Average Latency: 7.730 seconds\n",
            "  Average Throughput: 83.18 tokens/sec\n",
            "  Average Memory: 7.80 GB\n",
            "\n",
            "Sample Output:\n",
            "USER:  \n",
            "Describe this image in detail.\n",
            "ASSISTANT: The image is a beautiful, color-tinted photo of a pier with a wooden dock in the foreground. The photo is a beautiful, color-tinted photo of a pier wi...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python int4_tensor_prune_quant.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8meb3O5M6RGK",
        "outputId": "d9282b89-0bd9-4c35-e46a-8caf7fb72350"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-06 02:40:40.595912: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-06 02:40:40.612857: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764988840.633521    6228 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764988840.639791    6228 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764988840.655867    6228 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764988840.655894    6228 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764988840.655897    6228 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764988840.655899    6228 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-06 02:40:40.660642: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "quant only\n",
            "Loading sample image...\n",
            "Image size: (1000, 667)\n",
            "\n",
            "================================================================================\n",
            "Method 1: INT4 + Tensor-wise KV Pruning + KV Quantization\n",
            "================================================================================\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "Loading checkpoint shards: 100% 3/3 [00:16<00:00,  5.46s/it]\n",
            "INT4 model loaded (bitsandbytes).\n",
            "Running 3 inference passes...\n",
            " Run 1/3: 105.40 tokens/sec, 5.20 GB\n",
            " Run 2/3: 107.05 tokens/sec, 5.20 GB\n",
            " Run 3/3: 106.47 tokens/sec, 5.20 GB\n",
            "\n",
            "Method 1 Results (INT4 + Tensor-wise Prune + KV Quant):\n",
            "  Average Latency: 6.528 seconds\n",
            "  Average Throughput: 106.31 tokens/sec\n",
            "  Average Memory: 5.20 GB\n",
            "\n",
            "Sample Output:\n",
            "USER:  \n",
            "Describe this image in detail.\n",
            "ASSISTANT: The image features a wooden pier extending out over a large body of water, possibly a lake or a river. The pier is situated in a serene, natural envir...\n",
            "\n",
            "Pruning only\n",
            "Loading sample image...\n",
            "Image size: (1000, 667)\n",
            "\n",
            "================================================================================\n",
            "Method 1: INT4 + Tensor-wise KV Pruning + KV Quantization\n",
            "================================================================================\n",
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "Loading checkpoint shards: 100% 3/3 [00:16<00:00,  5.53s/it]\n",
            "INT4 model loaded (bitsandbytes).\n",
            "Running 3 inference passes...\n",
            " Run 1/3: 106.37 tokens/sec, 5.05 GB\n",
            " Run 2/3: 106.68 tokens/sec, 5.05 GB\n",
            " Run 3/3: 106.51 tokens/sec, 5.05 GB\n",
            "\n",
            "Method 1 Results (INT4 + Tensor-wise Prune + KV Quant):\n",
            "  Average Latency: 6.515 seconds\n",
            "  Average Throughput: 106.52 tokens/sec\n",
            "  Average Memory: 5.05 GB\n",
            "\n",
            "Sample Output:\n",
            "USER:  \n",
            "Describe this image in detail.\n",
            "ASSISTANT: The image shows a pier, which is a wooden pier with a pier, and a pier, and a pier, and a pier, and a pier, and a pier, and a pier, and a wooden pier ...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python int4_channel_prune_quant.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iCWqqDdq7zj4",
        "outputId": "3d12f9df-846d-4f08-d1bc-2f59c995ae7d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-06 02:43:39.364953: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-06 02:43:39.382305: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764989019.403545    7081 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764989019.409921    7081 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764989019.426438    7081 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764989019.426468    7081 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764989019.426471    7081 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764989019.426473    7081 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-06 02:43:39.431374: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "enable quant\n",
            "Loading sample image...\n",
            "Image size: (1000, 667)\n",
            "\n",
            "================================================================================\n",
            "Method 2: INT4 + Channel-wise KV Pruning + KV Quantization\n",
            "================================================================================\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "Loading checkpoint shards: 100% 3/3 [00:16<00:00,  5.46s/it]\n",
            "INT4 model loaded (bitsandbytes).\n",
            "Running 3 inference passes for Method 2...\n",
            " Run 1/3: 103.14 tokens/sec, 5.21 GB\n",
            " Run 2/3: 103.54 tokens/sec, 5.21 GB\n",
            " Run 3/3: 104.55 tokens/sec, 5.21 GB\n",
            "\n",
            "Method 2 Results (INT4 + Channel-wise Prune + KV Quant):\n",
            "  Average Latency: 6.690 seconds\n",
            "  Average Throughput: 103.74 tokens/sec\n",
            "  Average Memory: 5.21 GB\n",
            "\n",
            "Sample Output:\n",
            "USER:  \n",
            "Describe this image in detail.\n",
            "ASSISTANT:\n",
            "\n",
            "* The image features a pier on a lake, with a boat visible in the foreground.\n",
            "\n",
            "* The pier is situated next to a forest, adding a serene and natural a...\n",
            "\n",
            "disable quant\n",
            "Loading sample image...\n",
            "Image size: (1000, 667)\n",
            "\n",
            "================================================================================\n",
            "Method 2: INT4 + Channel-wise KV Pruning + KV Quantization\n",
            "================================================================================\n",
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "Loading checkpoint shards: 100% 3/3 [00:16<00:00,  5.51s/it]\n",
            "INT4 model loaded (bitsandbytes).\n",
            "Running 3 inference passes for Method 2...\n",
            " Run 1/3: 103.80 tokens/sec, 5.05 GB\n",
            " Run 2/3: 103.35 tokens/sec, 5.05 GB\n",
            " Run 3/3: 104.46 tokens/sec, 5.05 GB\n",
            "\n",
            "Method 2 Results (INT4 + Channel-wise Prune + KV Quant):\n",
            "  Average Latency: 6.682 seconds\n",
            "  Average Throughput: 103.87 tokens/sec\n",
            "  Average Memory: 5.05 GB\n",
            "\n",
            "Sample Output:\n",
            "USER:  \n",
            "Describe this image in detail.\n",
            "ASSISTANT:\n",
            "\n",
            "* The image features a pier on a lake, with a boat visible in the scene.\n",
            "\n",
            "* The pier is situated next to a forest, adding a serene and natural atmosp...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python channel_prune_quant.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E6WzfMF_9c2U",
        "outputId": "273a9817-3a21-4e99-b50e-9c539ba06d8a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-06 02:45:53.383713: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-06 02:45:53.401125: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764989153.422440    7774 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764989153.428849    7774 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764989153.445237    7774 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764989153.445267    7774 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764989153.445270    7774 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764989153.445273    7774 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-06 02:45:53.450135: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "enable quant\n",
            "Loading sample image...\n",
            "Image size: (1000, 667)\n",
            "\n",
            "================================================================================\n",
            "Method 2: Channel-wise KV Pruning + KV Quantization\n",
            "================================================================================\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Loading checkpoint shards: 100% 3/3 [00:04<00:00,  1.38s/it]\n",
            "LLava model loaded .\n",
            "Running 3 inference passes for Method 2...\n",
            " Run 1/3: 129.52 tokens/sec, 14.28 GB\n",
            " Run 2/3: 131.16 tokens/sec, 14.28 GB\n",
            " Run 3/3: 131.37 tokens/sec, 14.28 GB\n",
            "\n",
            "Method 2 Results (Channel-wise Prune + KV Quant):\n",
            "  Average Latency: 5.158 seconds\n",
            "  Average Throughput: 130.69 tokens/sec\n",
            "  Average Memory: 14.28 GB\n",
            "\n",
            "Sample Output:\n",
            "USER:  \n",
            "Describe this image in detail.\n",
            "ASSISTANT: The image features a pier or dock extending out into the water. The pier is situated near a mountainous area, adding a picturesque backdrop to the sce...\n",
            "\n",
            "disable quant\n",
            "Loading sample image...\n",
            "Image size: (1000, 667)\n",
            "\n",
            "================================================================================\n",
            "Method 2: Channel-wise KV Pruning + KV Quantization\n",
            "================================================================================\n",
            "Loading checkpoint shards: 100% 3/3 [00:04<00:00,  1.39s/it]\n",
            "LLava model loaded .\n",
            "Running 3 inference passes for Method 2...\n",
            " Run 1/3: 131.19 tokens/sec, 14.12 GB\n",
            " Run 2/3: 131.24 tokens/sec, 14.12 GB\n",
            " Run 3/3: 130.03 tokens/sec, 14.12 GB\n",
            "\n",
            "Method 2 Results (Channel-wise Prune + KV Quant):\n",
            "  Average Latency: 5.152 seconds\n",
            "  Average Throughput: 130.82 tokens/sec\n",
            "  Average Memory: 14.12 GB\n",
            "\n",
            "Sample Output:\n",
            "USER:  \n",
            "Describe this image in detail.\n",
            "ASSISTANT: The image features a pier or dock extending out into the water. The pier is situated near a mountainous area, adding a picturesque backdrop to the sce...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python test_llmcompressor_awq.py"
      ],
      "metadata": {
        "id": "6pPiSEpXKPlR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python test_llmcompressor_awq_vllm.py ##Downlaod quant mdoel with AWQ_TRY()"
      ],
      "metadata": {
        "id": "iaoy3AakgThw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install vllm==0.6.3"
      ],
      "metadata": {
        "id": "TJTVW5r_qwjE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python test_llmcompressor_awq_vllm.py"
      ],
      "metadata": {
        "id": "WBtL9gRWqoaK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Batch Test"
      ],
      "metadata": {
        "id": "BOpk631vetP2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python test_baseline_batch.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6AVaFYccexo2",
        "outputId": "ff0d5f8c-8cfc-4df6-995c-fcc81ec47b7d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-06 02:47:24.401637: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-06 02:47:24.418813: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764989244.439961    8216 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764989244.446295    8216 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764989244.462120    8216 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764989244.462147    8216 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764989244.462150    8216 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764989244.462153    8216 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-06 02:47:24.466814: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Loading sample image...\n",
            "Image size: (1000, 667)\n",
            "\n",
            "================================================================================\n",
            "Baseline_without_kv_cache Test for LLaVA\n",
            "================================================================================\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Loading checkpoint shards: 100% 3/3 [00:04<00:00,  1.37s/it]\n",
            "Model loaded.\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "\n",
            "==============================\n",
            "  Batch Size Stress Test (true batch forward)\n",
            "==============================\n",
            "\n",
            "--- Batch Size = 4 (1 forward pass with 4 images) ---\n",
            " ==> BS=4: latency=8.592s, throughput=323.08 tok/s, max mem=14.57 GB\n",
            "\n",
            "--- Batch Size = 8 (1 forward pass with 8 images) ---\n",
            " ==> BS=8: latency=10.642s, throughput=521.71 tok/s, max mem=15.98 GB\n",
            "\n",
            "--- Batch Size = 16 (1 forward pass with 16 images) ---\n",
            " ==> BS=16: latency=15.934s, throughput=696.86 tok/s, max mem=18.79 GB\n",
            "\n",
            "--- Batch Size = 32 (1 forward pass with 32 images) ---\n",
            "[ERROR] BS=32 failed: CUDA out of memory. Tried to allocate 400.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 203.38 MiB is free. Process 143986 has 21.96 GiB memory in use. Of the allocated memory 19.66 GiB is allocated by PyTorch, and 2.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n",
            "Baseline Batch Results:\n",
            "  BS=4: Latency=8.592s, Throughput=323.08 tok/s, Memory=14.57 GB\n",
            "  BS=8: Latency=10.642s, Throughput=521.71 tok/s, Memory=15.98 GB\n",
            "  BS=16: Latency=15.934s, Throughput=696.86 tok/s, Memory=18.79 GB\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python test_flash_attn_batch.py"
      ],
      "metadata": {
        "id": "VjY4U_Ug5m-9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e821da6-982e-4e94-afc2-588af1a45c50"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-06 02:48:39.071557: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-06 02:48:39.088381: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764989319.109215    8583 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764989319.115495    8583 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764989319.131380    8583 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764989319.131411    8583 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764989319.131414    8583 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764989319.131417    8583 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-06 02:48:39.136182: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Loading sample image...\n",
            "Image size: (1000, 667)\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "TEST 2: With Flash Attention 2 (Batch Test)\n",
            "--------------------------------------------------------------------------------\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Loading checkpoint shards: 100% 3/3 [00:04<00:00,  1.38s/it]\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "\n",
            "==============================\n",
            "  Batch Size Stress Test (true batch forward)\n",
            "==============================\n",
            "\n",
            "--- Batch Size = 4 (1 forward pass with 4 images) ---\n",
            " ==> BS=4: latency=8.598s, throughput=322.86 tok/s, max mem=14.57 GB\n",
            "\n",
            "--- Batch Size = 8 (1 forward pass with 8 images) ---\n",
            " ==> BS=8: latency=10.705s, throughput=518.65 tok/s, max mem=15.98 GB\n",
            "\n",
            "--- Batch Size = 16 (1 forward pass with 16 images) ---\n",
            " ==> BS=16: latency=16.003s, throughput=693.89 tok/s, max mem=18.79 GB\n",
            "\n",
            "--- Batch Size = 32 (1 forward pass with 32 images) ---\n",
            "[ERROR] BS=32 failed: CUDA out of memory. Tried to allocate 400.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 203.38 MiB is free. Process 148342 has 21.96 GiB memory in use. Of the allocated memory 19.66 GiB is allocated by PyTorch, and 2.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n",
            "Flash Attention 2 Batch Results:\n",
            "  BS=4: Latency=8.598s, Throughput=322.86 tok/s, Memory=14.57 GB\n",
            "  BS=8: Latency=10.705s, Throughput=518.65 tok/s, Memory=15.98 GB\n",
            "  BS=16: Latency=16.003s, Throughput=693.89 tok/s, Memory=18.79 GB\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python test_kv_batch.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ed8pCHFDwlXy",
        "outputId": "e7c06a1f-a908-40d7-fb7b-3547f2986af7"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-06 02:56:19.404613: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-06 02:56:19.421726: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764989779.442676   10687 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764989779.449240   10687 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764989779.465661   10687 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764989779.465690   10687 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764989779.465693   10687 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764989779.465696   10687 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-06 02:56:19.470456: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\n",
            "Using kv cache:\n",
            "Loading sample image...\n",
            "Image size: (1000, 667)\n",
            "\n",
            "================================================================================\n",
            "Baseline KV_cache Batch Test for LLaVA\n",
            "================================================================================\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Loading checkpoint shards: 100% 3/3 [00:04<00:00,  1.37s/it]\n",
            "Model loaded.\n",
            "\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "\n",
            "==============================\n",
            "  Batch Size Stress Test (tensorwise)\n",
            "==============================\n",
            "\n",
            "--- Batch Size = 4 ---\n",
            " ==> BS=4, latency=7.714s, throughput=358.83 tok/s, max_mem=14.71 GB\n",
            "\n",
            "--- Batch Size = 8 ---\n",
            " ==> BS=8, latency=9.513s, throughput=581.93 tok/s, max_mem=16.24 GB\n",
            "\n",
            "--- Batch Size = 16 ---\n",
            " ==> BS=16, latency=13.127s, throughput=843.46 tok/s, max_mem=19.32 GB\n",
            "\n",
            "--- Batch Size = 32 ---\n",
            "[ERROR] BS=32 failed: CUDA out of memory. Tried to allocate 400.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 75.38 MiB is free. Process 172836 has 22.08 GiB memory in use. Of the allocated memory 19.63 GiB is allocated by PyTorch, and 2.23 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n",
            "Baseline KV Cache Batch Results:\n",
            "  BS=4: Latency=7.714s, Throughput=358.83 tok/s, Memory=14.71 GB\n",
            "  BS=8: Latency=9.513s, Throughput=581.93 tok/s, Memory=16.24 GB\n",
            "  BS=16: Latency=13.127s, Throughput=843.46 tok/s, Memory=19.32 GB\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python test_kv_prune_batch.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G98BTrrIxdAg",
        "outputId": "f58da532-dfe0-4c8b-e460-7ce09cf09dac"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-06 02:51:58.857529: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-06 02:51:58.874212: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764989518.895022    9533 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764989518.901918    9533 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764989518.918375    9533 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764989518.918403    9533 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764989518.918406    9533 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764989518.918409    9533 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-06 02:51:58.923231: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\n",
            "Using quantile for Pruning:\n",
            "Loading sample image...\n",
            "Image size: (1000, 667)\n",
            "\n",
            "================================================================================\n",
            "Tensor-wise PRUNING Only KV_cache Batch Test for LLaVA\n",
            "================================================================================\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Loading checkpoint shards: 100% 3/3 [00:04<00:00,  1.39s/it]\n",
            "Model loaded.\n",
            "\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "\n",
            "==============================\n",
            "  Batch Size Stress Test (tensorwise)\n",
            "==============================\n",
            "\n",
            "--- Batch Size = 4 ---\n",
            " ==> BS=4, latency=7.561s, throughput=365.02 tok/s, max_mem=17.04 GB\n",
            "\n",
            "--- Batch Size = 8 ---\n",
            "[ERROR] BS=8 failed: quantile() input tensor is too large\n",
            "\n",
            "--- Batch Size = 16 ---\n",
            "[ERROR] BS=16 failed: CUDA out of memory. Tried to allocate 458.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 205.38 MiB is free. Process 158894 has 21.96 GiB memory in use. Of the allocated memory 19.47 GiB is allocated by PyTorch, and 2.26 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n",
            "--- Batch Size = 32 ---\n",
            "[ERROR] BS=32 failed: CUDA out of memory. Tried to allocate 400.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 75.38 MiB is free. Process 158894 has 22.08 GiB memory in use. Of the allocated memory 19.63 GiB is allocated by PyTorch, and 2.23 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n",
            "Tensor-wise Pruning KV Cache Batch Results:\n",
            "  BS=4: Latency=7.561s, Throughput=365.02 tok/s, Memory=17.04 GB\n",
            "\n",
            "\n",
            "Using Topk for Pruning:\n",
            "Loading sample image...\n",
            "Image size: (1000, 667)\n",
            "\n",
            "================================================================================\n",
            "Tensor-wise PRUNING Only KV_cache Batch Test for LLaVA\n",
            "================================================================================\n",
            "Loading checkpoint shards: 100% 3/3 [00:04<00:00,  1.40s/it]\n",
            "Model loaded.\n",
            "\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "\n",
            "==============================\n",
            "  Batch Size Stress Test (tensorwise)\n",
            "==============================\n",
            "\n",
            "--- Batch Size = 4 ---\n",
            " ==> BS=4, latency=7.507s, throughput=367.64 tok/s, max_mem=17.04 GB\n",
            "\n",
            "--- Batch Size = 8 ---\n",
            "[ERROR] BS=8 failed: CUDA out of memory. Tried to allocate 38.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 33.38 MiB is free. Process 158894 has 22.12 GiB memory in use. Of the allocated memory 19.16 GiB is allocated by PyTorch, and 2.73 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n",
            "--- Batch Size = 16 ---\n",
            "[ERROR] BS=16 failed: CUDA out of memory. Tried to allocate 138.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 51.38 MiB is free. Process 158894 has 22.11 GiB memory in use. Of the allocated memory 19.99 GiB is allocated by PyTorch, and 1.89 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n",
            "--- Batch Size = 32 ---\n",
            "[ERROR] BS=32 failed: CUDA out of memory. Tried to allocate 400.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 85.38 MiB is free. Process 158894 has 22.07 GiB memory in use. Of the allocated memory 19.63 GiB is allocated by PyTorch, and 2.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n",
            "Tensor-wise Pruning KV Cache Batch Results:\n",
            "  BS=4: Latency=7.507s, Throughput=367.64 tok/s, Memory=17.04 GB\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python test_kv_prune_quant_batch.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i6DnoWQxxux2",
        "outputId": "9d18149c-609d-44fc-97f7-3c30e4a8f421"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-06 02:57:38.537575: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-06 02:57:38.554199: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764989858.575002   11077 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764989858.581270   11077 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764989858.597136   11077 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764989858.597166   11077 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764989858.597169   11077 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764989858.597172   11077 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-06 02:57:38.601894: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\n",
            "Using quantile for Pruning:\n",
            "Loading sample image...\n",
            "Image size: (1000, 667)\n",
            "\n",
            "================================================================================\n",
            "Tensor-wise KV Pruning + Quantization Batch Test for LLaVA\n",
            "================================================================================\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Loading checkpoint shards: 100% 3/3 [00:04<00:00,  1.37s/it]\n",
            "Model loaded.\n",
            "\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "\n",
            "==============================\n",
            "  Batch Size Stress Test (tensorwise)\n",
            "==============================\n",
            "\n",
            "--- Batch Size = 4 ---\n",
            " ==> BS=4, latency=7.640s, throughput=362.32 tok/s, max_mem=17.66 GB\n",
            "\n",
            "--- Batch Size = 8 ---\n",
            "[ERROR] BS=8 failed: quantile() input tensor is too large\n",
            "\n",
            "--- Batch Size = 16 ---\n",
            "[ERROR] BS=16 failed: CUDA out of memory. Tried to allocate 458.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 205.38 MiB is free. Process 177012 has 21.96 GiB memory in use. Of the allocated memory 19.47 GiB is allocated by PyTorch, and 2.26 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n",
            "--- Batch Size = 32 ---\n",
            "[ERROR] BS=32 failed: CUDA out of memory. Tried to allocate 400.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 75.38 MiB is free. Process 177012 has 22.08 GiB memory in use. Of the allocated memory 19.63 GiB is allocated by PyTorch, and 2.23 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n",
            "Tensor-wise Prune + Quant KV Batch Results:\n",
            "  BS=4: Latency=7.640s, Throughput=362.32 tok/s, Memory=17.66 GB\n",
            "\n",
            "\n",
            "Using Topk for Pruning:\n",
            "Loading sample image...\n",
            "Image size: (1000, 667)\n",
            "\n",
            "================================================================================\n",
            "Tensor-wise KV Pruning + Quantization Batch Test for LLaVA\n",
            "================================================================================\n",
            "Loading checkpoint shards: 100% 3/3 [00:04<00:00,  1.38s/it]\n",
            "Model loaded.\n",
            "\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "\n",
            "==============================\n",
            "  Batch Size Stress Test (tensorwise)\n",
            "==============================\n",
            "\n",
            "--- Batch Size = 4 ---\n",
            " ==> BS=4, latency=7.637s, throughput=362.44 tok/s, max_mem=17.66 GB\n",
            "\n",
            "--- Batch Size = 8 ---\n",
            "[ERROR] BS=8 failed: CUDA out of memory. Tried to allocate 38.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 29.38 MiB is free. Process 177012 has 22.13 GiB memory in use. Of the allocated memory 20.25 GiB is allocated by PyTorch, and 1.65 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n",
            "--- Batch Size = 16 ---\n",
            "[ERROR] BS=16 failed: CUDA out of memory. Tried to allocate 138.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 51.38 MiB is free. Process 177012 has 22.11 GiB memory in use. Of the allocated memory 19.99 GiB is allocated by PyTorch, and 1.89 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n",
            "--- Batch Size = 32 ---\n",
            "[ERROR] BS=32 failed: CUDA out of memory. Tried to allocate 400.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 85.38 MiB is free. Process 177012 has 22.07 GiB memory in use. Of the allocated memory 19.63 GiB is allocated by PyTorch, and 2.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n",
            "Tensor-wise Prune + Quant KV Batch Results:\n",
            "  BS=4: Latency=7.637s, Throughput=362.44 tok/s, Memory=17.66 GB\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python test_kv_quant_batch.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRQqn2xh_EgX",
        "outputId": "f90501ca-e6a5-4559-c13f-ab633f7bdd20"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-06 02:58:59.268235: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-06 02:58:59.288130: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764989939.313304   11473 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764989939.320999   11473 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764989939.339598   11473 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764989939.339630   11473 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764989939.339633   11473 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764989939.339636   11473 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-06 02:58:59.345106: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\n",
            "Using quantile for Pruning:\n",
            "Loading sample image...\n",
            "Image size: (1000, 667)\n",
            "\n",
            "================================================================================\n",
            "Tensor-wise Quantization-Only KV_cache Batch Test for LLaVA\n",
            "================================================================================\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Loading checkpoint shards: 100% 3/3 [00:04<00:00,  1.37s/it]\n",
            "Model loaded.\n",
            "\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "\n",
            "==============================\n",
            "  Batch Size Stress Test (tensorwise)\n",
            "==============================\n",
            "\n",
            "--- Batch Size = 4 ---\n",
            " ==> BS=4, latency=7.733s, throughput=357.93 tok/s, max_mem=17.65 GB\n",
            "\n",
            "--- Batch Size = 8 ---\n",
            "[ERROR] BS=8 failed: CUDA out of memory. Tried to allocate 38.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 33.38 MiB is free. Process 181396 has 22.12 GiB memory in use. Of the allocated memory 21.59 GiB is allocated by PyTorch, and 309.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n",
            "--- Batch Size = 16 ---\n",
            "[ERROR] BS=16 failed: CUDA out of memory. Tried to allocate 150.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 57.38 MiB is free. Process 181396 has 22.10 GiB memory in use. Of the allocated memory 20.92 GiB is allocated by PyTorch, and 978.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n",
            "--- Batch Size = 32 ---\n",
            "[ERROR] BS=32 failed: CUDA out of memory. Tried to allocate 400.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 225.38 MiB is free. Process 181396 has 21.94 GiB memory in use. Of the allocated memory 19.63 GiB is allocated by PyTorch, and 2.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n",
            "Tensor-wise KV Quantization Batch Results:\n",
            "  BS=4: Latency=7.733s, Throughput=357.93 tok/s, Memory=17.65 GB\n",
            "\n",
            "\n",
            "Using Topk for Pruning:\n",
            "Loading sample image...\n",
            "Image size: (1000, 667)\n",
            "\n",
            "================================================================================\n",
            "Tensor-wise Quantization-Only KV_cache Batch Test for LLaVA\n",
            "================================================================================\n",
            "Loading checkpoint shards: 100% 3/3 [00:04<00:00,  1.38s/it]\n",
            "Model loaded.\n",
            "\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "\n",
            "==============================\n",
            "  Batch Size Stress Test (tensorwise)\n",
            "==============================\n",
            "\n",
            "--- Batch Size = 4 ---\n",
            " ==> BS=4, latency=7.664s, throughput=361.15 tok/s, max_mem=17.65 GB\n",
            "\n",
            "--- Batch Size = 8 ---\n",
            "[ERROR] BS=8 failed: CUDA out of memory. Tried to allocate 38.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 5.38 MiB is free. Process 181396 has 22.15 GiB memory in use. Of the allocated memory 21.63 GiB is allocated by PyTorch, and 299.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n",
            "--- Batch Size = 16 ---\n",
            "[ERROR] BS=16 failed: CUDA out of memory. Tried to allocate 150.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 67.38 MiB is free. Process 181396 has 22.09 GiB memory in use. Of the allocated memory 20.92 GiB is allocated by PyTorch, and 967.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n",
            "--- Batch Size = 32 ---\n",
            "[ERROR] BS=32 failed: CUDA out of memory. Tried to allocate 400.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 235.38 MiB is free. Process 181396 has 21.93 GiB memory in use. Of the allocated memory 19.63 GiB is allocated by PyTorch, and 2.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n",
            "Tensor-wise KV Quantization Batch Results:\n",
            "  BS=4: Latency=7.664s, Throughput=361.15 tok/s, Memory=17.65 GB\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python int8_tensor_prune_quant_batch.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m9otAnG4An_M",
        "outputId": "bf912d64-7b83-4701-f47f-51596b451f79"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-06 03:24:01.088021: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-06 03:24:01.107582: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764991441.129255   18507 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764991441.135787   18507 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764991441.153131   18507 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764991441.153161   18507 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764991441.153164   18507 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764991441.153167   18507 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-06 03:24:01.158192: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\n",
            "Using quantile for Pruning:\n",
            "Loading sample image...\n",
            "Image size: (1000, 667)\n",
            "\n",
            "================================================================================\n",
            "Tensor-wise Quantization-Only KV_cache Batch Test for LLaVA\n",
            "================================================================================\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "Loading checkpoint shards: 100% 3/3 [00:16<00:00,  5.50s/it]\n",
            "Model loaded.\n",
            "\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "\n",
            "==============================\n",
            "  Batch Size Stress Test (tensorwise)\n",
            "==============================\n",
            "\n",
            "--- Batch Size = 4 ---\n",
            " ==> BS=4, latency=17.244s, throughput=160.52 tok/s, max_mem=11.34 GB\n",
            "\n",
            "--- Batch Size = 8 ---\n",
            " ==> BS=8, latency=19.405s, throughput=285.29 tok/s, max_mem=15.78 GB\n",
            "\n",
            "--- Batch Size = 16 ---\n",
            "[ERROR] BS=16 failed: CUDA out of memory. Tried to allocate 76.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 31.38 MiB is free. Process 278230 has 22.12 GiB memory in use. Of the allocated memory 20.84 GiB is allocated by PyTorch, and 1.05 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n",
            "--- Batch Size = 32 ---\n",
            "[ERROR] BS=32 failed: CUDA out of memory. Tried to allocate 150.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 5.38 MiB is free. Process 278230 has 22.15 GiB memory in use. Of the allocated memory 20.40 GiB is allocated by PyTorch, and 1.51 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n",
            "Tensor-wise KV Quantization Batch Results:\n",
            "  BS=4: Latency=17.244s, Throughput=160.52 tok/s, Memory=11.34 GB\n",
            "  BS=8: Latency=19.405s, Throughput=285.29 tok/s, Memory=15.78 GB\n",
            "\n",
            "\n",
            "Using Topk for Pruning:\n",
            "Loading sample image...\n",
            "Image size: (1000, 667)\n",
            "\n",
            "================================================================================\n",
            "Tensor-wise Quantization-Only KV_cache Batch Test for LLaVA\n",
            "================================================================================\n",
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "Loading checkpoint shards: 100% 3/3 [00:16<00:00,  5.42s/it]\n",
            "Model loaded.\n",
            "\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "\n",
            "==============================\n",
            "  Batch Size Stress Test (tensorwise)\n",
            "==============================\n",
            "\n",
            "--- Batch Size = 4 ---\n",
            " ==> BS=4, latency=16.931s, throughput=163.49 tok/s, max_mem=11.34 GB\n",
            "\n",
            "--- Batch Size = 8 ---\n",
            " ==> BS=8, latency=19.390s, throughput=285.51 tok/s, max_mem=15.78 GB\n",
            "\n",
            "--- Batch Size = 16 ---\n",
            "[ERROR] BS=16 failed: CUDA out of memory. Tried to allocate 76.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 31.38 MiB is free. Process 278230 has 22.12 GiB memory in use. Of the allocated memory 20.84 GiB is allocated by PyTorch, and 1.05 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n",
            "--- Batch Size = 32 ---\n",
            "[ERROR] BS=32 failed: CUDA out of memory. Tried to allocate 150.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 5.38 MiB is free. Process 278230 has 22.15 GiB memory in use. Of the allocated memory 20.41 GiB is allocated by PyTorch, and 1.51 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n",
            "Tensor-wise KV Quantization Batch Results:\n",
            "  BS=4: Latency=16.931s, Throughput=163.49 tok/s, Memory=11.34 GB\n",
            "  BS=8: Latency=19.390s, Throughput=285.51 tok/s, Memory=15.78 GB\n",
            "\n",
            "\n",
            "Disable quantization for KV cache:\n",
            "Loading sample image...\n",
            "Image size: (1000, 667)\n",
            "\n",
            "================================================================================\n",
            "Tensor-wise Quantization-Only KV_cache Batch Test for LLaVA\n",
            "================================================================================\n",
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "Loading checkpoint shards: 100% 3/3 [00:16<00:00,  5.47s/it]\n",
            "Model loaded.\n",
            "\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "\n",
            "==============================\n",
            "  Batch Size Stress Test (tensorwise)\n",
            "==============================\n",
            "\n",
            "--- Batch Size = 4 ---\n",
            " ==> BS=4, latency=17.003s, throughput=162.80 tok/s, max_mem=8.40 GB\n",
            "\n",
            "--- Batch Size = 8 ---\n",
            " ==> BS=8, latency=19.028s, throughput=290.94 tok/s, max_mem=9.94 GB\n",
            "\n",
            "--- Batch Size = 16 ---\n",
            " ==> BS=16, latency=22.774s, throughput=486.18 tok/s, max_mem=13.02 GB\n",
            "\n",
            "--- Batch Size = 32 ---\n",
            "[ERROR] BS=32 failed: CUDA out of memory. Tried to allocate 150.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 3.38 MiB is free. Process 278230 has 22.15 GiB memory in use. Of the allocated memory 17.45 GiB is allocated by PyTorch, and 4.46 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n",
            "Tensor-wise KV Quantization Batch Results:\n",
            "  BS=4: Latency=17.003s, Throughput=162.80 tok/s, Memory=8.40 GB\n",
            "  BS=8: Latency=19.028s, Throughput=290.94 tok/s, Memory=9.94 GB\n",
            "  BS=16: Latency=22.774s, Throughput=486.18 tok/s, Memory=13.02 GB\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python int4_tensor_prune_quant_batch.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dO0cJsJcBIGb",
        "outputId": "d1bae631-9d64-43f8-f05f-28b8c0845f9d"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-06 03:02:20.679965: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-06 03:02:20.696873: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764990140.717636   12437 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764990140.723943   12437 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764990140.740118   12437 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764990140.740145   12437 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764990140.740148   12437 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764990140.740150   12437 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-06 03:02:20.745195: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\n",
            "Using quantile for Pruning:\n",
            "================================================================================\n",
            "Method 1: INT4 + Tensor-wise KV Pruning + KV Quantization (Batch Test)\n",
            "================================================================================\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "Loading checkpoint shards: 100% 3/3 [00:16<00:00,  5.52s/it]\n",
            "INT4 model loaded (bitsandbytes).\n",
            "\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "\n",
            "==============================\n",
            "  Batch Size Stress Test (tensorwise)\n",
            "==============================\n",
            "\n",
            "--- Batch Size = 4 ---\n",
            " ==> BS=4, latency=35.348s, throughput=78.31 tok/s, max_mem=8.79 GB\n",
            "\n",
            "--- Batch Size = 8 ---\n",
            "[ERROR] BS=8 failed: quantile() input tensor is too large\n",
            "\n",
            "--- Batch Size = 16 ---\n",
            "[ERROR] BS=16 failed: quantile() input tensor is too large\n",
            "\n",
            "--- Batch Size = 32 ---\n",
            "[ERROR] BS=32 failed: quantile() input tensor is too large\n",
            "\n",
            "Method 1 Batch Results (INT4 + Tensor-wise Prune + KV Quant):\n",
            "  BS=4: Latency=35.348s, Throughput=78.31 tok/s, Memory=8.79 GB\n",
            "\n",
            "\n",
            "Using Topk for Pruning:\n",
            "================================================================================\n",
            "Method 1: INT4 + Tensor-wise KV Pruning + KV Quantization (Batch Test)\n",
            "================================================================================\n",
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "Loading checkpoint shards: 100% 3/3 [00:16<00:00,  5.42s/it]\n",
            "INT4 model loaded (bitsandbytes).\n",
            "\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "\n",
            "==============================\n",
            "  Batch Size Stress Test (tensorwise)\n",
            "==============================\n",
            "\n",
            "--- Batch Size = 4 ---\n",
            " ==> BS=4, latency=35.218s, throughput=78.60 tok/s, max_mem=8.78 GB\n",
            "\n",
            "--- Batch Size = 8 ---\n",
            " ==> BS=8, latency=38.427s, throughput=144.06 tok/s, max_mem=13.24 GB\n",
            "\n",
            "--- Batch Size = 16 ---\n",
            "[ERROR] BS=16 failed: CUDA out of memory. Tried to allocate 76.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 71.38 MiB is free. Process 192546 has 22.09 GiB memory in use. Of the allocated memory 19.94 GiB is allocated by PyTorch, and 1.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n",
            "--- Batch Size = 32 ---\n",
            "[ERROR] BS=32 failed: CUDA out of memory. Tried to allocate 276.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 35.38 MiB is free. Process 192546 has 22.12 GiB memory in use. Of the allocated memory 19.18 GiB is allocated by PyTorch, and 2.72 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n",
            "Method 1 Batch Results (INT4 + Tensor-wise Prune + KV Quant):\n",
            "  BS=4: Latency=35.218s, Throughput=78.60 tok/s, Memory=8.78 GB\n",
            "  BS=8: Latency=38.427s, Throughput=144.06 tok/s, Memory=13.24 GB\n",
            "\n",
            "\n",
            "Disable quantization for KV cache:\n",
            "================================================================================\n",
            "Method 1: INT4 + Tensor-wise KV Pruning + KV Quantization (Batch Test)\n",
            "================================================================================\n",
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "Loading checkpoint shards: 100% 3/3 [00:14<00:00,  4.70s/it]\n",
            "INT4 model loaded (bitsandbytes).\n",
            "\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "\n",
            "==============================\n",
            "  Batch Size Stress Test (tensorwise)\n",
            "==============================\n",
            "\n",
            "--- Batch Size = 4 ---\n",
            " ==> BS=4, latency=35.239s, throughput=78.55 tok/s, max_mem=8.17 GB\n",
            "\n",
            "--- Batch Size = 8 ---\n",
            " ==> BS=8, latency=34.122s, throughput=159.66 tok/s, max_mem=11.96 GB\n",
            "\n",
            "--- Batch Size = 16 ---\n",
            "[ERROR] BS=16 failed: CUDA out of memory. Tried to allocate 76.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 71.38 MiB is free. Process 192546 has 22.09 GiB memory in use. Of the allocated memory 17.69 GiB is allocated by PyTorch, and 4.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n",
            "--- Batch Size = 32 ---\n",
            "[ERROR] BS=32 failed: CUDA out of memory. Tried to allocate 276.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 35.38 MiB is free. Process 192546 has 22.12 GiB memory in use. Of the allocated memory 19.18 GiB is allocated by PyTorch, and 2.72 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n",
            "Method 1 Batch Results (INT4 + Tensor-wise Prune + KV Quant):\n",
            "  BS=4: Latency=35.239s, Throughput=78.55 tok/s, Memory=8.17 GB\n",
            "  BS=8: Latency=34.122s, Throughput=159.66 tok/s, Memory=11.96 GB\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python int4_channel_prune_quant_batch.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mXPS6YqdBrP4",
        "outputId": "23321a78-dc6d-4e93-9206-1dff126a04a0"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-06 03:09:57.896720: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-06 03:09:57.913920: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764990597.935124   14472 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764990597.941563   14472 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764990597.957820   14472 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764990597.957849   14472 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764990597.957852   14472 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764990597.957854   14472 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-06 03:09:57.962571: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\n",
            "Using unstructure for Pruning:\n",
            "================================================================================\n",
            "Method 2: INT4 + Channel-wise KV Pruning + KV Quantization (Batch Test)\n",
            "================================================================================\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "Loading checkpoint shards: 100% 3/3 [00:16<00:00,  5.61s/it]\n",
            "INT4 model loaded (bitsandbytes).\n",
            "\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "\n",
            "==============================\n",
            "  Batch Size Stress Test (channelwise)\n",
            "==============================\n",
            "\n",
            "--- Batch Size = 4 ---\n",
            " ==> BS=4, latency=20.400s, throughput=127.45 tok/s, max_mem=8.71 GB\n",
            "\n",
            "--- Batch Size = 8 ---\n",
            " ==> BS=8, latency=22.191s, throughput=234.33 tok/s, max_mem=13.10 GB\n",
            "\n",
            "--- Batch Size = 16 ---\n",
            "[ERROR] BS=16 failed: CUDA out of memory. Tried to allocate 86.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 15.38 MiB is free. Process 224361 has 22.14 GiB memory in use. Of the allocated memory 21.10 GiB is allocated by PyTorch, and 831.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n",
            "--- Batch Size = 32 ---\n",
            "[ERROR] BS=32 failed: CUDA out of memory. Tried to allocate 298.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 91.38 MiB is free. Process 224361 has 22.07 GiB memory in use. Of the allocated memory 20.16 GiB is allocated by PyTorch, and 1.67 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n",
            "Method 2 Batch Results (INT4 + Channel-wise Prune + KV Quant):\n",
            "  BS=4: Latency=20.400s, Throughput=127.45 tok/s, Memory=8.71 GB\n",
            "  BS=8: Latency=22.191s, Throughput=234.33 tok/s, Memory=13.10 GB\n",
            "\n",
            "\n",
            "disable quantization for Pruning:\n",
            "================================================================================\n",
            "Method 2: INT4 + Channel-wise KV Pruning + KV Quantization (Batch Test)\n",
            "================================================================================\n",
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "Loading checkpoint shards: 100% 3/3 [00:16<00:00,  5.48s/it]\n",
            "INT4 model loaded (bitsandbytes).\n",
            "\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "\n",
            "==============================\n",
            "  Batch Size Stress Test (channelwise)\n",
            "==============================\n",
            "\n",
            "--- Batch Size = 4 ---\n",
            " ==> BS=4, latency=20.392s, throughput=127.50 tok/s, max_mem=8.08 GB\n",
            "\n",
            "--- Batch Size = 8 ---\n",
            " ==> BS=8, latency=22.208s, throughput=234.15 tok/s, max_mem=11.84 GB\n",
            "\n",
            "--- Batch Size = 16 ---\n",
            " ==> BS=16, latency=24.392s, throughput=426.36 tok/s, max_mem=19.34 GB\n",
            "\n",
            "--- Batch Size = 32 ---\n",
            "[ERROR] BS=32 failed: CUDA out of memory. Tried to allocate 298.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 91.38 MiB is free. Process 224361 has 22.07 GiB memory in use. Of the allocated memory 20.16 GiB is allocated by PyTorch, and 1.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n",
            "Method 2 Batch Results (INT4 + Channel-wise Prune + KV Quant):\n",
            "  BS=4: Latency=20.392s, Throughput=127.50 tok/s, Memory=8.08 GB\n",
            "  BS=8: Latency=22.208s, Throughput=234.15 tok/s, Memory=11.84 GB\n",
            "  BS=16: Latency=24.392s, Throughput=426.36 tok/s, Memory=19.34 GB\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python int8_channel_prune_quant_batch.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LEvKrie0E-e2",
        "outputId": "6f6c4132-0de5-45df-fcc9-5bf69a7670b1"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-06 03:14:59.471433: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-06 03:14:59.488783: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764990899.510257   15866 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764990899.516924   15866 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764990899.533301   15866 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764990899.533331   15866 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764990899.533334   15866 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764990899.533337   15866 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-06 03:14:59.538313: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\n",
            "Using unstructure for Pruning:\n",
            "================================================================================\n",
            "Method 2: INT8 + Channel-wise KV Pruning + KV Quantization (Batch Test)\n",
            "================================================================================\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "Loading checkpoint shards: 100% 3/3 [00:16<00:00,  5.40s/it]\n",
            "INT8 model loaded (bitsandbytes).\n",
            "\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "\n",
            "==============================\n",
            "  Batch Size Stress Test (channelwise)\n",
            "==============================\n",
            "\n",
            "--- Batch Size = 4 ---\n",
            " ==> BS=4, latency=17.079s, throughput=162.08 tok/s, max_mem=11.36 GB\n",
            "\n",
            "--- Batch Size = 8 ---\n",
            " ==> BS=8, latency=19.257s, throughput=287.48 tok/s, max_mem=15.86 GB\n",
            "\n",
            "--- Batch Size = 16 ---\n",
            "[ERROR] BS=16 failed: CUDA out of memory. Tried to allocate 76.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 27.38 MiB is free. Process 245081 has 22.13 GiB memory in use. Of the allocated memory 20.84 GiB is allocated by PyTorch, and 1.06 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n",
            "--- Batch Size = 32 ---\n",
            "[ERROR] BS=32 failed: CUDA out of memory. Tried to allocate 298.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 157.38 MiB is free. Process 245081 has 22.00 GiB memory in use. Of the allocated memory 19.17 GiB is allocated by PyTorch, and 2.59 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n",
            "Method 2 Batch Results (INT8 + Channel-wise Prune + KV Quant):\n",
            "  BS=4: Latency=17.079s, Throughput=162.08 tok/s, Memory=11.36 GB\n",
            "  BS=8: Latency=19.257s, Throughput=287.48 tok/s, Memory=15.86 GB\n",
            "\n",
            "\n",
            "disable quantization for Pruning:\n",
            "================================================================================\n",
            "Method 2: INT8 + Channel-wise KV Pruning + KV Quantization (Batch Test)\n",
            "================================================================================\n",
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "Loading checkpoint shards: 100% 3/3 [00:16<00:00,  5.43s/it]\n",
            "INT8 model loaded (bitsandbytes).\n",
            "\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "\n",
            "==============================\n",
            "  Batch Size Stress Test (channelwise)\n",
            "==============================\n",
            "\n",
            "--- Batch Size = 4 ---\n",
            " ==> BS=4, latency=17.031s, throughput=162.53 tok/s, max_mem=10.75 GB\n",
            "\n",
            "--- Batch Size = 8 ---\n",
            " ==> BS=8, latency=19.174s, throughput=288.72 tok/s, max_mem=14.61 GB\n",
            "\n",
            "--- Batch Size = 16 ---\n",
            "[ERROR] BS=16 failed: CUDA out of memory. Tried to allocate 76.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 59.38 MiB is free. Process 245081 has 22.10 GiB memory in use. Of the allocated memory 20.33 GiB is allocated by PyTorch, and 1.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n",
            "--- Batch Size = 32 ---\n",
            "[ERROR] BS=32 failed: CUDA out of memory. Tried to allocate 298.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 159.38 MiB is free. Process 245081 has 22.00 GiB memory in use. Of the allocated memory 19.18 GiB is allocated by PyTorch, and 2.59 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n",
            "Method 2 Batch Results (INT8 + Channel-wise Prune + KV Quant):\n",
            "  BS=4: Latency=17.031s, Throughput=162.53 tok/s, Memory=10.75 GB\n",
            "  BS=8: Latency=19.174s, Throughput=288.72 tok/s, Memory=14.61 GB\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python channel_prune_quant_batch.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UEiVgKw3FtwW",
        "outputId": "17eb3f02-0803-4d7c-d882-2d057d001501"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-06 03:20:08.944438: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-06 03:20:08.961650: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764991208.982719   17378 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764991208.989067   17378 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764991209.005220   17378 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764991209.005248   17378 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764991209.005251   17378 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764991209.005254   17378 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-06 03:20:09.010041: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\n",
            "Using unstructure for Pruning:\n",
            "Loading sample image...\n",
            "Image size: (1000, 667)\n",
            "\n",
            "================================================================================\n",
            "Method 2: Channel-wise KV Pruning + KV Quantization (Batch Test)\n",
            "================================================================================\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Loading checkpoint shards: 100% 3/3 [00:04<00:00,  1.38s/it]\n",
            "LLava fp16 model loaded.\n",
            "\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "\n",
            "==============================\n",
            "  Batch Size Stress Test (channelwise)\n",
            "==============================\n",
            "\n",
            "--- Batch Size = 4 ---\n",
            " ==> BS=4, latency=7.662s, throughput=361.28 tok/s, max_mem=17.67 GB\n",
            "\n",
            "--- Batch Size = 8 ---\n",
            "[ERROR] BS=8 failed: CUDA out of memory. Tried to allocate 38.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 33.38 MiB is free. Process 263934 has 22.12 GiB memory in use. Of the allocated memory 21.60 GiB is allocated by PyTorch, and 303.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n",
            "--- Batch Size = 16 ---\n",
            "[ERROR] BS=16 failed: CUDA out of memory. Tried to allocate 150.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 125.38 MiB is free. Process 263934 has 22.03 GiB memory in use. Of the allocated memory 20.05 GiB is allocated by PyTorch, and 1.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n",
            "--- Batch Size = 32 ---\n",
            "[ERROR] BS=32 failed: CUDA out of memory. Tried to allocate 400.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 75.38 MiB is free. Process 263934 has 22.08 GiB memory in use. Of the allocated memory 19.63 GiB is allocated by PyTorch, and 2.23 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n",
            "Method 2 Batch Results (Channel-wise Prune + KV Quant):\n",
            "  BS=4: Latency=7.662s, Throughput=361.28 tok/s, Memory=17.67 GB\n",
            "\n",
            "\n",
            "disable quantization for Pruning:\n",
            "Loading sample image...\n",
            "Image size: (1000, 667)\n",
            "\n",
            "================================================================================\n",
            "Method 2: Channel-wise KV Pruning + KV Quantization (Batch Test)\n",
            "================================================================================\n",
            "Loading checkpoint shards: 100% 3/3 [00:04<00:00,  1.39s/it]\n",
            "LLava fp16 model loaded.\n",
            "\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "\n",
            "==============================\n",
            "  Batch Size Stress Test (channelwise)\n",
            "==============================\n",
            "\n",
            "--- Batch Size = 4 ---\n",
            " ==> BS=4, latency=7.675s, throughput=360.64 tok/s, max_mem=17.06 GB\n",
            "\n",
            "--- Batch Size = 8 ---\n",
            " ==> BS=8, latency=9.775s, throughput=566.34 tok/s, max_mem=20.93 GB\n",
            "\n",
            "--- Batch Size = 16 ---\n",
            "[ERROR] BS=16 failed: CUDA out of memory. Tried to allocate 150.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 135.38 MiB is free. Process 263934 has 22.02 GiB memory in use. Of the allocated memory 20.05 GiB is allocated by PyTorch, and 1.75 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n",
            "--- Batch Size = 32 ---\n",
            "[ERROR] BS=32 failed: CUDA out of memory. Tried to allocate 400.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 85.38 MiB is free. Process 263934 has 22.07 GiB memory in use. Of the allocated memory 19.63 GiB is allocated by PyTorch, and 2.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n",
            "Method 2 Batch Results (Channel-wise Prune + KV Quant):\n",
            "  BS=4: Latency=7.675s, Throughput=360.64 tok/s, Memory=17.06 GB\n",
            "  BS=8: Latency=9.775s, Throughput=566.34 tok/s, Memory=20.93 GB\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python test_bitsandbytes_batch.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lK48zSaRS74Q",
        "outputId": "5669fcbf-6cd5-435a-e0a8-91f30c5f606c"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-06 03:21:37.172053: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-06 03:21:37.189633: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764991297.211299   17798 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764991297.217820   17798 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764991297.234308   17798 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764991297.234334   17798 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764991297.234337   17798 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764991297.234339   17798 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-06 03:21:37.239230: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Loading sample image...\n",
            "Image size: (1000, 667)\n",
            "\n",
            "================================================================================\n",
            "8bit batch Test for LLaVA\n",
            "================================================================================\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "Loading checkpoint shards: 100% 3/3 [00:16<00:00,  5.43s/it]\n",
            "Model loaded.\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "\n",
            "==============================\n",
            "  Batch Size Stress Test (true batch forward)\n",
            "==============================\n",
            "\n",
            "--- Batch Size = 4 (1 forward pass with 4 images) ---\n",
            " ==> BS=4: latency=17.774s, throughput=156.19 tok/s, max mem=8.32 GB\n",
            "\n",
            "--- Batch Size = 8 (1 forward pass with 8 images) ---\n",
            " ==> BS=8: latency=20.097s, throughput=276.25 tok/s, max mem=9.78 GB\n",
            "\n",
            "--- Batch Size = 16 (1 forward pass with 16 images) ---\n",
            " ==> BS=16: latency=25.495s, throughput=435.54 tok/s, max mem=12.71 GB\n",
            "\n",
            "--- Batch Size = 32 (1 forward pass with 32 images) ---\n",
            " ==> BS=32: latency=35.997s, throughput=616.94 tok/s, max mem=18.56 GB\n",
            "\"8bit Batch Results:\n",
            "  BS=4: Latency=17.774s, Throughput=156.19 tok/s, Memory=8.32 GB\n",
            "  BS=8: Latency=20.097s, Throughput=276.25 tok/s, Memory=9.78 GB\n",
            "  BS=16: Latency=25.495s, Throughput=435.54 tok/s, Memory=12.71 GB\n",
            "  BS=32: Latency=35.997s, Throughput=616.94 tok/s, Memory=18.56 GB\n"
          ]
        }
      ]
    }
  ]
}