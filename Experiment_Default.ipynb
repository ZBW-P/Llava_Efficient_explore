{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "runtime_attributes": {
        "runtime_version": "2025.10"
      }
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers accelerate pillow torch\n",
        "!pip install -q flash-attn --no-build-isolation"
      ],
      "metadata": {
        "id": "pCg-P6EKcHuQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf1468cd-6050-455e-a50e-de9db08f5ac7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/8.4 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/8.4 MB\u001b[0m \u001b[31m149.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m148.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m85.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m72.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for flash-attn (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llmcompressor datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I6VrjHIVYFyZ",
        "outputId": "f1631379-129c-4250-f765-c096f8ec755c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llmcompressor\n",
            "  Downloading llmcompressor-0.8.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Collecting loguru<=0.7.3,>=0.7.2 (from llmcompressor)\n",
            "  Downloading loguru-0.7.3-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: pyyaml<=6.0.3,>=6.0.1 in /usr/local/lib/python3.12/dist-packages (from llmcompressor) (6.0.3)\n",
            "Requirement already satisfied: numpy<=2.3.3,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from llmcompressor) (2.0.2)\n",
            "Requirement already satisfied: requests<=2.32.5,>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from llmcompressor) (2.32.4)\n",
            "Requirement already satisfied: tqdm<=4.67.1,>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from llmcompressor) (4.67.1)\n",
            "Requirement already satisfied: torch<=2.8.0,>=2.7.0 in /usr/local/lib/python3.12/dist-packages (from llmcompressor) (2.8.0+cu126)\n",
            "Collecting transformers<=4.56.2,>=4.53.0 (from llmcompressor)\n",
            "  Downloading transformers-4.56.2-py3-none-any.whl.metadata (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.1/40.1 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: accelerate<=1.10.1,>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from llmcompressor) (1.10.1)\n",
            "Requirement already satisfied: nvidia-ml-py<=13.580.82,>=12.560.30 in /usr/local/lib/python3.12/dist-packages (from llmcompressor) (12.575.51)\n",
            "Requirement already satisfied: pillow<=11.3.0,>=10.4.0 in /usr/local/lib/python3.12/dist-packages (from llmcompressor) (11.3.0)\n",
            "Collecting compressed-tensors==0.12.2 (from llmcompressor)\n",
            "  Downloading compressed_tensors-0.12.2-py3-none-any.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.12/dist-packages (from compressed-tensors==0.12.2->llmcompressor) (2.11.10)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.35.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate<=1.10.1,>=1.6.0->llmcompressor) (5.9.5)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate<=1.10.1,>=1.6.0->llmcompressor) (0.6.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.10)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<=2.32.5,>=2.32.2->llmcompressor) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<=2.32.5,>=2.32.2->llmcompressor) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<=2.32.5,>=2.32.2->llmcompressor) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<=2.32.5,>=2.32.2->llmcompressor) (2025.10.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (3.4.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<=4.56.2,>=4.53.0->llmcompressor) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<=4.56.2,>=4.53.0->llmcompressor) (0.22.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0->compressed-tensors==0.12.2->llmcompressor) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0->compressed-tensors==0.12.2->llmcompressor) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0->compressed-tensors==0.12.2->llmcompressor) (0.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<=2.8.0,>=2.7.0->llmcompressor) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<=2.8.0,>=2.7.0->llmcompressor) (3.0.3)\n",
            "Downloading llmcompressor-0.8.1-py3-none-any.whl (273 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m273.2/273.2 kB\u001b[0m \u001b[31m247.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading compressed_tensors-0.12.2-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.0/183.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading loguru-0.7.3-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformers-4.56.2-py3-none-any.whl (11.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m181.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: loguru, transformers, compressed-tensors, llmcompressor\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.57.0\n",
            "    Uninstalling transformers-4.57.0:\n",
            "      Successfully uninstalled transformers-4.57.0\n",
            "Successfully installed compressed-tensors-0.12.2 llmcompressor-0.8.1 loguru-0.7.3 transformers-4.56.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U bitsandbytes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_tuG642YIRs",
        "outputId": "2fc9d043-b0d6-4ce9-fc44-d781b6f88306"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: torch<3,>=2.3 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.8.0+cu126)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (25.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.3->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.3)\n",
            "Downloading bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl (59.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m51.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bitsandbytes\n",
            "Successfully installed bitsandbytes-0.48.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6RPnK-KkbI1D",
        "outputId": "89803f60-bcb0-4d9b-977d-9db639fe6d19"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.12.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python test_baseline.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZV_YISLiWAr0",
        "outputId": "c67870cf-058b-459e-d84f-42f779d74fbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-28 04:45:36.594803: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-28 04:45:36.611840: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764305136.633080    1764 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764305136.639397    1764 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764305136.655493    1764 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764305136.655518    1764 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764305136.655521    1764 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764305136.655524    1764 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-28 04:45:36.660195: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Loading sample image...\n",
            "Image size: (1000, 667)\n",
            "\n",
            "================================================================================\n",
            "Baseline_with_kv_cache Test for LLaVA\n",
            "================================================================================\n",
            "processor_config.json: 100% 173/173 [00:00<00:00, 1.66MB/s]\n",
            "chat_template.json: 100% 701/701 [00:00<00:00, 7.28MB/s]\n",
            "chat_template.jinja: 100% 674/674 [00:00<00:00, 7.18MB/s]\n",
            "preprocessor_config.json: 100% 505/505 [00:00<00:00, 5.76MB/s]\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "tokenizer_config.json: 1.45kB [00:00, 7.58MB/s]\n",
            "tokenizer.model: 100% 500k/500k [00:01<00:00, 285kB/s]\n",
            "tokenizer.json: 3.62MB [00:00, 185MB/s]\n",
            "added_tokens.json: 100% 41.0/41.0 [00:00<00:00, 520kB/s]\n",
            "special_tokens_map.json: 100% 552/552 [00:00<00:00, 5.42MB/s]\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "config.json: 100% 950/950 [00:00<00:00, 11.1MB/s]\n",
            "model.safetensors.index.json: 70.1kB [00:00, 196MB/s]\n",
            "Fetching 3 files:   0% 0/3 [00:00<?, ?it/s]\n",
            "model-00001-of-00003.safetensors:   0% 0.00/4.99G [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:   0% 0.00/4.96G [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:   0% 0.00/4.18G [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:   0% 24.6k/4.99G [00:01<67:13:45, 20.6kB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:   0% 599k/4.96G [00:01<4:04:17, 338kB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:   0% 119k/4.99G [00:01<18:51:07, 73.6kB/s] \u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:   0% 546k/4.18G [00:02<4:21:57, 266kB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:   5% 239M/4.99G [00:02<00:24, 191MB/s]    \u001b[A\n",
            "model-00001-of-00003.safetensors:   6% 306M/4.99G [00:04<01:08, 68.1MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:   8% 412M/4.99G [00:04<00:44, 102MB/s] \u001b[A\n",
            "model-00001-of-00003.safetensors:  14% 677M/4.99G [00:05<00:21, 201MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:   1% 67.7M/4.96G [00:05<05:55, 13.8MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  16% 811M/4.99G [00:05<00:20, 204MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:   3% 135M/4.96G [00:08<04:41, 17.2MB/s] \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:   2% 67.6M/4.18G [00:12<12:27, 5.50MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:   3% 135M/4.18G [00:13<05:08, 13.1MB/s] \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:   5% 269M/4.96G [00:13<03:23, 23.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:   6% 269M/4.18G [00:13<01:57, 33.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:   8% 403M/4.96G [00:13<01:46, 42.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:   8% 336M/4.18G [00:13<01:21, 47.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:   9% 470M/4.96G [00:13<01:22, 54.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  13% 537M/4.18G [00:13<00:36, 101MB/s] \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  14% 604M/4.18G [00:14<00:34, 103MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  12% 604M/4.96G [00:14<00:57, 75.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  18% 738M/4.18G [00:14<00:25, 134MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  18% 880M/4.99G [00:14<01:52, 36.6MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  19% 805M/4.18G [00:15<00:24, 140MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  19% 941M/4.99G [00:15<01:35, 42.6MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  15% 738M/4.96G [00:15<00:44, 94.2MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  20% 1.01G/4.99G [00:16<01:33, 42.8MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  16% 805M/4.96G [00:19<01:26, 48.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  22% 940M/4.18G [00:19<00:51, 63.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  22% 1.08G/4.99G [00:19<01:45, 37.3MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  19% 940M/4.96G [00:19<00:53, 75.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  24% 1.01G/4.18G [00:19<00:40, 77.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  20% 1.01G/4.96G [00:19<00:42, 92.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  27% 1.14G/4.18G [00:19<00:25, 118MB/s] \u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  23% 1.14G/4.99G [00:19<01:25, 45.2MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  22% 1.07G/4.96G [00:19<00:38, 100MB/s] \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  29% 1.21G/4.18G [00:20<00:26, 111MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  23% 1.14G/4.96G [00:20<00:39, 96.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  32% 1.34G/4.18G [00:20<00:19, 145MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  34% 1.41G/4.18G [00:21<00:21, 127MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  26% 1.27G/4.96G [00:21<00:32, 113MB/s] \u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  26% 1.28G/4.99G [00:21<01:10, 53.0MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  27% 1.34G/4.96G [00:22<00:31, 115MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  27% 1.34G/4.99G [00:25<01:34, 38.5MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  30% 1.48G/4.99G [00:26<01:09, 50.6MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  28% 1.41G/4.96G [00:26<01:21, 43.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  31% 1.54G/4.96G [00:27<00:52, 65.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  35% 1.48G/4.18G [00:27<01:09, 38.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  39% 1.61G/4.18G [00:27<00:44, 57.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  34% 1.68G/4.96G [00:28<00:44, 73.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  37% 1.81G/4.96G [00:30<00:39, 79.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  43% 1.81G/4.18G [00:30<00:34, 68.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  31% 1.54G/4.99G [00:30<01:37, 35.2MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  38% 1.88G/4.96G [00:30<00:34, 88.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  45% 1.88G/4.18G [00:31<00:33, 69.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  33% 1.67G/4.99G [00:31<01:05, 50.7MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  35% 1.74G/4.99G [00:32<01:04, 50.9MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  39% 1.94G/4.99G [00:35<00:53, 57.5MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  39% 1.95G/4.96G [00:35<01:16, 39.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  48% 2.01G/4.18G [00:35<00:45, 47.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  51% 2.15G/4.18G [00:35<00:29, 68.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  42% 2.08G/4.99G [00:35<00:36, 79.2MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  42% 2.08G/4.96G [00:35<00:47, 61.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  45% 2.21G/4.96G [00:36<00:32, 83.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  53% 2.21G/4.18G [00:36<00:26, 73.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  43% 2.15G/4.99G [00:39<00:56, 50.3MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  46% 2.28G/4.96G [00:40<00:55, 48.1MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  44% 2.21G/4.99G [00:40<00:50, 55.0MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  56% 2.35G/4.18G [00:40<00:34, 53.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  48% 2.42G/4.99G [00:40<00:25, 101MB/s] \u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  47% 2.35G/4.96G [00:40<00:44, 59.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  58% 2.42G/4.18G [00:41<00:35, 50.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  50% 2.48G/4.99G [00:42<00:35, 71.6MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  49% 2.42G/4.96G [00:42<00:52, 48.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  61% 2.55G/4.18G [00:42<00:23, 68.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  50% 2.48G/4.96G [00:43<00:49, 50.5MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  51% 2.55G/4.99G [00:44<00:39, 61.2MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  51% 2.55G/4.96G [00:44<00:39, 60.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  63% 2.62G/4.18G [00:44<00:26, 59.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  54% 2.68G/4.99G [00:44<00:24, 93.4MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  66% 2.75G/4.18G [00:44<00:15, 89.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  56% 2.78G/4.99G [00:44<00:20, 110MB/s] \u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  53% 2.62G/4.96G [00:45<00:36, 64.1MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  57% 2.85G/4.99G [00:45<00:18, 117MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  67% 2.82G/4.18G [00:45<00:16, 80.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  54% 2.68G/4.96G [00:45<00:34, 66.4MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  58% 2.92G/4.99G [00:46<00:20, 99.0MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  57% 2.82G/4.96G [00:47<00:29, 72.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  71% 2.95G/4.18G [00:51<00:28, 43.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  74% 3.09G/4.18G [00:51<00:18, 57.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  60% 2.98G/4.99G [00:52<00:58, 34.3MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  60% 2.95G/4.96G [00:52<00:44, 45.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  77% 3.22G/4.18G [00:52<00:12, 78.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  61% 3.05G/4.99G [00:52<00:45, 42.8MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  64% 3.18G/4.99G [00:52<00:25, 71.9MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  61% 3.02G/4.96G [00:52<00:35, 53.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  64% 3.15G/4.96G [00:52<00:22, 81.9MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  65% 3.25G/4.99G [00:53<00:22, 78.5MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  65% 3.22G/4.96G [00:53<00:18, 94.3MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  68% 3.38G/4.99G [00:53<00:13, 117MB/s] \u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  79% 3.29G/4.18G [00:53<00:12, 73.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  66% 3.29G/4.96G [00:54<00:19, 85.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  80% 3.35G/4.18G [00:54<00:11, 69.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  69% 3.42G/4.96G [00:54<00:12, 120MB/s] \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  82% 3.42G/4.18G [00:54<00:08, 86.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  69% 3.45G/4.99G [00:55<00:21, 71.5MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  85% 3.55G/4.18G [00:59<00:13, 45.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  70% 3.52G/4.99G [01:00<00:41, 35.5MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  70% 3.49G/4.96G [01:01<00:40, 36.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  87% 3.62G/4.18G [01:01<00:11, 46.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  72% 3.59G/4.99G [01:01<00:33, 42.4MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  73% 3.62G/4.96G [01:01<00:24, 54.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  90% 3.76G/4.18G [01:01<00:06, 69.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  73% 3.65G/4.99G [01:01<00:26, 51.3MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  93% 3.89G/4.18G [01:01<00:02, 100MB/s] \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  76% 3.76G/4.96G [01:02<00:15, 78.5MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  75% 3.72G/4.99G [01:02<00:19, 66.0MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  76% 3.79G/4.99G [01:03<00:20, 60.2MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  77% 3.85G/4.99G [01:05<00:20, 54.5MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  95% 3.96G/4.18G [01:05<00:03, 56.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  77% 3.82G/4.96G [01:05<00:23, 48.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  96% 4.02G/4.18G [01:06<00:02, 59.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  79% 3.92G/4.99G [01:06<00:19, 55.2MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  78% 3.89G/4.96G [01:06<00:20, 51.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  80% 3.96G/4.96G [01:06<00:16, 61.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  97% 4.04G/4.18G [01:07<00:02, 48.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  81% 4.03G/4.96G [01:08<00:16, 55.7MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  81% 4.05G/4.99G [01:08<00:17, 52.2MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  98% 4.11G/4.18G [01:09<00:01, 44.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors: 100% 4.18G/4.18G [01:09<00:00, 60.4MB/s]\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  83% 4.09G/4.96G [01:09<00:14, 61.5MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  85% 4.25G/4.99G [01:09<00:06, 107MB/s] \u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  85% 4.23G/4.96G [01:09<00:07, 99.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  87% 4.29G/4.96G [01:09<00:05, 122MB/s] \u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  88% 4.39G/4.99G [01:09<00:04, 136MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  88% 4.36G/4.96G [01:09<00:04, 136MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  89% 4.43G/4.96G [01:10<00:03, 152MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  89% 4.46G/4.99G [01:10<00:04, 132MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  91% 4.52G/4.99G [01:10<00:02, 158MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  91% 4.50G/4.96G [01:10<00:02, 156MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  92% 4.59G/4.99G [01:10<00:02, 157MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  93% 4.66G/4.99G [01:11<00:01, 189MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  92% 4.56G/4.96G [01:11<00:03, 118MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  95% 4.72G/4.99G [01:12<00:03, 87.3MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  93% 4.63G/4.96G [01:13<00:04, 77.9MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  96% 4.79G/4.99G [01:13<00:02, 94.5MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  95% 4.69G/4.96G [01:13<00:03, 88.7MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  97% 4.86G/4.99G [01:13<00:01, 109MB/s] \u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  96% 4.76G/4.96G [01:13<00:01, 107MB/s] \u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  99% 4.93G/4.99G [01:14<00:00, 128MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  97% 4.82G/4.96G [01:14<00:01, 127MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  99% 4.89G/4.96G [01:14<00:00, 143MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors: 100% 4.99G/4.99G [01:14<00:00, 67.0MB/s]\n",
            "Fetching 3 files:  33% 1/3 [01:15<02:30, 75.04s/it]\n",
            "\n",
            "model-00002-of-00003.safetensors: 100% 4.96G/4.96G [01:14<00:00, 66.2MB/s]\n",
            "Fetching 3 files: 100% 3/3 [01:15<00:00, 25.18s/it]\n",
            "Loading checkpoint shards: 100% 3/3 [00:04<00:00,  1.43s/it]\n",
            "generation_config.json: 100% 141/141 [00:00<00:00, 1.61MB/s]\n",
            "Model loaded.\n",
            "Running 3 inference passes...\n",
            " Run 1/3: 11.50 tokens/sec, 13.52 GB\n",
            " Run 2/3: 16.51 tokens/sec, 13.52 GB\n",
            " Run 3/3: 16.49 tokens/sec, 13.52 GB\n",
            "\n",
            "kv_cache_results Results:\n",
            "  Average Latency: 6.940 seconds\n",
            "  Average Throughput: 14.83 tokens/sec\n",
            "  Average Memory: 13.52 GB\n",
            "\n",
            "Sample Output:\n",
            "USER:  \n",
            "Describe this image in detail.\n",
            "ASSISTANT: The image features a pier extending out into a large body of water, likely a lake. The pier is made of wood and has a bench situated on it, providing ...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python test_flash_attn.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJZATiT0eCXB",
        "outputId": "cb18597c-2d01-43f3-adf5-aafe1d0be146"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-29 02:36:52.117531: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-29 02:36:52.137644: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764383812.158778   22475 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764383812.165324   22475 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764383812.181940   22475 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764383812.181968   22475 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764383812.181971   22475 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764383812.181973   22475 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-29 02:36:52.186951: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Loading sample image...\n",
            "Image size: (1000, 667)\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "TEST 2: With Flash Attention 2\n",
            "--------------------------------------------------------------------------------\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Loading checkpoint shards: 100% 3/3 [00:04<00:00,  1.40s/it]\n",
            "Running 3 inference passes...\n",
            " Run 1/3: 11.93 tokens/sec, 13.52 GB\n",
            " Run 2/3: 14.03 tokens/sec, 13.52 GB\n",
            " Run 3/3: 13.99 tokens/sec, 13.52 GB\n",
            "\n",
            "Flash Attention 2 Results:\n",
            "  Average Latency: 7.555 seconds\n",
            "  Average Throughput: 13.31 tokens/sec\n",
            "  Average Memory: 13.52 GB\n",
            "\n",
            "Sample Output:\n",
            "USER:  \n",
            "Describe this image in detail.\n",
            "ASSISTANT: The image features a pier extending out into a large body of water, likely a lake. The pier is made of wood and has a bench situated on it, providing ...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python test_bitsandbytes.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vW2O-BceeFvK",
        "outputId": "b3facea7-b489-4984-cfd0-ed152dcaa373"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-29 02:39:20.653179: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-29 02:39:20.673303: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764383960.694256   23143 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764383960.700634   23143 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764383960.717129   23143 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764383960.717154   23143 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764383960.717157   23143 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764383960.717160   23143 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-29 02:39:20.721990: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Loading sample image...\n",
            "Image size: (1000, 667)\n",
            "\n",
            "================================================================================\n",
            "bitsandbytes Quantization 4 bits Test for LLaVA\n",
            "================================================================================\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "Loading checkpoint shards: 100% 3/3 [00:16<00:00,  5.53s/it]\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "\n",
            "AWQ bitsandbytes quant 4 bit complete. Loading quantized model...\n",
            "\n",
            "Running 3 inference passes...\n",
            " Run 1/3: 12.25 tokens/sec, 4.69 GB\n",
            " Run 2/3: 13.91 tokens/sec, 4.69 GB\n",
            " Run 3/3: 13.62 tokens/sec, 4.69 GB\n",
            "\n",
            "bitsandbytes Results:\n",
            "  Avg Latency: 7.567 seconds\n",
            "  Avg Throughput: 13.26 tokens/sec\n",
            "  Avg Memory: 4.69 GB\n",
            "\n",
            "Sample Output:\n",
            "USER:  \n",
            "Describe this image in detail.\n",
            "ASSISTANT: The image features a wooden pier extending out over a body of water. The pier is situated in a serene, natural setting with a mountain in the backgrou...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python test_kv.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aik6Mkopek8e",
        "outputId": "808fc0c6-80ec-4735-9e78-d1193c4fb5ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-29 02:40:24.761685: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-29 02:40:24.781414: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764384024.802902   23502 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764384024.809458   23502 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764384024.826165   23502 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764384024.826193   23502 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764384024.826196   23502 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764384024.826199   23502 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-29 02:40:24.831232: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Loading sample image...\n",
            "Image size: (1000, 667)\n",
            "\n",
            "================================================================================\n",
            "KV_cache Test for LLaVA\n",
            "================================================================================\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Loading checkpoint shards: 100% 3/3 [00:04<00:00,  1.42s/it]\n",
            "Model loaded.\n",
            "Running 3 inference passes...\n",
            " Run 1/3: 106.07 tokens/sec, 13.55 GB\n",
            " Run 2/3: 108.03 tokens/sec, 13.55 GB\n",
            " Run 3/3: 107.89 tokens/sec, 13.55 GB\n",
            "\n",
            "kv_cache Results:\n",
            "  Average Latency: 6.467 seconds\n",
            "  Average Throughput: 107.33 tokens/sec\n",
            "  Average Memory: 13.55 GB\n",
            "\n",
            "Sample Output:\n",
            "USER:  \n",
            "Describe this image in detail.\n",
            "ASSISTANT: The image features a pier with a wooden walkway extending out into the water. The pier is situated near a mountain, providing a picturesque backdrop. ...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python test_kv_prune_quant.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H55M7Pi8yexw",
        "outputId": "62cdc10a-8021-443f-b413-d1435d11572a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-28 05:13:39.970014: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-28 05:13:39.987379: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764306820.009165    9258 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764306820.015633    9258 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764306820.031725    9258 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764306820.031754    9258 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764306820.031756    9258 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764306820.031759    9258 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-28 05:13:40.036494: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Loading sample image...\n",
            "Image size: (1000, 667)\n",
            "\n",
            "================================================================================\n",
            "Prun + Quantization KV_cache Test for LLaVA\n",
            "================================================================================\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Loading checkpoint shards: 100% 3/3 [00:04<00:00,  1.42s/it]\n",
            "Model loaded.\n",
            "Running 3 inference passes...\n",
            " Run 1/3: 202.79 tokens/sec, 14.26 GB\n",
            " Run 2/3: 205.27 tokens/sec, 14.26 GB\n",
            " Run 3/3: 204.80 tokens/sec, 14.26 GB\n",
            "\n",
            "Prun + Quant_kv_cache Results:\n",
            "  Average Latency: 3.148 seconds\n",
            "  Average Throughput: 204.29 tokens/sec\n",
            "  Average Memory: 14.26 GB\n",
            "\n",
            "Sample Output:\n",
            "USER:  \n",
            "Describe this image in detail.\n",
            "ASSISTANT:The image shows a pier extending out into the water. The pier is situated on the water's edge, and it is a long pier that extends out into the water. T...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python test_kv_prune.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YPBK2sapy0zb",
        "outputId": "462b4f20-9fa3-4fc6-e750-7bbf48fc2521"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-28 04:49:39.905786: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-28 04:49:39.923555: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764305379.945223    3001 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764305379.951672    3001 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764305379.968118    3001 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764305379.968152    3001 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764305379.968155    3001 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764305379.968158    3001 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-28 04:49:39.973023: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Loading sample image...\n",
            "Image size: (1000, 667)\n",
            "\n",
            "================================================================================\n",
            "Prun KV_cache Test for LLaVA\n",
            "================================================================================\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Loading checkpoint shards: 100% 3/3 [00:04<00:00,  1.42s/it]\n",
            "Model loaded.\n",
            "Running 3 inference passes...\n",
            " Run 1/3: 200.50 tokens/sec, 14.11 GB\n",
            " Run 2/3: 204.95 tokens/sec, 14.11 GB\n",
            " Run 3/3: 205.20 tokens/sec, 14.11 GB\n",
            "\n",
            "Prun_kv_cacheresults Results:\n",
            "  Average Latency: 3.159 seconds\n",
            "  Average Throughput: 203.55 tokens/sec\n",
            "  Average Memory: 14.11 GB\n",
            "\n",
            "Sample Output:\n",
            "USER:  \n",
            "Describe this image in detail.\n",
            "ASSISTANT:The image shows a pier extending out into the water. The pier is situated on the water's edge, and it is a long pier that extends out into the water. T...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python test_kv_quant.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afVotDivy6tQ",
        "outputId": "8e1953e9-379d-4ce7-a267-08b63efd8e4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-28 05:15:54.006989: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-28 05:15:54.024089: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764306954.046074    9861 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764306954.052725    9861 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764306954.069526    9861 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764306954.069558    9861 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764306954.069563    9861 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764306954.069567    9861 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-28 05:15:54.074279: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Loading sample image...\n",
            "Image size: (1000, 667)\n",
            "\n",
            "================================================================================\n",
            "Quantization KV_cache Test for LLaVA\n",
            "================================================================================\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Loading checkpoint shards: 100% 3/3 [00:04<00:00,  1.41s/it]\n",
            "Model loaded.\n",
            "Running 3 inference passes...\n",
            " Run 1/3: 106.46 tokens/sec, 14.28 GB\n",
            " Run 2/3: 108.11 tokens/sec, 14.28 GB\n",
            " Run 3/3: 108.13 tokens/sec, 14.28 GB\n",
            "\n",
            "Quant_kv_cache Results:\n",
            "  Average Latency: 6.452 seconds\n",
            "  Average Throughput: 107.56 tokens/sec\n",
            "  Average Memory: 14.28 GB\n",
            "\n",
            "Sample Output:\n",
            "USER:  \n",
            "Describe this image in detail.\n",
            "ASSISTANT: The image features a pier with a wooden walkway extending out into the water. The pier is situated in front of a beautiful mountain range, creating a ...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python int8_tensor_prune_quant.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5r5OpOrz5glA",
        "outputId": "1de74669-2378-44b9-e572-0362a6b0f39e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-29 02:24:00.293612: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-29 02:24:02.188570: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764383042.210425   18682 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764383042.216895   18682 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764383042.234012   18682 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764383042.234039   18682 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764383042.234042   18682 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764383042.234045   18682 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-29 02:24:02.238934: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Loading sample image...\n",
            "Image size: (1000, 667)\n",
            "\n",
            "================================================================================\n",
            "Method 1: INT8 + Tensor-wise KV Pruning + KV Quantization\n",
            "================================================================================\n",
            "processor_config.json: 100% 173/173 [00:00<00:00, 1.58MB/s]\n",
            "chat_template.json: 100% 701/701 [00:00<00:00, 5.73MB/s]\n",
            "chat_template.jinja: 100% 674/674 [00:00<00:00, 7.75MB/s]\n",
            "preprocessor_config.json: 100% 505/505 [00:00<00:00, 5.38MB/s]\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "tokenizer_config.json: 1.45kB [00:00, 7.35MB/s]\n",
            "tokenizer.model: 100% 500k/500k [00:01<00:00, 405kB/s]\n",
            "tokenizer.json: 3.62MB [00:00, 210MB/s]\n",
            "added_tokens.json: 100% 41.0/41.0 [00:00<00:00, 390kB/s]\n",
            "special_tokens_map.json: 100% 552/552 [00:00<00:00, 5.92MB/s]\n",
            "config.json: 100% 950/950 [00:00<00:00, 10.1MB/s]\n",
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "model.safetensors.index.json: 70.1kB [00:00, 193MB/s]\n",
            "Fetching 3 files:   0% 0/3 [00:00<?, ?it/s]\n",
            "model-00001-of-00003.safetensors:   0% 0.00/4.99G [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:   0% 0.00/4.96G [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:   0% 0.00/4.18G [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:   0% 13.5k/4.99G [00:01<122:44:48, 11.3kB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:   0% 114k/4.99G [00:02<21:50:35, 63.5kB/s]  \u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:   0% 546k/4.18G [00:02<4:41:54, 247kB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  10% 504M/4.99G [00:02<00:12, 372MB/s]    \u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:   0% 599k/4.96G [00:02<6:12:49, 222kB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:   2% 67.7M/4.18G [00:05<04:48, 14.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  14% 680M/4.99G [00:08<01:01, 70.7MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:   3% 135M/4.18G [00:12<06:20, 10.6MB/s] \u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  16% 808M/4.99G [00:12<01:19, 52.7MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:   5% 202M/4.18G [00:13<03:27, 19.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  18% 876M/4.99G [00:13<01:06, 61.7MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:   6% 269M/4.18G [00:13<02:06, 30.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:   1% 67.6M/4.96G [00:13<14:59, 5.44MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  10% 403M/4.18G [00:13<01:04, 58.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:   3% 135M/4.96G [00:13<06:25, 12.5MB/s] \u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  19% 943M/4.99G [00:14<01:04, 62.4MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:   5% 269M/4.96G [00:14<02:38, 29.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  11% 470M/4.18G [00:14<01:02, 59.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  20% 1.01G/4.99G [00:15<01:03, 62.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  13% 537M/4.18G [00:15<00:53, 67.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  22% 1.07G/4.99G [00:15<00:51, 75.7MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  16% 671M/4.18G [00:15<00:31, 111MB/s] \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:   9% 470M/4.96G [00:15<01:19, 56.7MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  24% 1.21G/4.99G [00:15<00:33, 114MB/s] \u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  12% 604M/4.96G [00:16<00:54, 80.1MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  26% 1.27G/4.99G [00:16<00:33, 111MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  18% 738M/4.18G [00:16<00:33, 103MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  14% 671M/4.96G [00:16<00:51, 83.6MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  27% 1.34G/4.99G [00:16<00:30, 122MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  19% 805M/4.18G [00:17<00:33, 100MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  15% 738M/4.96G [00:17<00:45, 92.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  21% 873M/4.18G [00:17<00:30, 109MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  18% 872M/4.96G [00:17<00:36, 111MB/s] \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  23% 940M/4.18G [00:18<00:32, 100MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  19% 940M/4.96G [00:18<00:34, 118MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  28% 1.41G/4.99G [00:18<00:50, 71.1MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  24% 1.01G/4.18G [00:18<00:28, 110MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  31% 1.54G/4.99G [00:19<00:33, 103MB/s] \u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  26% 1.07G/4.18G [00:19<00:29, 104MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  22% 1.07G/4.96G [00:20<00:41, 93.2MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  32% 1.61G/4.99G [00:20<00:41, 82.4MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  27% 1.14G/4.18G [00:20<00:36, 82.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  29% 1.21G/4.18G [00:22<00:42, 69.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  23% 1.14G/4.96G [00:25<01:31, 41.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  31% 1.27G/4.18G [00:25<01:13, 39.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  34% 1.68G/4.99G [00:27<01:49, 30.2MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  32% 1.34G/4.18G [00:27<01:10, 40.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  35% 1.75G/4.99G [00:29<01:40, 32.1MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  24% 1.21G/4.96G [00:29<02:00, 31.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  34% 1.41G/4.18G [00:29<01:13, 37.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  36% 1.82G/4.99G [00:29<01:15, 42.2MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  26% 1.28G/4.96G [00:29<01:33, 39.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  35% 1.48G/4.18G [00:29<00:55, 48.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  38% 1.89G/4.99G [00:29<01:00, 51.7MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  27% 1.34G/4.96G [00:30<01:15, 48.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  37% 1.54G/4.18G [00:30<00:44, 59.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  39% 1.96G/4.99G [00:30<00:46, 64.7MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  39% 1.61G/4.18G [00:30<00:36, 70.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  41% 2.02G/4.99G [00:30<00:39, 75.7MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  28% 1.41G/4.96G [00:30<01:04, 55.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  40% 1.68G/4.18G [00:30<00:25, 96.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  42% 2.09G/4.99G [00:31<00:32, 89.9MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  30% 1.48G/4.96G [00:31<00:51, 67.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  42% 1.75G/4.18G [00:31<00:24, 100MB/s] \u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  43% 2.16G/4.99G [00:32<00:33, 83.7MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  43% 1.81G/4.18G [00:32<00:24, 96.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  33% 1.61G/4.96G [00:33<00:49, 68.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  48% 2.01G/4.18G [00:33<00:15, 137MB/s] \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  34% 1.68G/4.96G [00:33<00:43, 75.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  50% 2.08G/4.18G [00:34<00:17, 120MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  45% 2.22G/4.99G [00:34<00:47, 58.8MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  37% 1.81G/4.96G [00:34<00:30, 102MB/s] \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  51% 2.15G/4.18G [00:34<00:16, 119MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  46% 2.28G/4.99G [00:34<00:40, 66.4MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  38% 1.88G/4.96G [00:35<00:31, 98.5MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  47% 2.35G/4.99G [00:35<00:36, 72.8MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  41% 2.01G/4.96G [00:35<00:20, 143MB/s] \u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  50% 2.48G/4.99G [00:35<00:23, 106MB/s] \u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  42% 2.08G/4.96G [00:36<00:24, 118MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  51% 2.55G/4.99G [00:37<00:29, 83.7MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  45% 2.21G/4.96G [00:37<00:21, 126MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  53% 2.21G/4.18G [00:39<00:47, 41.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  55% 2.28G/4.18G [00:39<00:36, 52.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  52% 2.62G/4.99G [00:41<01:01, 38.7MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  46% 2.28G/4.96G [00:42<00:58, 45.9MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  54% 2.68G/4.99G [00:43<01:02, 36.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  56% 2.35G/4.18G [00:46<01:13, 24.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  55% 2.75G/4.99G [00:46<01:08, 32.8MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  47% 2.35G/4.96G [00:46<01:21, 32.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  59% 2.48G/4.18G [00:46<00:38, 43.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  63% 2.62G/4.18G [00:46<00:23, 67.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  56% 2.82G/4.99G [00:46<00:50, 43.1MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  49% 2.42G/4.96G [00:46<01:03, 40.1MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  58% 2.88G/4.99G [00:47<00:40, 51.5MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  66% 2.75G/4.18G [00:47<00:17, 82.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  60% 3.02G/4.99G [00:47<00:23, 83.2MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  50% 2.48G/4.96G [00:48<01:01, 40.2MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  62% 3.08G/4.99G [00:48<00:23, 80.7MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  54% 2.68G/4.96G [00:49<00:30, 74.1MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  63% 3.15G/4.99G [00:49<00:21, 84.4MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  55% 2.75G/4.96G [00:49<00:26, 82.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  67% 2.82G/4.18G [00:49<00:22, 61.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  64% 3.22G/4.99G [00:49<00:18, 93.4MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  71% 2.95G/4.18G [00:50<00:14, 86.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  66% 3.29G/4.99G [00:50<00:17, 95.8MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  58% 2.89G/4.96G [00:50<00:21, 96.9MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  67% 3.35G/4.99G [00:50<00:15, 105MB/s] \u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  61% 3.02G/4.96G [00:50<00:15, 125MB/s] \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  72% 3.02G/4.18G [00:50<00:13, 85.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  69% 3.42G/4.99G [00:51<00:15, 103MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  62% 3.09G/4.96G [00:52<00:17, 105MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  70% 3.49G/4.99G [00:52<00:15, 99.6MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  64% 3.15G/4.96G [00:52<00:16, 108MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  76% 3.15G/4.18G [00:52<00:11, 85.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  71% 3.55G/4.99G [00:52<00:13, 108MB/s] \u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  77% 3.22G/4.18G [00:53<00:11, 83.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  79% 3.29G/4.18G [00:54<00:10, 85.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  65% 3.22G/4.96G [00:54<00:22, 78.8MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  73% 3.62G/4.99G [00:54<00:20, 66.4MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  66% 3.29G/4.96G [00:55<00:23, 72.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  80% 3.35G/4.18G [00:56<00:12, 63.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  82% 3.42G/4.18G [00:58<00:14, 51.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  68% 3.35G/4.96G [00:58<00:33, 48.0MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  74% 3.69G/4.99G [00:58<00:32, 39.7MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  75% 3.76G/4.99G [01:01<00:38, 32.5MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  84% 3.49G/4.18G [01:01<00:19, 34.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  77% 3.82G/4.99G [01:01<00:28, 40.5MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  69% 3.42G/4.96G [01:01<00:46, 33.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  87% 3.62G/4.18G [01:02<00:09, 56.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  78% 3.89G/4.99G [01:02<00:21, 51.4MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  88% 3.69G/4.18G [01:02<00:07, 66.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  79% 3.96G/4.99G [01:03<00:18, 54.9MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  70% 3.49G/4.96G [01:03<00:42, 34.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  72% 3.56G/4.96G [01:04<00:32, 43.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  73% 3.63G/4.96G [01:04<00:23, 57.7MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  81% 4.02G/4.99G [01:04<00:17, 54.3MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  82% 4.09G/4.99G [01:04<00:12, 72.9MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  90% 3.76G/4.18G [01:04<00:08, 51.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  74% 3.69G/4.96G [01:04<00:19, 65.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  92% 3.84G/4.18G [01:05<00:05, 65.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  83% 4.16G/4.99G [01:05<00:11, 73.9MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  95% 3.98G/4.18G [01:05<00:02, 94.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  77% 3.83G/4.96G [01:06<00:13, 83.6MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  85% 4.22G/4.99G [01:06<00:09, 78.5MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  79% 3.89G/4.96G [01:06<00:10, 99.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  80% 3.96G/4.96G [01:06<00:08, 122MB/s] \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  97% 4.04G/4.18G [01:06<00:01, 91.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  81% 4.03G/4.96G [01:06<00:06, 141MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  86% 4.29G/4.99G [01:07<00:08, 78.1MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  98% 4.11G/4.18G [01:07<00:00, 94.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  89% 4.43G/4.99G [01:07<00:04, 117MB/s] \u001b[A\n",
            "model-00001-of-00003.safetensors:  90% 4.49G/4.99G [01:08<00:04, 123MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors: 100% 4.18G/4.18G [01:08<00:00, 61.1MB/s]\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  83% 4.09G/4.96G [01:08<00:11, 77.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  84% 4.16G/4.96G [01:09<00:08, 92.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  85% 4.23G/4.96G [01:09<00:06, 117MB/s] \u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  91% 4.56G/4.99G [01:09<00:04, 94.0MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  93% 4.63G/4.99G [01:09<00:03, 116MB/s] \u001b[A\n",
            "model-00001-of-00003.safetensors:  94% 4.69G/4.99G [01:10<00:02, 120MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  87% 4.30G/4.96G [01:10<00:07, 87.0MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  95% 4.72G/4.99G [01:10<00:02, 90.8MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  88% 4.36G/4.96G [01:10<00:05, 103MB/s] \u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  96% 4.79G/4.99G [01:12<00:03, 61.9MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  89% 4.43G/4.96G [01:13<00:08, 59.2MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  97% 4.86G/4.99G [01:13<00:02, 63.9MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  91% 4.49G/4.96G [01:13<00:06, 69.6MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  99% 4.93G/4.99G [01:13<00:00, 88.1MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  93% 4.63G/4.96G [01:13<00:02, 122MB/s] \u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  95% 4.69G/4.96G [01:13<00:01, 150MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  96% 4.76G/4.96G [01:14<00:01, 178MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors: 100% 4.99G/4.99G [01:14<00:00, 67.2MB/s]\n",
            "Fetching 3 files:  33% 1/3 [01:14<02:29, 74.81s/it]\n",
            "\n",
            "model-00002-of-00003.safetensors:  97% 4.82G/4.96G [01:14<00:00, 179MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  99% 4.89G/4.96G [01:14<00:00, 217MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors: 100% 4.96G/4.96G [01:14<00:00, 66.3MB/s]\n",
            "Fetching 3 files: 100% 3/3 [01:15<00:00, 25.07s/it]\n",
            "Loading checkpoint shards: 100% 3/3 [00:16<00:00,  5.50s/it]\n",
            "generation_config.json: 100% 141/141 [00:00<00:00, 1.20MB/s]\n",
            "INT8 model loaded (bitsandbytes).\n",
            "Running 3 inference passes...\n",
            " Run 1/3: 75.72 tokens/sec, 7.96 GB\n",
            " Run 2/3: 83.29 tokens/sec, 7.96 GB\n",
            " Run 3/3: 84.08 tokens/sec, 7.96 GB\n",
            "\n",
            "Method 1 Results (INT8 + Tensor-wise Prune + KV Quant):\n",
            "  Average Latency: 7.928 seconds\n",
            "  Average Throughput: 81.03 tokens/sec\n",
            "  Average Memory: 7.96 GB\n",
            "\n",
            "Sample Output:\n",
            "USER:  \n",
            "Describe this image in detail.\n",
            "ASSISTANT: The image is a beautiful, serene, and picturesque view of a pier and a mountain in the distance. The image is a beautiful, serene, and picturesque vie...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python int4_tensor_prune_quant.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8meb3O5M6RGK",
        "outputId": "1865637f-704b-41e6-f402-7624d3e6f5ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-29 02:26:56.893150: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-29 02:26:56.913265: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764383216.935026   19669 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764383216.941908   19669 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764383216.959237   19669 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764383216.959274   19669 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764383216.959276   19669 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764383216.959279   19669 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-29 02:26:56.964221: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Loading sample image...\n",
            "Image size: (1000, 667)\n",
            "\n",
            "================================================================================\n",
            "Method 1: INT4 + Tensor-wise KV Pruning + KV Quantization\n",
            "================================================================================\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "Loading checkpoint shards: 100% 3/3 [00:17<00:00,  5.69s/it]\n",
            "INT4 model loaded (bitsandbytes).\n",
            "Running 3 inference passes...\n",
            " Run 1/3: 100.70 tokens/sec, 5.21 GB\n",
            " Run 2/3: 100.18 tokens/sec, 5.21 GB\n",
            " Run 3/3: 101.83 tokens/sec, 5.21 GB\n",
            "\n",
            "Method 1 Results (INT4 + Tensor-wise Prune + KV Quant):\n",
            "  Average Latency: 6.878 seconds\n",
            "  Average Throughput: 100.91 tokens/sec\n",
            "  Average Memory: 5.21 GB\n",
            "\n",
            "Sample Output:\n",
            "USER:  \n",
            "Describe this image in detail.\n",
            "ASSISTANT: The image shows a pier, which is a wooden pier with a pier, and a pier, and a pier, and a pier, and a pier, and a pier, and a pier, and a wooden pier ...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python int4_channel_prune_quant.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iCWqqDdq7zj4",
        "outputId": "9b2f1e9c-a784-4522-efd5-a82fd1054c6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-29 02:31:09.576002: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-29 02:31:09.596174: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764383469.617313   20852 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764383469.623792   20852 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764383469.640721   20852 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764383469.640752   20852 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764383469.640755   20852 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764383469.640757   20852 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-29 02:31:09.645619: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Loading sample image...\n",
            "Image size: (1000, 667)\n",
            "\n",
            "================================================================================\n",
            "Method 2: INT4 + Channel-wise KV Pruning + KV Quantization\n",
            "================================================================================\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "Loading checkpoint shards: 100% 3/3 [00:16<00:00,  5.59s/it]\n",
            "INT4 model loaded (bitsandbytes).\n",
            "Running 3 inference passes for Method 2...\n",
            " Run 1/3: 108.76 tokens/sec, 5.20 GB\n",
            " Run 2/3: 110.85 tokens/sec, 5.20 GB\n",
            " Run 3/3: 111.10 tokens/sec, 5.20 GB\n",
            "\n",
            "Method 2 Results (INT4 + Channel-wise Prune + KV Quant):\n",
            "  Average Latency: 6.224 seconds\n",
            "  Average Throughput: 110.23 tokens/sec\n",
            "  Average Memory: 5.20 GB\n",
            "\n",
            "Sample Output:\n",
            "USER:  \n",
            "Describe this image in detail.\n",
            "ASSISTANT: The image features a pier or dock extending out into the water. The pier is surrounded by a lush, green forest, creating a serene and picturesque scen...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python channel_prune_quant.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E6WzfMF_9c2U",
        "outputId": "aa02b698-827c-4d71-c054-f8bb4a4c18d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-29 02:32:48.300225: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-29 02:32:48.319456: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764383568.340172   21390 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764383568.346562   21390 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764383568.363720   21390 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764383568.363748   21390 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764383568.363751   21390 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764383568.363754   21390 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-29 02:32:48.368661: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Loading sample image...\n",
            "Image size: (1000, 667)\n",
            "\n",
            "================================================================================\n",
            "Method 2: Channel-wise KV Pruning + KV Quantization\n",
            "================================================================================\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Loading checkpoint shards: 100% 3/3 [00:04<00:00,  1.41s/it]\n",
            "LLava model loaded .\n",
            "Running 3 inference passes for Method 2...\n",
            " Run 1/3: 118.17 tokens/sec, 14.28 GB\n",
            " Run 2/3: 118.77 tokens/sec, 14.28 GB\n",
            " Run 3/3: 118.44 tokens/sec, 14.28 GB\n",
            "\n",
            "Method 2 Results (Channel-wise Prune + KV Quant):\n",
            "  Average Latency: 5.774 seconds\n",
            "  Average Throughput: 118.46 tokens/sec\n",
            "  Average Memory: 14.28 GB\n",
            "\n",
            "Sample Output:\n",
            "USER:  \n",
            "Describe this image in detail.\n",
            "ASSISTANT: The image features a pier extending out into a large body of water, likely a lake or a river. The pier is surrounded by a lush, green forest, creating...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python test_llmcompressor_awq.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6pPiSEpXKPlR",
        "outputId": "864bde01-147a-44c1-bd52-6a63e3ea9cc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-28 22:04:40.922647: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-28 22:04:40.941506: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764367480.964241   31943 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764367480.971767   31943 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764367480.990977   31943 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764367480.991003   31943 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764367480.991006   31943 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764367480.991009   31943 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-28 22:04:40.995914: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
            "  warnings.warn(\n",
            "Loading sample image...\n",
            "Image size: (1000, 667)\n",
            "\n",
            "\n",
            "AWQ quantization complete. Loading quantized model...\n",
            "\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Compressing model: 224it [00:00, 608.92it/s]\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "Running 3 inference passes...\n",
            " Run 1/3: 1.47 tokens/sec, 17.08 GB\n",
            " Run 2/3: 1.56 tokens/sec, 17.08 GB\n",
            " Run 3/3: 1.56 tokens/sec, 17.08 GB\n",
            "\n",
            "Baseline Results:\n",
            "  Avg Latency: 55.010 seconds\n",
            "  Avg Throughput: 1.53 tokens/sec\n",
            "  Avg Memory: 17.08 GB\n",
            "\n",
            "Sample Output:\n",
            "USER:  \n",
            "Please detail describe this image\n",
            " ASSISTANT: The image features a pier extending out over a large body of water, possibly a lake. The pier is made of wood and has a bench situated on it, prov...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python test_llmcompressor_awq_vllm.py ##Downlaod quant mdoel with AWQ_TRY()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iaoy3AakgThw",
        "outputId": "7233f7ee-c2c4-40ce-ae85-5b82660e83be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-03 18:04:12.831095: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-03 18:04:12.848621: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764785052.870455     810 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764785052.876998     810 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764785052.893542     810 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764785052.893572     810 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764785052.893575     810 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764785052.893577     810 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-03 18:04:12.898550: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "================================================================================\n",
            "AWQ Quantization Test for LLaVA\n",
            "================================================================================\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "config.json: 100% 950/950 [00:00<00:00, 8.30MB/s]\n",
            "model.safetensors.index.json: 70.1kB [00:00, 180MB/s]\n",
            "Fetching 3 files:   0% 0/3 [00:00<?, ?it/s]\n",
            "model-00003-of-00003.safetensors:   0% 0.00/4.18G [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:   0% 0.00/4.96G [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:   0% 0.00/4.99G [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:   0% 599k/4.96G [00:02<5:15:53, 262kB/s]\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:   0% 546k/4.18G [00:02<4:58:57, 233kB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:   0% 11.0k/4.99G [00:02<300:14:16, 4.62kB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:   1% 67.7M/4.96G [00:02<02:10, 37.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:   0% 160k/4.99G [00:02<16:26:52, 84.3kB/s]  \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:   3% 135M/4.96G [00:02<00:59, 81.4MB/s] \u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:   7% 336M/4.96G [00:02<00:21, 213MB/s] \u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:   9% 470M/4.96G [00:05<00:50, 88.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:   0% 285k/4.99G [00:05<26:35:04, 52.2kB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:   0% 1.74M/4.18G [00:05<3:44:20, 310kB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  14% 671M/4.96G [00:06<00:30, 139MB/s] \u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  18% 872M/4.96G [00:06<00:20, 195MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  22% 1.07G/4.96G [00:07<00:18, 213MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  26% 1.27G/4.96G [00:07<00:13, 264MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:   0% 2.53M/4.99G [00:12<5:37:19, 247kB/s] \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  28% 1.41G/4.96G [00:12<00:41, 85.9MB/s]\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:   2% 68.8M/4.18G [00:13<10:50, 6.31MB/s] \u001b[A\n",
            "model-00003-of-00003.safetensors:   3% 136M/4.18G [00:13<04:58, 13.5MB/s] \u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:   1% 44.6M/4.99G [00:14<16:09, 5.10MB/s] \u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:   6% 270M/4.18G [00:14<01:56, 33.7MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:   2% 106M/4.99G [00:14<05:33, 14.7MB/s] \u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  10% 404M/4.18G [00:15<01:08, 54.9MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  19% 807M/4.18G [00:15<00:23, 143MB/s] \u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:   3% 173M/4.99G [00:15<03:36, 22.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  24% 1.01G/4.18G [00:16<00:17, 177MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  27% 1.14G/4.18G [00:16<00:15, 191MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  31% 1.54G/4.96G [00:17<01:00, 56.7MB/s]\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  32% 1.34G/4.18G [00:17<00:13, 204MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  34% 1.41G/4.18G [00:19<00:24, 111MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  32% 1.61G/4.96G [00:19<01:10, 47.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  35% 1.75G/4.96G [00:20<00:52, 61.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:   5% 237M/4.99G [00:20<04:35, 17.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  35% 1.48G/4.18G [00:21<00:31, 86.4MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  40% 1.68G/4.18G [00:22<00:20, 123MB/s] \u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:   7% 338M/4.99G [00:22<02:56, 26.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  38% 1.88G/4.96G [00:22<00:47, 64.6MB/s]\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  43% 1.81G/4.18G [00:22<00:16, 144MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:   8% 405M/4.99G [00:22<02:07, 36.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  45% 1.88G/4.18G [00:26<00:32, 69.9MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:   9% 472M/4.99G [00:26<02:36, 28.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  10% 480M/4.99G [00:26<02:33, 29.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  39% 1.95G/4.96G [00:27<01:14, 40.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  41% 2.01G/4.96G [00:28<01:07, 43.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  45% 2.21G/4.96G [00:30<00:48, 56.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  49% 2.42G/4.96G [00:30<00:27, 92.6MB/s]\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  48% 2.01G/4.18G [00:30<00:45, 48.0MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  51% 2.15G/4.18G [00:31<00:30, 67.4MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  53% 2.21G/4.18G [00:31<00:26, 75.3MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  10% 514M/4.99G [00:31<04:39, 16.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  56% 2.35G/4.18G [00:31<00:17, 105MB/s] \u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  15% 747M/4.99G [00:32<01:26, 48.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  58% 2.42G/4.18G [00:32<00:18, 93.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  18% 881M/4.99G [00:32<01:02, 66.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  61% 2.55G/4.18G [00:33<00:16, 101MB/s] \u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  53% 2.62G/4.96G [00:34<00:32, 72.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  54% 2.68G/4.96G [00:34<00:28, 80.6MB/s]\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  63% 2.62G/4.18G [00:34<00:15, 98.6MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  55% 2.75G/4.96G [00:35<00:26, 81.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  18% 910M/4.99G [00:35<01:38, 41.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  58% 2.89G/4.96G [00:36<00:21, 98.6MB/s]\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  64% 2.68G/4.18G [00:36<00:20, 71.4MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  20% 1.01G/4.99G [00:36<01:17, 51.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  66% 2.75G/4.18G [00:37<00:20, 70.8MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  60% 2.95G/4.96G [00:39<00:32, 61.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  64% 3.15G/4.96G [00:40<00:20, 86.5MB/s]\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  69% 2.89G/4.18G [00:42<00:31, 41.3MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  65% 3.22G/4.96G [00:42<00:28, 61.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  22% 1.07G/4.99G [00:43<02:28, 26.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  68% 3.35G/4.96G [00:43<00:19, 82.9MB/s]\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  74% 3.09G/4.18G [00:43<00:17, 64.1MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  70% 3.49G/4.96G [00:43<00:13, 106MB/s] \u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  72% 3.56G/4.96G [00:44<00:13, 103MB/s]\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  76% 3.15G/4.18G [00:45<00:16, 60.7MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  74% 3.69G/4.96G [00:45<00:10, 123MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  23% 1.14G/4.99G [00:46<02:34, 24.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  77% 3.22G/4.18G [00:46<00:17, 54.2MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  82% 3.42G/4.18G [00:47<00:08, 92.3MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  84% 3.49G/4.18G [00:47<00:06, 104MB/s] \u001b[A\n",
            "model-00003-of-00003.safetensors:  85% 3.56G/4.18G [00:48<00:05, 107MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  76% 3.76G/4.96G [00:48<00:18, 64.2MB/s]\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  87% 3.62G/4.18G [00:48<00:04, 111MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  77% 3.82G/4.96G [00:48<00:15, 72.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  24% 1.21G/4.99G [00:48<02:29, 25.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  78% 3.89G/4.96G [00:49<00:12, 85.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  80% 3.96G/4.96G [00:49<00:09, 105MB/s] \u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  88% 3.69G/4.18G [00:49<00:04, 100MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  26% 1.27G/4.99G [00:50<02:04, 29.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  27% 1.34G/4.99G [00:50<01:34, 38.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  90% 3.76G/4.18G [00:51<00:05, 78.5MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  28% 1.41G/4.99G [00:51<01:13, 48.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  83% 4.09G/4.96G [00:51<00:10, 86.1MB/s]\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  92% 3.82G/4.18G [00:51<00:03, 88.6MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  30% 1.48G/4.99G [00:51<01:01, 57.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  84% 4.16G/4.96G [00:51<00:08, 89.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  31% 1.54G/4.99G [00:52<00:53, 64.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  85% 4.23G/4.96G [00:52<00:08, 85.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  87% 4.29G/4.96G [00:53<00:06, 98.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  32% 1.61G/4.99G [00:53<00:59, 57.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  33% 1.67G/4.99G [00:54<00:46, 71.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  93% 3.89G/4.18G [00:54<00:06, 46.6MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  88% 4.36G/4.96G [00:55<00:10, 58.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  35% 1.74G/4.99G [00:56<01:01, 52.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  96% 4.02G/4.18G [00:56<00:02, 56.5MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  89% 4.43G/4.96G [00:56<00:08, 63.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  91% 4.50G/4.96G [00:56<00:05, 80.9MB/s]\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  97% 4.04G/4.18G [00:56<00:02, 56.4MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  92% 4.56G/4.96G [00:57<00:03, 101MB/s] \u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  93% 4.63G/4.96G [00:57<00:02, 124MB/s]\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  98% 4.11G/4.18G [00:57<00:01, 64.1MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  36% 1.81G/4.99G [00:58<01:16, 41.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  38% 1.87G/4.99G [00:59<01:02, 50.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  95% 4.70G/4.96G [00:59<00:04, 60.4MB/s]\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors: 100% 4.18G/4.18G [01:00<00:00, 69.4MB/s]\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  39% 1.94G/4.99G [01:00<01:01, 49.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  40% 2.01G/4.99G [01:01<00:46, 64.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  42% 2.07G/4.99G [01:01<00:34, 84.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  96% 4.76G/4.96G [01:01<00:03, 53.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  43% 2.14G/4.99G [01:02<00:38, 73.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  44% 2.21G/4.99G [01:02<00:30, 90.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  46% 2.27G/4.99G [01:03<00:24, 109MB/s] \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  47% 2.34G/4.99G [01:03<00:23, 111MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  97% 4.83G/4.96G [01:04<00:03, 38.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  48% 2.41G/4.99G [01:04<00:26, 98.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  99% 4.89G/4.96G [01:04<00:01, 46.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  50% 2.48G/4.99G [01:05<00:23, 109MB/s] \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  51% 2.54G/4.99G [01:05<00:19, 123MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  54% 2.68G/4.99G [01:05<00:11, 194MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors: 100% 4.96G/4.96G [01:05<00:00, 75.3MB/s]\n",
            "\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  55% 2.74G/4.99G [01:05<00:10, 206MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  56% 2.81G/4.99G [01:06<00:10, 202MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  58% 2.88G/4.99G [01:06<00:10, 209MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  59% 2.95G/4.99G [01:07<00:11, 183MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  60% 3.01G/4.99G [01:07<00:09, 218MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  62% 3.08G/4.99G [01:07<00:07, 250MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  66% 3.28G/4.99G [01:07<00:04, 377MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  68% 3.41G/4.99G [01:09<00:08, 180MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  70% 3.48G/4.99G [01:09<00:09, 167MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  71% 3.55G/4.99G [01:09<00:07, 184MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  72% 3.62G/4.99G [01:10<00:06, 201MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  75% 3.75G/4.99G [01:10<00:04, 263MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  77% 3.82G/4.99G [01:10<00:04, 270MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  78% 3.89G/4.99G [01:11<00:04, 238MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  79% 3.96G/4.99G [01:12<00:08, 122MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  81% 4.03G/4.99G [01:12<00:07, 126MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  82% 4.09G/4.99G [01:13<00:06, 131MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  83% 4.16G/4.99G [01:13<00:05, 165MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  87% 4.36G/4.99G [01:13<00:01, 317MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  89% 4.43G/4.99G [01:13<00:01, 314MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  91% 4.53G/4.99G [01:14<00:01, 313MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  92% 4.59G/4.99G [01:14<00:01, 323MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  95% 4.73G/4.99G [01:14<00:00, 381MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  96% 4.79G/4.99G [01:14<00:00, 367MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  97% 4.86G/4.99G [01:15<00:00, 366MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  99% 4.93G/4.99G [01:15<00:00, 364MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors: 100% 4.99G/4.99G [01:15<00:00, 66.2MB/s]\n",
            "Fetching 3 files: 100% 3/3 [01:15<00:00, 25.31s/it]\n",
            "Loading checkpoint shards: 100% 3/3 [00:04<00:00,  1.40s/it]\n",
            "generation_config.json: 100% 141/141 [00:00<00:00, 1.20MB/s]\n",
            "processor_config.json: 100% 173/173 [00:00<00:00, 1.91MB/s]\n",
            "chat_template.json: 100% 701/701 [00:00<00:00, 7.48MB/s]\n",
            "chat_template.jinja: 100% 674/674 [00:00<00:00, 7.46MB/s]\n",
            "preprocessor_config.json: 100% 505/505 [00:00<00:00, 4.61MB/s]\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "tokenizer_config.json: 1.45kB [00:00, 7.56MB/s]\n",
            "tokenizer.model: 100% 500k/500k [00:00<00:00, 987kB/s]\n",
            "tokenizer.json: 3.62MB [00:00, 177MB/s]\n",
            "added_tokens.json: 100% 41.0/41.0 [00:00<00:00, 459kB/s]\n",
            "special_tokens_map.json: 100% 552/552 [00:00<00:00, 6.11MB/s]\n",
            "README.md: 2.23kB [00:00, 12.6MB/s]\n",
            "data/test-00000-of-00009.parquet: 100% 471M/471M [00:03<00:00, 139MB/s]\n",
            "data/test-00001-of-00009.parquet: 100% 475M/475M [00:04<00:00, 116MB/s]\n",
            "data/test-00002-of-00009.parquet: 100% 486M/486M [00:04<00:00, 119MB/s]\n",
            "data/test-00003-of-00009.parquet: 100% 472M/472M [00:09<00:00, 47.4MB/s]\n",
            "data/test-00004-of-00009.parquet: 100% 490M/490M [00:05<00:00, 91.1MB/s]\n",
            "data/test-00005-of-00009.parquet: 100% 500M/500M [00:14<00:00, 34.4MB/s]\n",
            "data/test-00006-of-00009.parquet: 100% 528M/528M [00:05<00:00, 102MB/s]\n",
            "data/test-00007-of-00009.parquet: 100% 511M/511M [00:04<00:00, 120MB/s]\n",
            "data/test-00008-of-00009.parquet: 100% 477M/477M [00:03<00:00, 128MB/s]\n",
            "Generating test split: 100% 31783/31783 [00:11<00:00, 2770.77 examples/s]\n",
            "Preprocessing: 100% 32/32 [00:01<00:00, 25.58 examples/s] \n",
            "Tokenizing: 100% 32/32 [00:00<00:00, 60.70 examples/s] \n",
            "2025-12-03T18:07:31.820450+0000 | reset | INFO - Compression lifecycle reset\n",
            "2025-12-03T18:07:31.830737+0000 | _create_default_logger | INFO - Logging all LLM Compressor modifier-level logs to sparse_logs/03-12-2025_18.07.31.log\n",
            "2025-12-03T18:07:31.831189+0000 | from_modifiers | INFO - Creating recipe from modifiers\n",
            "2025-12-03T18:07:31.913747+0000 | initialize | INFO - Compression lifecycle initialized for 1 modifiers\n",
            "2025-12-03T18:07:31.913984+0000 | IndependentPipeline | INFO - Inferred `SequentialPipeline` for `GPTQModifier`\n",
            "2025-12-03T18:07:41.248778+0000 | get_sequential_targets | WARNING - Passing sequential targets through modifiers is deprecated, please use `oneshot(sequential_targets=...)`\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Preparing cache:   0% 0/32 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Preparing cache: 100% 32/32 [00:03<00:00,  9.75it/s]\n",
            "(1/33): Calibrating: 100% 32/32 [00:05<00:00,  5.92it/s]\n",
            "2025-12-03T18:07:52.500286+0000 | compress_modules | INFO - Quantizing model.language_model.layers.0.self_attn.q_proj using 32 samples\n",
            "2025-12-03T18:07:55.401602+0000 | compress | METRIC - time 2.90s\n",
            "2025-12-03T18:07:55.401882+0000 | compress | METRIC - error 0.27\n",
            "2025-12-03T18:07:55.402457+0000 | compress | METRIC - GPU 0 | usage: 15.12% | total memory: 24 GB\n",
            "2025-12-03T18:07:55.402691+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:07:55.402993+0000 | compress_modules | INFO - Quantizing model.language_model.layers.0.self_attn.k_proj using 32 samples\n",
            "2025-12-03T18:07:57.514467+0000 | compress | METRIC - time 2.11s\n",
            "2025-12-03T18:07:57.515229+0000 | compress | METRIC - error 0.25\n",
            "2025-12-03T18:07:57.515480+0000 | compress | METRIC - GPU 0 | usage: 15.12% | total memory: 24 GB\n",
            "2025-12-03T18:07:57.515648+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:07:57.515905+0000 | compress_modules | INFO - Quantizing model.language_model.layers.0.self_attn.v_proj using 32 samples\n",
            "2025-12-03T18:07:59.623847+0000 | compress | METRIC - time 2.11s\n",
            "2025-12-03T18:07:59.624606+0000 | compress | METRIC - error 0.02\n",
            "2025-12-03T18:07:59.624852+0000 | compress | METRIC - GPU 0 | usage: 15.12% | total memory: 24 GB\n",
            "2025-12-03T18:07:59.625027+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:07:59.625279+0000 | compress_modules | INFO - Quantizing model.language_model.layers.0.self_attn.o_proj using 32 samples\n",
            "2025-12-03T18:08:01.775998+0000 | compress | METRIC - time 2.15s\n",
            "2025-12-03T18:08:01.776773+0000 | compress | METRIC - error 0.00\n",
            "2025-12-03T18:08:01.777029+0000 | compress | METRIC - GPU 0 | usage: 15.12% | total memory: 24 GB\n",
            "2025-12-03T18:08:01.777212+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:08:01.777457+0000 | compress_modules | INFO - Quantizing model.language_model.layers.0.mlp.gate_proj using 32 samples\n",
            "2025-12-03T18:08:04.064474+0000 | compress | METRIC - time 2.29s\n",
            "2025-12-03T18:08:04.065281+0000 | compress | METRIC - error 0.09\n",
            "2025-12-03T18:08:04.065511+0000 | compress | METRIC - GPU 0 | usage: 15.82% | total memory: 24 GB\n",
            "2025-12-03T18:08:04.065681+0000 | compress | METRIC - Compressed module size: 90.21056 MB\n",
            "2025-12-03T18:08:04.065935+0000 | compress_modules | INFO - Quantizing model.language_model.layers.0.mlp.up_proj using 32 samples\n",
            "2025-12-03T18:08:06.317394+0000 | compress | METRIC - time 2.25s\n",
            "2025-12-03T18:08:06.318206+0000 | compress | METRIC - error 0.08\n",
            "2025-12-03T18:08:06.318438+0000 | compress | METRIC - GPU 0 | usage: 15.82% | total memory: 24 GB\n",
            "2025-12-03T18:08:06.318601+0000 | compress | METRIC - Compressed module size: 90.21056 MB\n",
            "2025-12-03T18:08:06.318869+0000 | compress_modules | INFO - Quantizing model.language_model.layers.0.mlp.down_proj using 32 samples\n",
            "2025-12-03T18:08:12.441887+0000 | compress | METRIC - time 6.12s\n",
            "2025-12-03T18:08:12.443879+0000 | compress | METRIC - error 0.00\n",
            "2025-12-03T18:08:12.444158+0000 | compress | METRIC - GPU 0 | usage: 19.85% | total memory: 24 GB\n",
            "2025-12-03T18:08:12.444350+0000 | compress | METRIC - Compressed module size: 90.189824 MB\n",
            "(1/33): Propagating: 100% 32/32 [00:01<00:00, 28.52it/s]\n",
            "(2/33): Calibrating: 100% 32/32 [00:01<00:00, 16.09it/s]\n",
            "2025-12-03T18:08:16.428137+0000 | compress_modules | INFO - Quantizing model.language_model.layers.1.self_attn.q_proj using 32 samples\n",
            "2025-12-03T18:08:18.594645+0000 | compress | METRIC - time 2.17s\n",
            "2025-12-03T18:08:18.595630+0000 | compress | METRIC - error 0.59\n",
            "2025-12-03T18:08:18.595881+0000 | compress | METRIC - GPU 0 | usage: 11.34% | total memory: 24 GB\n",
            "2025-12-03T18:08:18.596059+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:08:18.596328+0000 | compress_modules | INFO - Quantizing model.language_model.layers.1.self_attn.k_proj using 32 samples\n",
            "2025-12-03T18:08:20.691964+0000 | compress | METRIC - time 2.10s\n",
            "2025-12-03T18:08:20.692933+0000 | compress | METRIC - error 0.64\n",
            "2025-12-03T18:08:20.693183+0000 | compress | METRIC - GPU 0 | usage: 11.34% | total memory: 24 GB\n",
            "2025-12-03T18:08:20.693354+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:08:20.693607+0000 | compress_modules | INFO - Quantizing model.language_model.layers.1.self_attn.v_proj using 32 samples\n",
            "2025-12-03T18:08:22.770565+0000 | compress | METRIC - time 2.08s\n",
            "2025-12-03T18:08:22.771554+0000 | compress | METRIC - error 0.05\n",
            "2025-12-03T18:08:22.771800+0000 | compress | METRIC - GPU 0 | usage: 11.34% | total memory: 24 GB\n",
            "2025-12-03T18:08:22.771972+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:08:22.772231+0000 | compress_modules | INFO - Quantizing model.language_model.layers.1.self_attn.o_proj using 32 samples\n",
            "2025-12-03T18:08:24.844639+0000 | compress | METRIC - time 2.07s\n",
            "2025-12-03T18:08:24.845624+0000 | compress | METRIC - error 0.00\n",
            "2025-12-03T18:08:24.845882+0000 | compress | METRIC - GPU 0 | usage: 11.34% | total memory: 24 GB\n",
            "2025-12-03T18:08:24.846054+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:08:24.846328+0000 | compress_modules | INFO - Quantizing model.language_model.layers.1.mlp.gate_proj using 32 samples\n",
            "2025-12-03T18:08:27.122759+0000 | compress | METRIC - time 2.28s\n",
            "2025-12-03T18:08:27.123761+0000 | compress | METRIC - error 0.33\n",
            "2025-12-03T18:08:27.124021+0000 | compress | METRIC - GPU 0 | usage: 12.05% | total memory: 24 GB\n",
            "2025-12-03T18:08:27.124193+0000 | compress | METRIC - Compressed module size: 90.21056 MB\n",
            "2025-12-03T18:08:27.124514+0000 | compress_modules | INFO - Quantizing model.language_model.layers.1.mlp.up_proj using 32 samples\n",
            "2025-12-03T18:08:29.363979+0000 | compress | METRIC - time 2.24s\n",
            "2025-12-03T18:08:29.364972+0000 | compress | METRIC - error 0.28\n",
            "2025-12-03T18:08:29.365210+0000 | compress | METRIC - GPU 0 | usage: 12.05% | total memory: 24 GB\n",
            "2025-12-03T18:08:29.365372+0000 | compress | METRIC - Compressed module size: 90.21056 MB\n",
            "2025-12-03T18:08:29.365627+0000 | compress_modules | INFO - Quantizing model.language_model.layers.1.mlp.down_proj using 32 samples\n",
            "2025-12-03T18:08:35.528276+0000 | compress | METRIC - time 6.16s\n",
            "2025-12-03T18:08:35.530336+0000 | compress | METRIC - error 4.70\n",
            "2025-12-03T18:08:35.530583+0000 | compress | METRIC - GPU 0 | usage: 16.07% | total memory: 24 GB\n",
            "2025-12-03T18:08:35.530745+0000 | compress | METRIC - Compressed module size: 90.189824 MB\n",
            "(2/33): Propagating: 100% 32/32 [00:00<00:00, 82.68it/s]\n",
            "(3/33): Calibrating: 100% 32/32 [00:01<00:00, 16.15it/s]\n",
            "2025-12-03T18:08:38.206320+0000 | compress_modules | INFO - Quantizing model.language_model.layers.2.self_attn.q_proj using 32 samples\n",
            "2025-12-03T18:08:40.350279+0000 | compress | METRIC - time 2.14s\n",
            "2025-12-03T18:08:40.351265+0000 | compress | METRIC - error 1.96\n",
            "2025-12-03T18:08:40.351500+0000 | compress | METRIC - GPU 0 | usage: 11.34% | total memory: 24 GB\n",
            "2025-12-03T18:08:40.351659+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:08:40.351933+0000 | compress_modules | INFO - Quantizing model.language_model.layers.2.self_attn.k_proj using 32 samples\n",
            "2025-12-03T18:08:42.432228+0000 | compress | METRIC - time 2.08s\n",
            "2025-12-03T18:08:42.433175+0000 | compress | METRIC - error 1.39\n",
            "2025-12-03T18:08:42.433402+0000 | compress | METRIC - GPU 0 | usage: 11.34% | total memory: 24 GB\n",
            "2025-12-03T18:08:42.433561+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:08:42.433822+0000 | compress_modules | INFO - Quantizing model.language_model.layers.2.self_attn.v_proj using 32 samples\n",
            "2025-12-03T18:08:44.551073+0000 | compress | METRIC - time 2.12s\n",
            "2025-12-03T18:08:44.552043+0000 | compress | METRIC - error 0.23\n",
            "2025-12-03T18:08:44.552293+0000 | compress | METRIC - GPU 0 | usage: 11.34% | total memory: 24 GB\n",
            "2025-12-03T18:08:44.552463+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:08:44.552713+0000 | compress_modules | INFO - Quantizing model.language_model.layers.2.self_attn.o_proj using 32 samples\n",
            "2025-12-03T18:08:46.654858+0000 | compress | METRIC - time 2.10s\n",
            "2025-12-03T18:08:46.655863+0000 | compress | METRIC - error 0.01\n",
            "2025-12-03T18:08:46.656104+0000 | compress | METRIC - GPU 0 | usage: 11.34% | total memory: 24 GB\n",
            "2025-12-03T18:08:46.656277+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:08:46.656524+0000 | compress_modules | INFO - Quantizing model.language_model.layers.2.mlp.gate_proj using 32 samples\n",
            "2025-12-03T18:08:48.892808+0000 | compress | METRIC - time 2.24s\n",
            "2025-12-03T18:08:48.893763+0000 | compress | METRIC - error 0.63\n",
            "2025-12-03T18:08:48.894012+0000 | compress | METRIC - GPU 0 | usage: 12.05% | total memory: 24 GB\n",
            "2025-12-03T18:08:48.894196+0000 | compress | METRIC - Compressed module size: 90.21056 MB\n",
            "2025-12-03T18:08:48.894454+0000 | compress_modules | INFO - Quantizing model.language_model.layers.2.mlp.up_proj using 32 samples\n",
            "2025-12-03T18:08:51.166659+0000 | compress | METRIC - time 2.27s\n",
            "2025-12-03T18:08:51.167643+0000 | compress | METRIC - error 0.53\n",
            "2025-12-03T18:08:51.167890+0000 | compress | METRIC - GPU 0 | usage: 12.05% | total memory: 24 GB\n",
            "2025-12-03T18:08:51.168063+0000 | compress | METRIC - Compressed module size: 90.21056 MB\n",
            "2025-12-03T18:08:51.168318+0000 | compress_modules | INFO - Quantizing model.language_model.layers.2.mlp.down_proj using 32 samples\n",
            "2025-12-03T18:08:57.372034+0000 | compress | METRIC - time 6.20s\n",
            "2025-12-03T18:08:57.374182+0000 | compress | METRIC - error 0.01\n",
            "2025-12-03T18:08:57.374442+0000 | compress | METRIC - GPU 0 | usage: 16.07% | total memory: 24 GB\n",
            "2025-12-03T18:08:57.374619+0000 | compress | METRIC - Compressed module size: 90.189824 MB\n",
            "(3/33): Propagating: 100% 32/32 [00:00<00:00, 85.27it/s]\n",
            "(4/33): Calibrating: 100% 32/32 [00:02<00:00, 15.88it/s]\n",
            "2025-12-03T18:09:00.066910+0000 | compress_modules | INFO - Quantizing model.language_model.layers.3.self_attn.q_proj using 32 samples\n",
            "2025-12-03T18:09:02.207375+0000 | compress | METRIC - time 2.14s\n",
            "2025-12-03T18:09:02.208414+0000 | compress | METRIC - error 5.14\n",
            "2025-12-03T18:09:02.208658+0000 | compress | METRIC - GPU 0 | usage: 11.34% | total memory: 24 GB\n",
            "2025-12-03T18:09:02.208843+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:09:02.209117+0000 | compress_modules | INFO - Quantizing model.language_model.layers.3.self_attn.k_proj using 32 samples\n",
            "2025-12-03T18:09:04.303661+0000 | compress | METRIC - time 2.09s\n",
            "2025-12-03T18:09:04.304674+0000 | compress | METRIC - error 2.70\n",
            "2025-12-03T18:09:04.304927+0000 | compress | METRIC - GPU 0 | usage: 11.34% | total memory: 24 GB\n",
            "2025-12-03T18:09:04.305098+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:09:04.305383+0000 | compress_modules | INFO - Quantizing model.language_model.layers.3.self_attn.v_proj using 32 samples\n",
            "2025-12-03T18:09:06.416541+0000 | compress | METRIC - time 2.11s\n",
            "2025-12-03T18:09:06.417585+0000 | compress | METRIC - error 0.56\n",
            "2025-12-03T18:09:06.417840+0000 | compress | METRIC - GPU 0 | usage: 11.34% | total memory: 24 GB\n",
            "2025-12-03T18:09:06.418010+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:09:06.418290+0000 | compress_modules | INFO - Quantizing model.language_model.layers.3.self_attn.o_proj using 32 samples\n",
            "2025-12-03T18:09:08.522163+0000 | compress | METRIC - time 2.10s\n",
            "2025-12-03T18:09:08.523228+0000 | compress | METRIC - error 0.01\n",
            "2025-12-03T18:09:08.523467+0000 | compress | METRIC - GPU 0 | usage: 11.34% | total memory: 24 GB\n",
            "2025-12-03T18:09:08.523629+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:09:08.523895+0000 | compress_modules | INFO - Quantizing model.language_model.layers.3.mlp.gate_proj using 32 samples\n",
            "2025-12-03T18:09:10.755259+0000 | compress | METRIC - time 2.23s\n",
            "2025-12-03T18:09:10.756261+0000 | compress | METRIC - error 1.06\n",
            "2025-12-03T18:09:10.756498+0000 | compress | METRIC - GPU 0 | usage: 12.05% | total memory: 24 GB\n",
            "2025-12-03T18:09:10.756659+0000 | compress | METRIC - Compressed module size: 90.21056 MB\n",
            "2025-12-03T18:09:10.756927+0000 | compress_modules | INFO - Quantizing model.language_model.layers.3.mlp.up_proj using 32 samples\n",
            "2025-12-03T18:09:13.009757+0000 | compress | METRIC - time 2.25s\n",
            "2025-12-03T18:09:13.010811+0000 | compress | METRIC - error 0.88\n",
            "2025-12-03T18:09:13.011066+0000 | compress | METRIC - GPU 0 | usage: 12.05% | total memory: 24 GB\n",
            "2025-12-03T18:09:13.011237+0000 | compress | METRIC - Compressed module size: 90.21056 MB\n",
            "2025-12-03T18:09:13.012058+0000 | compress_modules | INFO - Quantizing model.language_model.layers.3.mlp.down_proj using 32 samples\n",
            "2025-12-03T18:09:19.159127+0000 | compress | METRIC - time 6.15s\n",
            "2025-12-03T18:09:19.161241+0000 | compress | METRIC - error 0.03\n",
            "2025-12-03T18:09:19.161486+0000 | compress | METRIC - GPU 0 | usage: 16.07% | total memory: 24 GB\n",
            "2025-12-03T18:09:19.161640+0000 | compress | METRIC - Compressed module size: 90.189824 MB\n",
            "(4/33): Propagating: 100% 32/32 [00:00<00:00, 84.26it/s]\n",
            "(5/33): Calibrating: 100% 32/32 [00:02<00:00, 15.80it/s]\n",
            "2025-12-03T18:09:21.855880+0000 | compress_modules | INFO - Quantizing model.language_model.layers.4.self_attn.q_proj using 32 samples\n",
            "2025-12-03T18:09:23.993538+0000 | compress | METRIC - time 2.14s\n",
            "2025-12-03T18:09:23.994554+0000 | compress | METRIC - error 4.66\n",
            "2025-12-03T18:09:23.994831+0000 | compress | METRIC - GPU 0 | usage: 11.27% | total memory: 24 GB\n",
            "2025-12-03T18:09:23.995011+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:09:23.995275+0000 | compress_modules | INFO - Quantizing model.language_model.layers.4.self_attn.k_proj using 32 samples\n",
            "2025-12-03T18:09:26.061778+0000 | compress | METRIC - time 2.07s\n",
            "2025-12-03T18:09:26.062838+0000 | compress | METRIC - error 2.65\n",
            "2025-12-03T18:09:26.063083+0000 | compress | METRIC - GPU 0 | usage: 11.27% | total memory: 24 GB\n",
            "2025-12-03T18:09:26.063255+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:09:26.063522+0000 | compress_modules | INFO - Quantizing model.language_model.layers.4.self_attn.v_proj using 32 samples\n",
            "2025-12-03T18:09:28.153824+0000 | compress | METRIC - time 2.09s\n",
            "2025-12-03T18:09:28.154848+0000 | compress | METRIC - error 0.57\n",
            "2025-12-03T18:09:28.155091+0000 | compress | METRIC - GPU 0 | usage: 11.27% | total memory: 24 GB\n",
            "2025-12-03T18:09:28.155257+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:09:28.155501+0000 | compress_modules | INFO - Quantizing model.language_model.layers.4.self_attn.o_proj using 32 samples\n",
            "2025-12-03T18:09:30.289332+0000 | compress | METRIC - time 2.13s\n",
            "2025-12-03T18:09:30.290328+0000 | compress | METRIC - error 0.01\n",
            "2025-12-03T18:09:30.290557+0000 | compress | METRIC - GPU 0 | usage: 11.27% | total memory: 24 GB\n",
            "2025-12-03T18:09:30.290715+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:09:30.290977+0000 | compress_modules | INFO - Quantizing model.language_model.layers.4.mlp.gate_proj using 32 samples\n",
            "2025-12-03T18:09:32.579386+0000 | compress | METRIC - time 2.29s\n",
            "2025-12-03T18:09:32.580417+0000 | compress | METRIC - error 1.46\n",
            "2025-12-03T18:09:32.580653+0000 | compress | METRIC - GPU 0 | usage: 11.98% | total memory: 24 GB\n",
            "2025-12-03T18:09:32.580829+0000 | compress | METRIC - Compressed module size: 90.21056 MB\n",
            "2025-12-03T18:09:32.581094+0000 | compress_modules | INFO - Quantizing model.language_model.layers.4.mlp.up_proj using 32 samples\n",
            "2025-12-03T18:09:34.912267+0000 | compress | METRIC - time 2.33s\n",
            "2025-12-03T18:09:34.913288+0000 | compress | METRIC - error 1.16\n",
            "2025-12-03T18:09:34.913531+0000 | compress | METRIC - GPU 0 | usage: 11.98% | total memory: 24 GB\n",
            "2025-12-03T18:09:34.913692+0000 | compress | METRIC - Compressed module size: 90.21056 MB\n",
            "2025-12-03T18:09:34.913956+0000 | compress_modules | INFO - Quantizing model.language_model.layers.4.mlp.down_proj using 32 samples\n",
            "2025-12-03T18:09:41.095744+0000 | compress | METRIC - time 6.18s\n",
            "2025-12-03T18:09:41.097974+0000 | compress | METRIC - error 0.06\n",
            "2025-12-03T18:09:41.098228+0000 | compress | METRIC - GPU 0 | usage: 16.01% | total memory: 24 GB\n",
            "2025-12-03T18:09:41.098395+0000 | compress | METRIC - Compressed module size: 90.189824 MB\n",
            "(5/33): Propagating: 100% 32/32 [00:00<00:00, 83.43it/s]\n",
            "(6/33): Calibrating: 100% 32/32 [00:02<00:00, 15.62it/s]\n",
            "2025-12-03T18:09:43.837270+0000 | compress_modules | INFO - Quantizing model.language_model.layers.5.self_attn.q_proj using 32 samples\n",
            "2025-12-03T18:09:45.994534+0000 | compress | METRIC - time 2.16s\n",
            "2025-12-03T18:09:45.995561+0000 | compress | METRIC - error 3.74\n",
            "2025-12-03T18:09:45.995811+0000 | compress | METRIC - GPU 0 | usage: 11.27% | total memory: 24 GB\n",
            "2025-12-03T18:09:45.995982+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:09:45.996237+0000 | compress_modules | INFO - Quantizing model.language_model.layers.5.self_attn.k_proj using 32 samples\n",
            "2025-12-03T18:09:48.092251+0000 | compress | METRIC - time 2.10s\n",
            "2025-12-03T18:09:48.093268+0000 | compress | METRIC - error 3.06\n",
            "2025-12-03T18:09:48.093505+0000 | compress | METRIC - GPU 0 | usage: 11.27% | total memory: 24 GB\n",
            "2025-12-03T18:09:48.093667+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:09:48.093928+0000 | compress_modules | INFO - Quantizing model.language_model.layers.5.self_attn.v_proj using 32 samples\n",
            "2025-12-03T18:09:50.215680+0000 | compress | METRIC - time 2.12s\n",
            "2025-12-03T18:09:50.216710+0000 | compress | METRIC - error 0.70\n",
            "2025-12-03T18:09:50.216986+0000 | compress | METRIC - GPU 0 | usage: 11.27% | total memory: 24 GB\n",
            "2025-12-03T18:09:50.217183+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:09:50.217432+0000 | compress_modules | INFO - Quantizing model.language_model.layers.5.self_attn.o_proj using 32 samples\n",
            "2025-12-03T18:09:52.319735+0000 | compress | METRIC - time 2.10s\n",
            "2025-12-03T18:09:52.320779+0000 | compress | METRIC - error 0.02\n",
            "2025-12-03T18:09:52.321090+0000 | compress | METRIC - GPU 0 | usage: 11.27% | total memory: 24 GB\n",
            "2025-12-03T18:09:52.321339+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:09:52.321655+0000 | compress_modules | INFO - Quantizing model.language_model.layers.5.mlp.gate_proj using 32 samples\n",
            "2025-12-03T18:09:54.661416+0000 | compress | METRIC - time 2.34s\n",
            "2025-12-03T18:09:54.662450+0000 | compress | METRIC - error 1.77\n",
            "2025-12-03T18:09:54.662715+0000 | compress | METRIC - GPU 0 | usage: 11.98% | total memory: 24 GB\n",
            "2025-12-03T18:09:54.662900+0000 | compress | METRIC - Compressed module size: 90.21056 MB\n",
            "2025-12-03T18:09:54.663173+0000 | compress_modules | INFO - Quantizing model.language_model.layers.5.mlp.up_proj using 32 samples\n",
            "2025-12-03T18:09:56.934131+0000 | compress | METRIC - time 2.27s\n",
            "2025-12-03T18:09:56.935151+0000 | compress | METRIC - error 1.40\n",
            "2025-12-03T18:09:56.935417+0000 | compress | METRIC - GPU 0 | usage: 11.98% | total memory: 24 GB\n",
            "2025-12-03T18:09:56.935579+0000 | compress | METRIC - Compressed module size: 90.21056 MB\n",
            "2025-12-03T18:09:56.935841+0000 | compress_modules | INFO - Quantizing model.language_model.layers.5.mlp.down_proj using 32 samples\n",
            "2025-12-03T18:10:03.034974+0000 | compress | METRIC - time 6.10s\n",
            "2025-12-03T18:10:03.037283+0000 | compress | METRIC - error 0.08\n",
            "2025-12-03T18:10:03.037539+0000 | compress | METRIC - GPU 0 | usage: 16.01% | total memory: 24 GB\n",
            "2025-12-03T18:10:03.037709+0000 | compress | METRIC - Compressed module size: 90.189824 MB\n",
            "(6/33): Propagating: 100% 32/32 [00:00<00:00, 83.73it/s]\n",
            "(7/33): Calibrating: 100% 32/32 [00:02<00:00, 15.53it/s]\n",
            "2025-12-03T18:10:05.771564+0000 | compress_modules | INFO - Quantizing model.language_model.layers.6.self_attn.q_proj using 32 samples\n",
            "2025-12-03T18:10:07.965895+0000 | compress | METRIC - time 2.19s\n",
            "2025-12-03T18:10:07.966943+0000 | compress | METRIC - error 5.64\n",
            "2025-12-03T18:10:07.967220+0000 | compress | METRIC - GPU 0 | usage: 11.27% | total memory: 24 GB\n",
            "2025-12-03T18:10:07.967425+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:10:07.967716+0000 | compress_modules | INFO - Quantizing model.language_model.layers.6.self_attn.k_proj using 32 samples\n",
            "2025-12-03T18:10:10.070524+0000 | compress | METRIC - time 2.10s\n",
            "2025-12-03T18:10:10.071597+0000 | compress | METRIC - error 4.03\n",
            "2025-12-03T18:10:10.071851+0000 | compress | METRIC - GPU 0 | usage: 11.27% | total memory: 24 GB\n",
            "2025-12-03T18:10:10.072026+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:10:10.072279+0000 | compress_modules | INFO - Quantizing model.language_model.layers.6.self_attn.v_proj using 32 samples\n",
            "2025-12-03T18:10:12.213691+0000 | compress | METRIC - time 2.14s\n",
            "2025-12-03T18:10:12.214001+0000 | compress | METRIC - error 1.06\n",
            "2025-12-03T18:10:12.214229+0000 | compress | METRIC - GPU 0 | usage: 11.27% | total memory: 24 GB\n",
            "2025-12-03T18:10:12.214397+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:10:12.214649+0000 | compress_modules | INFO - Quantizing model.language_model.layers.6.self_attn.o_proj using 32 samples\n",
            "2025-12-03T18:10:14.333496+0000 | compress | METRIC - time 2.12s\n",
            "2025-12-03T18:10:14.334557+0000 | compress | METRIC - error 0.04\n",
            "2025-12-03T18:10:14.334810+0000 | compress | METRIC - GPU 0 | usage: 11.27% | total memory: 24 GB\n",
            "2025-12-03T18:10:14.334986+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:10:14.335249+0000 | compress_modules | INFO - Quantizing model.language_model.layers.6.mlp.gate_proj using 32 samples\n",
            "2025-12-03T18:10:16.580462+0000 | compress | METRIC - time 2.25s\n",
            "2025-12-03T18:10:16.581512+0000 | compress | METRIC - error 2.26\n",
            "2025-12-03T18:10:16.581755+0000 | compress | METRIC - GPU 0 | usage: 11.98% | total memory: 24 GB\n",
            "2025-12-03T18:10:16.581941+0000 | compress | METRIC - Compressed module size: 90.21056 MB\n",
            "2025-12-03T18:10:16.582204+0000 | compress_modules | INFO - Quantizing model.language_model.layers.6.mlp.up_proj using 32 samples\n",
            "2025-12-03T18:10:18.887774+0000 | compress | METRIC - time 2.31s\n",
            "2025-12-03T18:10:18.888882+0000 | compress | METRIC - error 1.72\n",
            "2025-12-03T18:10:18.889132+0000 | compress | METRIC - GPU 0 | usage: 11.98% | total memory: 24 GB\n",
            "2025-12-03T18:10:18.889303+0000 | compress | METRIC - Compressed module size: 90.21056 MB\n",
            "2025-12-03T18:10:18.889556+0000 | compress_modules | INFO - Quantizing model.language_model.layers.6.mlp.down_proj using 32 samples\n",
            "2025-12-03T18:10:25.035745+0000 | compress | METRIC - time 6.15s\n",
            "2025-12-03T18:10:25.038015+0000 | compress | METRIC - error 0.11\n",
            "2025-12-03T18:10:25.038261+0000 | compress | METRIC - GPU 0 | usage: 16.01% | total memory: 24 GB\n",
            "2025-12-03T18:10:25.038423+0000 | compress | METRIC - Compressed module size: 90.189824 MB\n",
            "(7/33): Propagating: 100% 32/32 [00:00<00:00, 83.72it/s]\n",
            "(8/33): Calibrating: 100% 32/32 [00:02<00:00, 15.38it/s]\n",
            "2025-12-03T18:10:27.805521+0000 | compress_modules | INFO - Quantizing model.language_model.layers.7.self_attn.q_proj using 32 samples\n",
            "2025-12-03T18:10:29.989943+0000 | compress | METRIC - time 2.18s\n",
            "2025-12-03T18:10:29.990991+0000 | compress | METRIC - error 5.63\n",
            "2025-12-03T18:10:29.991282+0000 | compress | METRIC - GPU 0 | usage: 11.27% | total memory: 24 GB\n",
            "2025-12-03T18:10:29.991521+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:10:29.991884+0000 | compress_modules | INFO - Quantizing model.language_model.layers.7.self_attn.k_proj using 32 samples\n",
            "2025-12-03T18:10:32.108713+0000 | compress | METRIC - time 2.12s\n",
            "2025-12-03T18:10:32.109749+0000 | compress | METRIC - error 4.46\n",
            "2025-12-03T18:10:32.109998+0000 | compress | METRIC - GPU 0 | usage: 11.27% | total memory: 24 GB\n",
            "2025-12-03T18:10:32.110159+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:10:32.110418+0000 | compress_modules | INFO - Quantizing model.language_model.layers.7.self_attn.v_proj using 32 samples\n",
            "2025-12-03T18:10:34.198041+0000 | compress | METRIC - time 2.09s\n",
            "2025-12-03T18:10:34.199062+0000 | compress | METRIC - error 1.30\n",
            "2025-12-03T18:10:34.199316+0000 | compress | METRIC - GPU 0 | usage: 11.27% | total memory: 24 GB\n",
            "2025-12-03T18:10:34.199479+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:10:34.199727+0000 | compress_modules | INFO - Quantizing model.language_model.layers.7.self_attn.o_proj using 32 samples\n",
            "2025-12-03T18:10:36.280217+0000 | compress | METRIC - time 2.08s\n",
            "2025-12-03T18:10:36.281260+0000 | compress | METRIC - error 0.06\n",
            "2025-12-03T18:10:36.281493+0000 | compress | METRIC - GPU 0 | usage: 11.27% | total memory: 24 GB\n",
            "2025-12-03T18:10:36.281653+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:10:36.281918+0000 | compress_modules | INFO - Quantizing model.language_model.layers.7.mlp.gate_proj using 32 samples\n",
            "2025-12-03T18:10:38.609127+0000 | compress | METRIC - time 2.33s\n",
            "2025-12-03T18:10:38.610226+0000 | compress | METRIC - error 2.82\n",
            "2025-12-03T18:10:38.610476+0000 | compress | METRIC - GPU 0 | usage: 11.98% | total memory: 24 GB\n",
            "2025-12-03T18:10:38.610641+0000 | compress | METRIC - Compressed module size: 90.21056 MB\n",
            "2025-12-03T18:10:38.610925+0000 | compress_modules | INFO - Quantizing model.language_model.layers.7.mlp.up_proj using 32 samples\n",
            "2025-12-03T18:10:40.866963+0000 | compress | METRIC - time 2.26s\n",
            "2025-12-03T18:10:40.868012+0000 | compress | METRIC - error 2.16\n",
            "2025-12-03T18:10:40.868252+0000 | compress | METRIC - GPU 0 | usage: 11.98% | total memory: 24 GB\n",
            "2025-12-03T18:10:40.868411+0000 | compress | METRIC - Compressed module size: 90.21056 MB\n",
            "2025-12-03T18:10:40.868678+0000 | compress_modules | INFO - Quantizing model.language_model.layers.7.mlp.down_proj using 32 samples\n",
            "2025-12-03T18:10:47.084221+0000 | compress | METRIC - time 6.22s\n",
            "2025-12-03T18:10:47.086385+0000 | compress | METRIC - error 0.16\n",
            "2025-12-03T18:10:47.086633+0000 | compress | METRIC - GPU 0 | usage: 16.01% | total memory: 24 GB\n",
            "2025-12-03T18:10:47.086807+0000 | compress | METRIC - Compressed module size: 90.189824 MB\n",
            "(8/33): Propagating: 100% 32/32 [00:00<00:00, 82.59it/s]\n",
            "(9/33): Calibrating: 100% 32/32 [00:02<00:00, 15.37it/s]\n",
            "2025-12-03T18:10:49.861733+0000 | compress_modules | INFO - Quantizing model.language_model.layers.8.self_attn.q_proj using 32 samples\n",
            "2025-12-03T18:10:52.002510+0000 | compress | METRIC - time 2.14s\n",
            "2025-12-03T18:10:52.003565+0000 | compress | METRIC - error 5.53\n",
            "2025-12-03T18:10:52.003824+0000 | compress | METRIC - GPU 0 | usage: 11.27% | total memory: 24 GB\n",
            "2025-12-03T18:10:52.003997+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:10:52.004260+0000 | compress_modules | INFO - Quantizing model.language_model.layers.8.self_attn.k_proj using 32 samples\n",
            "2025-12-03T18:10:54.103871+0000 | compress | METRIC - time 2.10s\n",
            "2025-12-03T18:10:54.104925+0000 | compress | METRIC - error 4.87\n",
            "2025-12-03T18:10:54.105167+0000 | compress | METRIC - GPU 0 | usage: 11.27% | total memory: 24 GB\n",
            "2025-12-03T18:10:54.105343+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:10:54.105597+0000 | compress_modules | INFO - Quantizing model.language_model.layers.8.self_attn.v_proj using 32 samples\n",
            "2025-12-03T18:10:56.226932+0000 | compress | METRIC - time 2.12s\n",
            "2025-12-03T18:10:56.228100+0000 | compress | METRIC - error 1.39\n",
            "2025-12-03T18:10:56.228372+0000 | compress | METRIC - GPU 0 | usage: 11.27% | total memory: 24 GB\n",
            "2025-12-03T18:10:56.228560+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:10:56.228850+0000 | compress_modules | INFO - Quantizing model.language_model.layers.8.self_attn.o_proj using 32 samples\n",
            "2025-12-03T18:10:58.267536+0000 | compress | METRIC - time 2.04s\n",
            "2025-12-03T18:10:58.268594+0000 | compress | METRIC - error 0.12\n",
            "2025-12-03T18:10:58.268845+0000 | compress | METRIC - GPU 0 | usage: 11.27% | total memory: 24 GB\n",
            "2025-12-03T18:10:58.269015+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:10:58.269274+0000 | compress_modules | INFO - Quantizing model.language_model.layers.8.mlp.gate_proj using 32 samples\n",
            "2025-12-03T18:11:00.531932+0000 | compress | METRIC - time 2.26s\n",
            "2025-12-03T18:11:00.532968+0000 | compress | METRIC - error 3.08\n",
            "2025-12-03T18:11:00.533216+0000 | compress | METRIC - GPU 0 | usage: 11.98% | total memory: 24 GB\n",
            "2025-12-03T18:11:00.533384+0000 | compress | METRIC - Compressed module size: 90.21056 MB\n",
            "2025-12-03T18:11:00.533638+0000 | compress_modules | INFO - Quantizing model.language_model.layers.8.mlp.up_proj using 32 samples\n",
            "2025-12-03T18:11:02.795362+0000 | compress | METRIC - time 2.26s\n",
            "2025-12-03T18:11:02.796424+0000 | compress | METRIC - error 2.54\n",
            "2025-12-03T18:11:02.796651+0000 | compress | METRIC - GPU 0 | usage: 11.98% | total memory: 24 GB\n",
            "2025-12-03T18:11:02.796823+0000 | compress | METRIC - Compressed module size: 90.21056 MB\n",
            "2025-12-03T18:11:02.797105+0000 | compress_modules | INFO - Quantizing model.language_model.layers.8.mlp.down_proj using 32 samples\n",
            "2025-12-03T18:11:08.976183+0000 | compress | METRIC - time 6.18s\n",
            "2025-12-03T18:11:08.978552+0000 | compress | METRIC - error 0.19\n",
            "2025-12-03T18:11:08.978826+0000 | compress | METRIC - GPU 0 | usage: 16.01% | total memory: 24 GB\n",
            "2025-12-03T18:11:08.979004+0000 | compress | METRIC - Compressed module size: 90.189824 MB\n",
            "(9/33): Propagating: 100% 32/32 [00:00<00:00, 83.78it/s]\n",
            "(10/33): Calibrating: 100% 32/32 [00:02<00:00, 15.45it/s]\n",
            "2025-12-03T18:11:11.739465+0000 | compress_modules | INFO - Quantizing model.language_model.layers.9.self_attn.q_proj using 32 samples\n",
            "2025-12-03T18:11:13.865377+0000 | compress | METRIC - time 2.13s\n",
            "2025-12-03T18:11:13.866451+0000 | compress | METRIC - error 5.89\n",
            "2025-12-03T18:11:13.866693+0000 | compress | METRIC - GPU 0 | usage: 11.27% | total memory: 24 GB\n",
            "2025-12-03T18:11:13.866873+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:11:13.867149+0000 | compress_modules | INFO - Quantizing model.language_model.layers.9.self_attn.k_proj using 32 samples\n",
            "2025-12-03T18:11:15.936105+0000 | compress | METRIC - time 2.07s\n",
            "2025-12-03T18:11:15.937159+0000 | compress | METRIC - error 5.57\n",
            "2025-12-03T18:11:15.937407+0000 | compress | METRIC - GPU 0 | usage: 11.27% | total memory: 24 GB\n",
            "2025-12-03T18:11:15.937565+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:11:15.937824+0000 | compress_modules | INFO - Quantizing model.language_model.layers.9.self_attn.v_proj using 32 samples\n",
            "2025-12-03T18:11:18.007463+0000 | compress | METRIC - time 2.07s\n",
            "2025-12-03T18:11:18.008542+0000 | compress | METRIC - error 1.62\n",
            "2025-12-03T18:11:18.008776+0000 | compress | METRIC - GPU 0 | usage: 11.27% | total memory: 24 GB\n",
            "2025-12-03T18:11:18.008953+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:11:18.009217+0000 | compress_modules | INFO - Quantizing model.language_model.layers.9.self_attn.o_proj using 32 samples\n",
            "2025-12-03T18:11:20.099672+0000 | compress | METRIC - time 2.09s\n",
            "2025-12-03T18:11:20.100766+0000 | compress | METRIC - error 0.18\n",
            "2025-12-03T18:11:20.101034+0000 | compress | METRIC - GPU 0 | usage: 11.27% | total memory: 24 GB\n",
            "2025-12-03T18:11:20.101200+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:11:20.101466+0000 | compress_modules | INFO - Quantizing model.language_model.layers.9.mlp.gate_proj using 32 samples\n",
            "2025-12-03T18:11:22.353158+0000 | compress | METRIC - time 2.25s\n",
            "2025-12-03T18:11:22.354210+0000 | compress | METRIC - error 3.44\n",
            "2025-12-03T18:11:22.354449+0000 | compress | METRIC - GPU 0 | usage: 11.98% | total memory: 24 GB\n",
            "2025-12-03T18:11:22.354608+0000 | compress | METRIC - Compressed module size: 90.21056 MB\n",
            "2025-12-03T18:11:22.354873+0000 | compress_modules | INFO - Quantizing model.language_model.layers.9.mlp.up_proj using 32 samples\n",
            "2025-12-03T18:11:24.594259+0000 | compress | METRIC - time 2.24s\n",
            "2025-12-03T18:11:24.595306+0000 | compress | METRIC - error 2.91\n",
            "2025-12-03T18:11:24.595542+0000 | compress | METRIC - GPU 0 | usage: 11.98% | total memory: 24 GB\n",
            "2025-12-03T18:11:24.595701+0000 | compress | METRIC - Compressed module size: 90.21056 MB\n",
            "2025-12-03T18:11:24.595965+0000 | compress_modules | INFO - Quantizing model.language_model.layers.9.mlp.down_proj using 32 samples\n",
            "2025-12-03T18:11:30.657370+0000 | compress | METRIC - time 6.06s\n",
            "2025-12-03T18:11:30.659632+0000 | compress | METRIC - error 0.22\n",
            "2025-12-03T18:11:30.659958+0000 | compress | METRIC - GPU 0 | usage: 16.01% | total memory: 24 GB\n",
            "2025-12-03T18:11:30.660130+0000 | compress | METRIC - Compressed module size: 90.189824 MB\n",
            "(10/33): Propagating: 100% 32/32 [00:00<00:00, 84.07it/s]\n",
            "(11/33): Calibrating: 100% 32/32 [00:02<00:00, 15.49it/s]\n",
            "2025-12-03T18:11:33.393583+0000 | compress_modules | INFO - Quantizing model.language_model.layers.10.self_attn.q_proj using 32 samples\n",
            "2025-12-03T18:11:35.553938+0000 | compress | METRIC - time 2.16s\n",
            "2025-12-03T18:11:35.554988+0000 | compress | METRIC - error 5.95\n",
            "2025-12-03T18:11:35.555234+0000 | compress | METRIC - GPU 0 | usage: 11.27% | total memory: 24 GB\n",
            "2025-12-03T18:11:35.555400+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:11:35.555649+0000 | compress_modules | INFO - Quantizing model.language_model.layers.10.self_attn.k_proj using 32 samples\n",
            "2025-12-03T18:11:37.642940+0000 | compress | METRIC - time 2.09s\n",
            "2025-12-03T18:11:37.643977+0000 | compress | METRIC - error 6.15\n",
            "2025-12-03T18:11:37.644215+0000 | compress | METRIC - GPU 0 | usage: 11.27% | total memory: 24 GB\n",
            "2025-12-03T18:11:37.644380+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:11:37.644626+0000 | compress_modules | INFO - Quantizing model.language_model.layers.10.self_attn.v_proj using 32 samples\n",
            "2025-12-03T18:11:39.817962+0000 | compress | METRIC - time 2.17s\n",
            "2025-12-03T18:11:39.819054+0000 | compress | METRIC - error 1.87\n",
            "2025-12-03T18:11:39.819362+0000 | compress | METRIC - GPU 0 | usage: 11.27% | total memory: 24 GB\n",
            "2025-12-03T18:11:39.819544+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:11:39.819857+0000 | compress_modules | INFO - Quantizing model.language_model.layers.10.self_attn.o_proj using 32 samples\n",
            "2025-12-03T18:11:41.925873+0000 | compress | METRIC - time 2.11s\n",
            "2025-12-03T18:11:41.926945+0000 | compress | METRIC - error 0.23\n",
            "2025-12-03T18:11:41.927200+0000 | compress | METRIC - GPU 0 | usage: 11.27% | total memory: 24 GB\n",
            "2025-12-03T18:11:41.927368+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:11:41.927615+0000 | compress_modules | INFO - Quantizing model.language_model.layers.10.mlp.gate_proj using 32 samples\n",
            "2025-12-03T18:11:44.264528+0000 | compress | METRIC - time 2.34s\n",
            "2025-12-03T18:11:44.265612+0000 | compress | METRIC - error 3.73\n",
            "2025-12-03T18:11:44.265889+0000 | compress | METRIC - GPU 0 | usage: 11.98% | total memory: 24 GB\n",
            "2025-12-03T18:11:44.266067+0000 | compress | METRIC - Compressed module size: 90.21056 MB\n",
            "2025-12-03T18:11:44.266327+0000 | compress_modules | INFO - Quantizing model.language_model.layers.10.mlp.up_proj using 32 samples\n",
            "2025-12-03T18:11:46.559767+0000 | compress | METRIC - time 2.29s\n",
            "2025-12-03T18:11:46.560871+0000 | compress | METRIC - error 3.19\n",
            "2025-12-03T18:11:46.561116+0000 | compress | METRIC - GPU 0 | usage: 11.98% | total memory: 24 GB\n",
            "2025-12-03T18:11:46.561290+0000 | compress | METRIC - Compressed module size: 90.21056 MB\n",
            "2025-12-03T18:11:46.561561+0000 | compress_modules | INFO - Quantizing model.language_model.layers.10.mlp.down_proj using 32 samples\n",
            "2025-12-03T18:11:52.706352+0000 | compress | METRIC - time 6.14s\n",
            "2025-12-03T18:11:52.708571+0000 | compress | METRIC - error 0.28\n",
            "2025-12-03T18:11:52.708823+0000 | compress | METRIC - GPU 0 | usage: 16.01% | total memory: 24 GB\n",
            "2025-12-03T18:11:52.709002+0000 | compress | METRIC - Compressed module size: 90.189824 MB\n",
            "(11/33): Propagating: 100% 32/32 [00:00<00:00, 83.50it/s]\n",
            "(12/33): Calibrating: 100% 32/32 [00:02<00:00, 15.44it/s]\n",
            "2025-12-03T18:11:55.480732+0000 | compress_modules | INFO - Quantizing model.language_model.layers.11.self_attn.q_proj using 32 samples\n",
            "2025-12-03T18:11:57.640765+0000 | compress | METRIC - time 2.16s\n",
            "2025-12-03T18:11:57.641921+0000 | compress | METRIC - error 7.17\n",
            "2025-12-03T18:11:57.642231+0000 | compress | METRIC - GPU 0 | usage: 11.27% | total memory: 24 GB\n",
            "2025-12-03T18:11:57.642417+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:11:57.642699+0000 | compress_modules | INFO - Quantizing model.language_model.layers.11.self_attn.k_proj using 32 samples\n",
            "2025-12-03T18:11:59.751552+0000 | compress | METRIC - time 2.11s\n",
            "2025-12-03T18:11:59.752623+0000 | compress | METRIC - error 6.60\n",
            "2025-12-03T18:11:59.752874+0000 | compress | METRIC - GPU 0 | usage: 11.27% | total memory: 24 GB\n",
            "2025-12-03T18:11:59.753044+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:11:59.753303+0000 | compress_modules | INFO - Quantizing model.language_model.layers.11.self_attn.v_proj using 32 samples\n",
            "2025-12-03T18:12:01.841334+0000 | compress | METRIC - time 2.09s\n",
            "2025-12-03T18:12:01.842444+0000 | compress | METRIC - error 2.65\n",
            "2025-12-03T18:12:01.842682+0000 | compress | METRIC - GPU 0 | usage: 11.27% | total memory: 24 GB\n",
            "2025-12-03T18:12:01.842859+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:12:01.843110+0000 | compress_modules | INFO - Quantizing model.language_model.layers.11.self_attn.o_proj using 32 samples\n",
            "2025-12-03T18:12:03.927805+0000 | compress | METRIC - time 2.08s\n",
            "2025-12-03T18:12:03.928866+0000 | compress | METRIC - error 0.30\n",
            "2025-12-03T18:12:03.929096+0000 | compress | METRIC - GPU 0 | usage: 11.27% | total memory: 24 GB\n",
            "2025-12-03T18:12:03.929252+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:12:03.929505+0000 | compress_modules | INFO - Quantizing model.language_model.layers.11.mlp.gate_proj using 32 samples\n",
            "2025-12-03T18:12:06.230644+0000 | compress | METRIC - time 2.30s\n",
            "2025-12-03T18:12:06.231708+0000 | compress | METRIC - error 4.22\n",
            "2025-12-03T18:12:06.231960+0000 | compress | METRIC - GPU 0 | usage: 11.98% | total memory: 24 GB\n",
            "2025-12-03T18:12:06.232124+0000 | compress | METRIC - Compressed module size: 90.21056 MB\n",
            "2025-12-03T18:12:06.232375+0000 | compress_modules | INFO - Quantizing model.language_model.layers.11.mlp.up_proj using 32 samples\n",
            "2025-12-03T18:12:08.565708+0000 | compress | METRIC - time 2.33s\n",
            "2025-12-03T18:12:08.566820+0000 | compress | METRIC - error 3.74\n",
            "2025-12-03T18:12:08.567083+0000 | compress | METRIC - GPU 0 | usage: 11.98% | total memory: 24 GB\n",
            "2025-12-03T18:12:08.567259+0000 | compress | METRIC - Compressed module size: 90.21056 MB\n",
            "2025-12-03T18:12:08.567510+0000 | compress_modules | INFO - Quantizing model.language_model.layers.11.mlp.down_proj using 32 samples\n",
            "2025-12-03T18:12:14.678929+0000 | compress | METRIC - time 6.11s\n",
            "2025-12-03T18:12:14.681157+0000 | compress | METRIC - error 0.29\n",
            "2025-12-03T18:12:14.681402+0000 | compress | METRIC - GPU 0 | usage: 16.01% | total memory: 24 GB\n",
            "2025-12-03T18:12:14.681563+0000 | compress | METRIC - Compressed module size: 90.189824 MB\n",
            "(12/33): Propagating: 100% 32/32 [00:00<00:00, 83.95it/s]\n",
            "(13/33): Calibrating: 100% 32/32 [00:02<00:00, 15.42it/s]\n",
            "2025-12-03T18:12:17.436945+0000 | compress_modules | INFO - Quantizing model.language_model.layers.12.self_attn.q_proj using 32 samples\n",
            "2025-12-03T18:12:19.600629+0000 | compress | METRIC - time 2.16s\n",
            "2025-12-03T18:12:19.601691+0000 | compress | METRIC - error 7.85\n",
            "2025-12-03T18:12:19.601961+0000 | compress | METRIC - GPU 0 | usage: 11.27% | total memory: 24 GB\n",
            "2025-12-03T18:12:19.602139+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:12:19.602391+0000 | compress_modules | INFO - Quantizing model.language_model.layers.12.self_attn.k_proj using 32 samples\n",
            "2025-12-03T18:12:21.676562+0000 | compress | METRIC - time 2.07s\n",
            "2025-12-03T18:12:21.677686+0000 | compress | METRIC - error 7.87\n",
            "2025-12-03T18:12:21.677944+0000 | compress | METRIC - GPU 0 | usage: 11.27% | total memory: 24 GB\n",
            "2025-12-03T18:12:21.678130+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:12:21.678394+0000 | compress_modules | INFO - Quantizing model.language_model.layers.12.self_attn.v_proj using 32 samples\n",
            "2025-12-03T18:12:23.756798+0000 | compress | METRIC - time 2.08s\n",
            "2025-12-03T18:12:23.757883+0000 | compress | METRIC - error 2.78\n",
            "2025-12-03T18:12:23.758118+0000 | compress | METRIC - GPU 0 | usage: 11.27% | total memory: 24 GB\n",
            "2025-12-03T18:12:23.758288+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:12:23.758558+0000 | compress_modules | INFO - Quantizing model.language_model.layers.12.self_attn.o_proj using 32 samples\n",
            "2025-12-03T18:12:25.859102+0000 | compress | METRIC - time 2.10s\n",
            "2025-12-03T18:12:25.860213+0000 | compress | METRIC - error 0.22\n",
            "2025-12-03T18:12:25.860456+0000 | compress | METRIC - GPU 0 | usage: 11.27% | total memory: 24 GB\n",
            "2025-12-03T18:12:25.860627+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:12:25.860897+0000 | compress_modules | INFO - Quantizing model.language_model.layers.12.mlp.gate_proj using 32 samples\n",
            "2025-12-03T18:12:28.139118+0000 | compress | METRIC - time 2.28s\n",
            "2025-12-03T18:12:28.140188+0000 | compress | METRIC - error 4.55\n",
            "2025-12-03T18:12:28.140436+0000 | compress | METRIC - GPU 0 | usage: 11.98% | total memory: 24 GB\n",
            "2025-12-03T18:12:28.140613+0000 | compress | METRIC - Compressed module size: 90.21056 MB\n",
            "2025-12-03T18:12:28.140880+0000 | compress_modules | INFO - Quantizing model.language_model.layers.12.mlp.up_proj using 32 samples\n",
            "2025-12-03T18:12:30.460590+0000 | compress | METRIC - time 2.32s\n",
            "2025-12-03T18:12:30.461660+0000 | compress | METRIC - error 4.10\n",
            "2025-12-03T18:12:30.461916+0000 | compress | METRIC - GPU 0 | usage: 11.98% | total memory: 24 GB\n",
            "2025-12-03T18:12:30.462098+0000 | compress | METRIC - Compressed module size: 90.21056 MB\n",
            "2025-12-03T18:12:30.462355+0000 | compress_modules | INFO - Quantizing model.language_model.layers.12.mlp.down_proj using 32 samples\n",
            "2025-12-03T18:12:36.652959+0000 | compress | METRIC - time 6.19s\n",
            "2025-12-03T18:12:36.655227+0000 | compress | METRIC - error 0.32\n",
            "2025-12-03T18:12:36.655472+0000 | compress | METRIC - GPU 0 | usage: 16.01% | total memory: 24 GB\n",
            "2025-12-03T18:12:36.655648+0000 | compress | METRIC - Compressed module size: 90.189824 MB\n",
            "(13/33): Propagating: 100% 32/32 [00:00<00:00, 84.37it/s]\n",
            "(14/33): Calibrating: 100% 32/32 [00:02<00:00, 15.40it/s]\n",
            "2025-12-03T18:12:39.426252+0000 | compress_modules | INFO - Quantizing model.language_model.layers.13.self_attn.q_proj using 32 samples\n",
            "2025-12-03T18:12:41.631090+0000 | compress | METRIC - time 2.20s\n",
            "2025-12-03T18:12:41.632179+0000 | compress | METRIC - error 8.89\n",
            "2025-12-03T18:12:41.632425+0000 | compress | METRIC - GPU 0 | usage: 11.27% | total memory: 24 GB\n",
            "2025-12-03T18:12:41.632589+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:12:41.632856+0000 | compress_modules | INFO - Quantizing model.language_model.layers.13.self_attn.k_proj using 32 samples\n",
            "2025-12-03T18:12:43.755967+0000 | compress | METRIC - time 2.12s\n",
            "2025-12-03T18:12:43.757083+0000 | compress | METRIC - error 7.99\n",
            "2025-12-03T18:12:43.757334+0000 | compress | METRIC - GPU 0 | usage: 11.27% | total memory: 24 GB\n",
            "2025-12-03T18:12:43.757507+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:12:43.757757+0000 | compress_modules | INFO - Quantizing model.language_model.layers.13.self_attn.v_proj using 32 samples\n",
            "2025-12-03T18:12:45.847607+0000 | compress | METRIC - time 2.09s\n",
            "2025-12-03T18:12:45.848678+0000 | compress | METRIC - error 2.97\n",
            "2025-12-03T18:12:45.848932+0000 | compress | METRIC - GPU 0 | usage: 11.27% | total memory: 24 GB\n",
            "2025-12-03T18:12:45.849110+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:12:45.849373+0000 | compress_modules | INFO - Quantizing model.language_model.layers.13.self_attn.o_proj using 32 samples\n",
            "2025-12-03T18:12:47.937443+0000 | compress | METRIC - time 2.09s\n",
            "2025-12-03T18:12:47.938536+0000 | compress | METRIC - error 0.18\n",
            "2025-12-03T18:12:47.938771+0000 | compress | METRIC - GPU 0 | usage: 11.27% | total memory: 24 GB\n",
            "2025-12-03T18:12:47.938953+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:12:47.939208+0000 | compress_modules | INFO - Quantizing model.language_model.layers.13.mlp.gate_proj using 32 samples\n",
            "2025-12-03T18:12:50.247289+0000 | compress | METRIC - time 2.31s\n",
            "2025-12-03T18:12:50.248347+0000 | compress | METRIC - error 4.83\n",
            "2025-12-03T18:12:50.248586+0000 | compress | METRIC - GPU 0 | usage: 11.98% | total memory: 24 GB\n",
            "2025-12-03T18:12:50.248753+0000 | compress | METRIC - Compressed module size: 90.21056 MB\n",
            "2025-12-03T18:12:50.249015+0000 | compress_modules | INFO - Quantizing model.language_model.layers.13.mlp.up_proj using 32 samples\n",
            "2025-12-03T18:12:52.541448+0000 | compress | METRIC - time 2.29s\n",
            "2025-12-03T18:12:52.542543+0000 | compress | METRIC - error 4.49\n",
            "2025-12-03T18:12:52.542798+0000 | compress | METRIC - GPU 0 | usage: 11.98% | total memory: 24 GB\n",
            "2025-12-03T18:12:52.542975+0000 | compress | METRIC - Compressed module size: 90.21056 MB\n",
            "2025-12-03T18:12:52.543234+0000 | compress_modules | INFO - Quantizing model.language_model.layers.13.mlp.down_proj using 32 samples\n",
            "2025-12-03T18:12:58.668481+0000 | compress | METRIC - time 6.13s\n",
            "2025-12-03T18:12:58.671005+0000 | compress | METRIC - error 0.39\n",
            "2025-12-03T18:12:58.671299+0000 | compress | METRIC - GPU 0 | usage: 16.01% | total memory: 24 GB\n",
            "2025-12-03T18:12:58.671512+0000 | compress | METRIC - Compressed module size: 90.189824 MB\n",
            "(14/33): Propagating: 100% 32/32 [00:00<00:00, 84.17it/s]\n",
            "(15/33): Calibrating: 100% 32/32 [00:02<00:00, 15.40it/s]\n",
            "2025-12-03T18:13:01.439256+0000 | compress_modules | INFO - Quantizing model.language_model.layers.14.self_attn.q_proj using 32 samples\n",
            "2025-12-03T18:13:03.573130+0000 | compress | METRIC - time 2.13s\n",
            "2025-12-03T18:13:03.574197+0000 | compress | METRIC - error 8.29\n",
            "2025-12-03T18:13:03.574430+0000 | compress | METRIC - GPU 0 | usage: 11.27% | total memory: 24 GB\n",
            "2025-12-03T18:13:03.574585+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:13:03.574851+0000 | compress_modules | INFO - Quantizing model.language_model.layers.14.self_attn.k_proj using 32 samples\n",
            "2025-12-03T18:13:05.651648+0000 | compress | METRIC - time 2.08s\n",
            "2025-12-03T18:13:05.652722+0000 | compress | METRIC - error 8.54\n",
            "2025-12-03T18:13:05.652975+0000 | compress | METRIC - GPU 0 | usage: 11.27% | total memory: 24 GB\n",
            "2025-12-03T18:13:05.653139+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:13:05.653402+0000 | compress_modules | INFO - Quantizing model.language_model.layers.14.self_attn.v_proj using 32 samples\n",
            "2025-12-03T18:13:07.735764+0000 | compress | METRIC - time 2.08s\n",
            "2025-12-03T18:13:07.736844+0000 | compress | METRIC - error 2.99\n",
            "2025-12-03T18:13:07.737098+0000 | compress | METRIC - GPU 0 | usage: 11.27% | total memory: 24 GB\n",
            "2025-12-03T18:13:07.737275+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:13:07.737531+0000 | compress_modules | INFO - Quantizing model.language_model.layers.14.self_attn.o_proj using 32 samples\n",
            "2025-12-03T18:13:09.830924+0000 | compress | METRIC - time 2.09s\n",
            "2025-12-03T18:13:09.832002+0000 | compress | METRIC - error 0.26\n",
            "2025-12-03T18:13:09.832242+0000 | compress | METRIC - GPU 0 | usage: 11.27% | total memory: 24 GB\n",
            "2025-12-03T18:13:09.832402+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:13:09.832644+0000 | compress_modules | INFO - Quantizing model.language_model.layers.14.mlp.gate_proj using 32 samples\n",
            "2025-12-03T18:13:12.112513+0000 | compress | METRIC - time 2.28s\n",
            "2025-12-03T18:13:12.113597+0000 | compress | METRIC - error 5.51\n",
            "2025-12-03T18:13:12.113847+0000 | compress | METRIC - GPU 0 | usage: 11.98% | total memory: 24 GB\n",
            "2025-12-03T18:13:12.114018+0000 | compress | METRIC - Compressed module size: 90.21056 MB\n",
            "2025-12-03T18:13:12.114262+0000 | compress_modules | INFO - Quantizing model.language_model.layers.14.mlp.up_proj using 32 samples\n",
            "2025-12-03T18:13:14.393353+0000 | compress | METRIC - time 2.28s\n",
            "2025-12-03T18:13:14.394425+0000 | compress | METRIC - error 5.13\n",
            "2025-12-03T18:13:14.394657+0000 | compress | METRIC - GPU 0 | usage: 11.98% | total memory: 24 GB\n",
            "2025-12-03T18:13:14.394838+0000 | compress | METRIC - Compressed module size: 90.21056 MB\n",
            "2025-12-03T18:13:14.395103+0000 | compress_modules | INFO - Quantizing model.language_model.layers.14.mlp.down_proj using 32 samples\n",
            "2025-12-03T18:13:20.578656+0000 | compress | METRIC - time 6.18s\n",
            "2025-12-03T18:13:20.580888+0000 | compress | METRIC - error 0.41\n",
            "2025-12-03T18:13:20.581182+0000 | compress | METRIC - GPU 0 | usage: 16.01% | total memory: 24 GB\n",
            "2025-12-03T18:13:20.581356+0000 | compress | METRIC - Compressed module size: 90.189824 MB\n",
            "(15/33): Propagating: 100% 32/32 [00:00<00:00, 83.81it/s]\n",
            "(16/33): Calibrating: 100% 32/32 [00:02<00:00, 15.41it/s]\n",
            "2025-12-03T18:13:23.345449+0000 | compress_modules | INFO - Quantizing model.language_model.layers.15.self_attn.q_proj using 32 samples\n",
            "2025-12-03T18:13:25.470226+0000 | compress | METRIC - time 2.12s\n",
            "2025-12-03T18:13:25.471292+0000 | compress | METRIC - error 9.72\n",
            "2025-12-03T18:13:25.471527+0000 | compress | METRIC - GPU 0 | usage: 11.27% | total memory: 24 GB\n",
            "2025-12-03T18:13:25.471685+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:13:25.471949+0000 | compress_modules | INFO - Quantizing model.language_model.layers.15.self_attn.k_proj using 32 samples\n",
            "2025-12-03T18:13:27.559778+0000 | compress | METRIC - time 2.09s\n",
            "2025-12-03T18:13:27.560897+0000 | compress | METRIC - error 8.89\n",
            "2025-12-03T18:13:27.561124+0000 | compress | METRIC - GPU 0 | usage: 11.27% | total memory: 24 GB\n",
            "2025-12-03T18:13:27.561297+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:13:27.561545+0000 | compress_modules | INFO - Quantizing model.language_model.layers.15.self_attn.v_proj using 32 samples\n",
            "2025-12-03T18:13:29.616430+0000 | compress | METRIC - time 2.05s\n",
            "2025-12-03T18:13:29.617538+0000 | compress | METRIC - error 3.36\n",
            "2025-12-03T18:13:29.617773+0000 | compress | METRIC - GPU 0 | usage: 11.27% | total memory: 24 GB\n",
            "2025-12-03T18:13:29.617956+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:13:29.618210+0000 | compress_modules | INFO - Quantizing model.language_model.layers.15.self_attn.o_proj using 32 samples\n",
            "2025-12-03T18:13:31.708052+0000 | compress | METRIC - time 2.09s\n",
            "2025-12-03T18:13:31.709134+0000 | compress | METRIC - error 0.21\n",
            "2025-12-03T18:13:31.709375+0000 | compress | METRIC - GPU 0 | usage: 11.27% | total memory: 24 GB\n",
            "2025-12-03T18:13:31.709546+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:13:31.709808+0000 | compress_modules | INFO - Quantizing model.language_model.layers.15.mlp.gate_proj using 32 samples\n",
            "2025-12-03T18:13:34.061308+0000 | compress | METRIC - time 2.35s\n",
            "2025-12-03T18:13:34.062377+0000 | compress | METRIC - error 6.26\n",
            "2025-12-03T18:13:34.062615+0000 | compress | METRIC - GPU 0 | usage: 11.98% | total memory: 24 GB\n",
            "2025-12-03T18:13:34.062775+0000 | compress | METRIC - Compressed module size: 90.21056 MB\n",
            "2025-12-03T18:13:34.063067+0000 | compress_modules | INFO - Quantizing model.language_model.layers.15.mlp.up_proj using 32 samples\n",
            "2025-12-03T18:13:36.334411+0000 | compress | METRIC - time 2.27s\n",
            "2025-12-03T18:13:36.335561+0000 | compress | METRIC - error 5.90\n",
            "2025-12-03T18:13:36.335845+0000 | compress | METRIC - GPU 0 | usage: 11.98% | total memory: 24 GB\n",
            "2025-12-03T18:13:36.336031+0000 | compress | METRIC - Compressed module size: 90.21056 MB\n",
            "2025-12-03T18:13:36.336320+0000 | compress_modules | INFO - Quantizing model.language_model.layers.15.mlp.down_proj using 32 samples\n",
            "2025-12-03T18:13:42.477296+0000 | compress | METRIC - time 6.14s\n",
            "2025-12-03T18:13:42.479772+0000 | compress | METRIC - error 0.59\n",
            "2025-12-03T18:13:42.480078+0000 | compress | METRIC - GPU 0 | usage: 16.01% | total memory: 24 GB\n",
            "2025-12-03T18:13:42.480264+0000 | compress | METRIC - Compressed module size: 90.189824 MB\n",
            "(16/33): Propagating: 100% 32/32 [00:00<00:00, 84.50it/s]\n",
            "(17/33): Calibrating: 100% 32/32 [00:02<00:00, 15.39it/s]\n",
            "2025-12-03T18:13:45.248101+0000 | compress_modules | INFO - Quantizing model.language_model.layers.16.self_attn.q_proj using 32 samples\n",
            "2025-12-03T18:13:47.360762+0000 | compress | METRIC - time 2.11s\n",
            "2025-12-03T18:13:47.361868+0000 | compress | METRIC - error 11.27\n",
            "2025-12-03T18:13:47.362106+0000 | compress | METRIC - GPU 0 | usage: 11.27% | total memory: 24 GB\n",
            "2025-12-03T18:13:47.362277+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:13:47.362526+0000 | compress_modules | INFO - Quantizing model.language_model.layers.16.self_attn.k_proj using 32 samples\n",
            "2025-12-03T18:13:49.420276+0000 | compress | METRIC - time 2.06s\n",
            "2025-12-03T18:13:49.421371+0000 | compress | METRIC - error 9.91\n",
            "2025-12-03T18:13:49.421607+0000 | compress | METRIC - GPU 0 | usage: 11.27% | total memory: 24 GB\n",
            "2025-12-03T18:13:49.421762+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:13:49.422019+0000 | compress_modules | INFO - Quantizing model.language_model.layers.16.self_attn.v_proj using 32 samples\n",
            "2025-12-03T18:13:51.492395+0000 | compress | METRIC - time 2.07s\n",
            "2025-12-03T18:13:51.493523+0000 | compress | METRIC - error 3.91\n",
            "2025-12-03T18:13:51.493762+0000 | compress | METRIC - GPU 0 | usage: 11.27% | total memory: 24 GB\n",
            "2025-12-03T18:13:51.493949+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:13:51.494206+0000 | compress_modules | INFO - Quantizing model.language_model.layers.16.self_attn.o_proj using 32 samples\n",
            "2025-12-03T18:13:53.556121+0000 | compress | METRIC - time 2.06s\n",
            "2025-12-03T18:13:53.557122+0000 | compress | METRIC - error 0.12\n",
            "2025-12-03T18:13:53.557360+0000 | compress | METRIC - GPU 0 | usage: 11.27% | total memory: 24 GB\n",
            "2025-12-03T18:13:53.557504+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:13:53.557732+0000 | compress_modules | INFO - Quantizing model.language_model.layers.16.mlp.gate_proj using 32 samples\n",
            "2025-12-03T18:13:55.828513+0000 | compress | METRIC - time 2.27s\n",
            "2025-12-03T18:13:55.829607+0000 | compress | METRIC - error 7.26\n",
            "2025-12-03T18:13:55.829855+0000 | compress | METRIC - GPU 0 | usage: 11.98% | total memory: 24 GB\n",
            "2025-12-03T18:13:55.830023+0000 | compress | METRIC - Compressed module size: 90.21056 MB\n",
            "2025-12-03T18:13:55.830267+0000 | compress_modules | INFO - Quantizing model.language_model.layers.16.mlp.up_proj using 32 samples\n",
            "2025-12-03T18:13:58.131748+0000 | compress | METRIC - time 2.30s\n",
            "2025-12-03T18:13:58.132911+0000 | compress | METRIC - error 6.69\n",
            "2025-12-03T18:13:58.133177+0000 | compress | METRIC - GPU 0 | usage: 11.98% | total memory: 24 GB\n",
            "2025-12-03T18:13:58.133353+0000 | compress | METRIC - Compressed module size: 90.21056 MB\n",
            "2025-12-03T18:13:58.133644+0000 | compress_modules | INFO - Quantizing model.language_model.layers.16.mlp.down_proj using 32 samples\n",
            "2025-12-03T18:14:04.305562+0000 | compress | METRIC - time 6.17s\n",
            "2025-12-03T18:14:04.307922+0000 | compress | METRIC - error 0.81\n",
            "2025-12-03T18:14:04.308171+0000 | compress | METRIC - GPU 0 | usage: 16.01% | total memory: 24 GB\n",
            "2025-12-03T18:14:04.308346+0000 | compress | METRIC - Compressed module size: 90.189824 MB\n",
            "(17/33): Propagating: 100% 32/32 [00:00<00:00, 82.99it/s]\n",
            "(18/33): Calibrating: 100% 32/32 [00:02<00:00, 15.38it/s]\n",
            "2025-12-03T18:14:07.083611+0000 | compress_modules | INFO - Quantizing model.language_model.layers.17.self_attn.q_proj using 32 samples\n",
            "2025-12-03T18:14:09.310350+0000 | compress | METRIC - time 2.23s\n",
            "2025-12-03T18:14:09.311468+0000 | compress | METRIC - error 11.51\n",
            "2025-12-03T18:14:09.311709+0000 | compress | METRIC - GPU 0 | usage: 11.27% | total memory: 24 GB\n",
            "2025-12-03T18:14:09.311906+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:14:09.312178+0000 | compress_modules | INFO - Quantizing model.language_model.layers.17.self_attn.k_proj using 32 samples\n",
            "2025-12-03T18:14:11.429540+0000 | compress | METRIC - time 2.12s\n",
            "2025-12-03T18:14:11.430649+0000 | compress | METRIC - error 10.07\n",
            "2025-12-03T18:14:11.430898+0000 | compress | METRIC - GPU 0 | usage: 11.27% | total memory: 24 GB\n",
            "2025-12-03T18:14:11.431067+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:14:11.431348+0000 | compress_modules | INFO - Quantizing model.language_model.layers.17.self_attn.v_proj using 32 samples\n",
            "2025-12-03T18:14:13.511120+0000 | compress | METRIC - time 2.08s\n",
            "2025-12-03T18:14:13.512247+0000 | compress | METRIC - error 4.17\n",
            "2025-12-03T18:14:13.512483+0000 | compress | METRIC - GPU 0 | usage: 11.27% | total memory: 24 GB\n",
            "2025-12-03T18:14:13.512640+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:14:13.512904+0000 | compress_modules | INFO - Quantizing model.language_model.layers.17.self_attn.o_proj using 32 samples\n",
            "2025-12-03T18:14:15.593736+0000 | compress | METRIC - time 2.08s\n",
            "2025-12-03T18:14:15.594867+0000 | compress | METRIC - error 0.18\n",
            "2025-12-03T18:14:15.595102+0000 | compress | METRIC - GPU 0 | usage: 11.27% | total memory: 24 GB\n",
            "2025-12-03T18:14:15.595264+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:14:15.595507+0000 | compress_modules | INFO - Quantizing model.language_model.layers.17.mlp.gate_proj using 32 samples\n",
            "2025-12-03T18:14:17.894902+0000 | compress | METRIC - time 2.30s\n",
            "2025-12-03T18:14:17.896008+0000 | compress | METRIC - error 8.44\n",
            "2025-12-03T18:14:17.896252+0000 | compress | METRIC - GPU 0 | usage: 11.98% | total memory: 24 GB\n",
            "2025-12-03T18:14:17.896413+0000 | compress | METRIC - Compressed module size: 90.21056 MB\n",
            "2025-12-03T18:14:17.896657+0000 | compress_modules | INFO - Quantizing model.language_model.layers.17.mlp.up_proj using 32 samples\n",
            "2025-12-03T18:14:20.228538+0000 | compress | METRIC - time 2.33s\n",
            "2025-12-03T18:14:20.229646+0000 | compress | METRIC - error 7.57\n",
            "2025-12-03T18:14:20.229898+0000 | compress | METRIC - GPU 0 | usage: 11.98% | total memory: 24 GB\n",
            "2025-12-03T18:14:20.230066+0000 | compress | METRIC - Compressed module size: 90.21056 MB\n",
            "2025-12-03T18:14:20.230314+0000 | compress_modules | INFO - Quantizing model.language_model.layers.17.mlp.down_proj using 32 samples\n",
            "2025-12-03T18:14:26.335896+0000 | compress | METRIC - time 6.11s\n",
            "2025-12-03T18:14:26.338229+0000 | compress | METRIC - error 0.81\n",
            "2025-12-03T18:14:26.338467+0000 | compress | METRIC - GPU 0 | usage: 16.01% | total memory: 24 GB\n",
            "2025-12-03T18:14:26.338625+0000 | compress | METRIC - Compressed module size: 90.189824 MB\n",
            "(18/33): Propagating: 100% 32/32 [00:00<00:00, 84.69it/s]\n",
            "(19/33): Calibrating: 100% 32/32 [00:02<00:00, 15.38it/s]\n",
            "2025-12-03T18:14:29.101478+0000 | compress_modules | INFO - Quantizing model.language_model.layers.18.self_attn.q_proj using 32 samples\n",
            "2025-12-03T18:14:31.255834+0000 | compress | METRIC - time 2.15s\n",
            "2025-12-03T18:14:31.256948+0000 | compress | METRIC - error 13.84\n",
            "2025-12-03T18:14:31.257228+0000 | compress | METRIC - GPU 0 | usage: 11.27% | total memory: 24 GB\n",
            "2025-12-03T18:14:31.257398+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:14:31.257644+0000 | compress_modules | INFO - Quantizing model.language_model.layers.18.self_attn.k_proj using 32 samples\n",
            "2025-12-03T18:14:33.378571+0000 | compress | METRIC - time 2.12s\n",
            "2025-12-03T18:14:33.379690+0000 | compress | METRIC - error 11.22\n",
            "2025-12-03T18:14:33.379943+0000 | compress | METRIC - GPU 0 | usage: 11.27% | total memory: 24 GB\n",
            "2025-12-03T18:14:33.380108+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:14:33.380364+0000 | compress_modules | INFO - Quantizing model.language_model.layers.18.self_attn.v_proj using 32 samples\n",
            "2025-12-03T18:14:35.482515+0000 | compress | METRIC - time 2.10s\n",
            "2025-12-03T18:14:35.483657+0000 | compress | METRIC - error 5.22\n",
            "2025-12-03T18:14:35.483907+0000 | compress | METRIC - GPU 0 | usage: 11.27% | total memory: 24 GB\n",
            "2025-12-03T18:14:35.484077+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:14:35.484338+0000 | compress_modules | INFO - Quantizing model.language_model.layers.18.self_attn.o_proj using 32 samples\n",
            "2025-12-03T18:14:37.571262+0000 | compress | METRIC - time 2.09s\n",
            "2025-12-03T18:14:37.572356+0000 | compress | METRIC - error 0.11\n",
            "2025-12-03T18:14:37.572588+0000 | compress | METRIC - GPU 0 | usage: 11.27% | total memory: 24 GB\n",
            "2025-12-03T18:14:37.572742+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:14:37.573003+0000 | compress_modules | INFO - Quantizing model.language_model.layers.18.mlp.gate_proj using 32 samples\n",
            "2025-12-03T18:14:39.908535+0000 | compress | METRIC - time 2.34s\n",
            "2025-12-03T18:14:39.909681+0000 | compress | METRIC - error 9.85\n",
            "2025-12-03T18:14:39.909928+0000 | compress | METRIC - GPU 0 | usage: 11.98% | total memory: 24 GB\n",
            "2025-12-03T18:14:39.910095+0000 | compress | METRIC - Compressed module size: 90.21056 MB\n",
            "2025-12-03T18:14:39.910354+0000 | compress_modules | INFO - Quantizing model.language_model.layers.18.mlp.up_proj using 32 samples\n",
            "2025-12-03T18:14:42.204652+0000 | compress | METRIC - time 2.29s\n",
            "2025-12-03T18:14:42.205799+0000 | compress | METRIC - error 8.63\n",
            "2025-12-03T18:14:42.206043+0000 | compress | METRIC - GPU 0 | usage: 11.98% | total memory: 24 GB\n",
            "2025-12-03T18:14:42.206233+0000 | compress | METRIC - Compressed module size: 90.21056 MB\n",
            "2025-12-03T18:14:42.206487+0000 | compress_modules | INFO - Quantizing model.language_model.layers.18.mlp.down_proj using 32 samples\n",
            "2025-12-03T18:14:48.435814+0000 | compress | METRIC - time 6.23s\n",
            "2025-12-03T18:14:48.438194+0000 | compress | METRIC - error 1.05\n",
            "2025-12-03T18:14:48.438447+0000 | compress | METRIC - GPU 0 | usage: 16.01% | total memory: 24 GB\n",
            "2025-12-03T18:14:48.438626+0000 | compress | METRIC - Compressed module size: 90.189824 MB\n",
            "(19/33): Propagating: 100% 32/32 [00:00<00:00, 84.22it/s]\n",
            "(20/33): Calibrating: 100% 32/32 [00:02<00:00, 15.37it/s]\n",
            "2025-12-03T18:14:51.204587+0000 | compress_modules | INFO - Quantizing model.language_model.layers.19.self_attn.q_proj using 32 samples\n",
            "2025-12-03T18:14:53.349839+0000 | compress | METRIC - time 2.14s\n",
            "2025-12-03T18:14:53.350969+0000 | compress | METRIC - error 14.75\n",
            "2025-12-03T18:14:53.351215+0000 | compress | METRIC - GPU 0 | usage: 11.27% | total memory: 24 GB\n",
            "2025-12-03T18:14:53.351383+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:14:53.351631+0000 | compress_modules | INFO - Quantizing model.language_model.layers.19.self_attn.k_proj using 32 samples\n",
            "2025-12-03T18:14:55.464647+0000 | compress | METRIC - time 2.11s\n",
            "2025-12-03T18:14:55.465776+0000 | compress | METRIC - error 11.76\n",
            "2025-12-03T18:14:55.466026+0000 | compress | METRIC - GPU 0 | usage: 11.27% | total memory: 24 GB\n",
            "2025-12-03T18:14:55.466188+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:14:55.466442+0000 | compress_modules | INFO - Quantizing model.language_model.layers.19.self_attn.v_proj using 32 samples\n",
            "2025-12-03T18:14:57.556476+0000 | compress | METRIC - time 2.09s\n",
            "2025-12-03T18:14:57.557620+0000 | compress | METRIC - error 5.26\n",
            "2025-12-03T18:14:57.557875+0000 | compress | METRIC - GPU 0 | usage: 11.27% | total memory: 24 GB\n",
            "2025-12-03T18:14:57.558042+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:14:57.558305+0000 | compress_modules | INFO - Quantizing model.language_model.layers.19.self_attn.o_proj using 32 samples\n",
            "2025-12-03T18:14:59.659589+0000 | compress | METRIC - time 2.10s\n",
            "2025-12-03T18:14:59.660709+0000 | compress | METRIC - error 0.20\n",
            "2025-12-03T18:14:59.660956+0000 | compress | METRIC - GPU 0 | usage: 11.27% | total memory: 24 GB\n",
            "2025-12-03T18:14:59.661114+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:14:59.661361+0000 | compress_modules | INFO - Quantizing model.language_model.layers.19.mlp.gate_proj using 32 samples\n",
            "2025-12-03T18:15:01.952602+0000 | compress | METRIC - time 2.29s\n",
            "2025-12-03T18:15:01.953831+0000 | compress | METRIC - error 10.78\n",
            "2025-12-03T18:15:01.954110+0000 | compress | METRIC - GPU 0 | usage: 11.98% | total memory: 24 GB\n",
            "2025-12-03T18:15:01.954288+0000 | compress | METRIC - Compressed module size: 90.21056 MB\n",
            "2025-12-03T18:15:01.954550+0000 | compress_modules | INFO - Quantizing model.language_model.layers.19.mlp.up_proj using 32 samples\n",
            "2025-12-03T18:15:04.241902+0000 | compress | METRIC - time 2.29s\n",
            "2025-12-03T18:15:04.243044+0000 | compress | METRIC - error 9.48\n",
            "2025-12-03T18:15:04.243317+0000 | compress | METRIC - GPU 0 | usage: 11.98% | total memory: 24 GB\n",
            "2025-12-03T18:15:04.243487+0000 | compress | METRIC - Compressed module size: 90.21056 MB\n",
            "2025-12-03T18:15:04.243727+0000 | compress_modules | INFO - Quantizing model.language_model.layers.19.mlp.down_proj using 32 samples\n",
            "2025-12-03T18:15:10.453666+0000 | compress | METRIC - time 6.21s\n",
            "2025-12-03T18:15:10.456086+0000 | compress | METRIC - error 1.17\n",
            "2025-12-03T18:15:10.456368+0000 | compress | METRIC - GPU 0 | usage: 16.01% | total memory: 24 GB\n",
            "2025-12-03T18:15:10.456548+0000 | compress | METRIC - Compressed module size: 90.189824 MB\n",
            "(20/33): Propagating: 100% 32/32 [00:00<00:00, 83.74it/s]\n",
            "(21/33): Calibrating: 100% 32/32 [00:02<00:00, 15.34it/s]\n",
            "2025-12-03T18:15:13.231902+0000 | compress_modules | INFO - Quantizing model.language_model.layers.20.self_attn.q_proj using 32 samples\n",
            "2025-12-03T18:15:15.319542+0000 | compress | METRIC - time 2.09s\n",
            "2025-12-03T18:15:15.320663+0000 | compress | METRIC - error 15.18\n",
            "2025-12-03T18:15:15.320912+0000 | compress | METRIC - GPU 0 | usage: 11.27% | total memory: 24 GB\n",
            "2025-12-03T18:15:15.321079+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:15:15.321327+0000 | compress_modules | INFO - Quantizing model.language_model.layers.20.self_attn.k_proj using 32 samples\n",
            "2025-12-03T18:15:17.364531+0000 | compress | METRIC - time 2.04s\n",
            "2025-12-03T18:15:17.365663+0000 | compress | METRIC - error 12.44\n",
            "2025-12-03T18:15:17.365918+0000 | compress | METRIC - GPU 0 | usage: 11.27% | total memory: 24 GB\n",
            "2025-12-03T18:15:17.366089+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:15:17.366346+0000 | compress_modules | INFO - Quantizing model.language_model.layers.20.self_attn.v_proj using 32 samples\n",
            "2025-12-03T18:15:19.496451+0000 | compress | METRIC - time 2.13s\n",
            "2025-12-03T18:15:19.497620+0000 | compress | METRIC - error 5.52\n",
            "2025-12-03T18:15:19.497870+0000 | compress | METRIC - GPU 0 | usage: 11.27% | total memory: 24 GB\n",
            "2025-12-03T18:15:19.498061+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:15:19.498316+0000 | compress_modules | INFO - Quantizing model.language_model.layers.20.self_attn.o_proj using 32 samples\n",
            "2025-12-03T18:15:21.639909+0000 | compress | METRIC - time 2.14s\n",
            "2025-12-03T18:15:21.641042+0000 | compress | METRIC - error 0.15\n",
            "2025-12-03T18:15:21.641287+0000 | compress | METRIC - GPU 0 | usage: 11.27% | total memory: 24 GB\n",
            "2025-12-03T18:15:21.641460+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:15:21.641719+0000 | compress_modules | INFO - Quantizing model.language_model.layers.20.mlp.gate_proj using 32 samples\n",
            "2025-12-03T18:15:23.987523+0000 | compress | METRIC - time 2.35s\n",
            "2025-12-03T18:15:23.988675+0000 | compress | METRIC - error 11.58\n",
            "2025-12-03T18:15:23.988925+0000 | compress | METRIC - GPU 0 | usage: 11.98% | total memory: 24 GB\n",
            "2025-12-03T18:15:23.989092+0000 | compress | METRIC - Compressed module size: 90.21056 MB\n",
            "2025-12-03T18:15:23.989340+0000 | compress_modules | INFO - Quantizing model.language_model.layers.20.mlp.up_proj using 32 samples\n",
            "2025-12-03T18:15:26.269375+0000 | compress | METRIC - time 2.28s\n",
            "2025-12-03T18:15:26.270515+0000 | compress | METRIC - error 10.05\n",
            "2025-12-03T18:15:26.270747+0000 | compress | METRIC - GPU 0 | usage: 11.98% | total memory: 24 GB\n",
            "2025-12-03T18:15:26.270923+0000 | compress | METRIC - Compressed module size: 90.21056 MB\n",
            "2025-12-03T18:15:26.271171+0000 | compress_modules | INFO - Quantizing model.language_model.layers.20.mlp.down_proj using 32 samples\n",
            "2025-12-03T18:15:32.377100+0000 | compress | METRIC - time 6.11s\n",
            "2025-12-03T18:15:32.379412+0000 | compress | METRIC - error 1.34\n",
            "2025-12-03T18:15:32.379647+0000 | compress | METRIC - GPU 0 | usage: 16.01% | total memory: 24 GB\n",
            "2025-12-03T18:15:32.379818+0000 | compress | METRIC - Compressed module size: 90.189824 MB\n",
            "(21/33): Propagating: 100% 32/32 [00:00<00:00, 83.70it/s]\n",
            "(22/33): Calibrating: 100% 32/32 [00:02<00:00, 15.43it/s]\n",
            "2025-12-03T18:15:35.140775+0000 | compress_modules | INFO - Quantizing model.language_model.layers.21.self_attn.q_proj using 32 samples\n",
            "2025-12-03T18:15:37.278426+0000 | compress | METRIC - time 2.14s\n",
            "2025-12-03T18:15:37.279595+0000 | compress | METRIC - error 18.79\n",
            "2025-12-03T18:15:37.279840+0000 | compress | METRIC - GPU 0 | usage: 11.27% | total memory: 24 GB\n",
            "2025-12-03T18:15:37.280009+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:15:37.280265+0000 | compress_modules | INFO - Quantizing model.language_model.layers.21.self_attn.k_proj using 32 samples\n",
            "2025-12-03T18:15:39.367770+0000 | compress | METRIC - time 2.09s\n",
            "2025-12-03T18:15:39.368932+0000 | compress | METRIC - error 13.90\n",
            "2025-12-03T18:15:39.369171+0000 | compress | METRIC - GPU 0 | usage: 11.27% | total memory: 24 GB\n",
            "2025-12-03T18:15:39.369342+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:15:39.369588+0000 | compress_modules | INFO - Quantizing model.language_model.layers.21.self_attn.v_proj using 32 samples\n",
            "2025-12-03T18:15:41.463519+0000 | compress | METRIC - time 2.09s\n",
            "2025-12-03T18:15:41.464711+0000 | compress | METRIC - error 6.65\n",
            "2025-12-03T18:15:41.464975+0000 | compress | METRIC - GPU 0 | usage: 11.27% | total memory: 24 GB\n",
            "2025-12-03T18:15:41.465139+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:15:41.465392+0000 | compress_modules | INFO - Quantizing model.language_model.layers.21.self_attn.o_proj using 32 samples\n",
            "2025-12-03T18:15:43.558609+0000 | compress | METRIC - time 2.09s\n",
            "2025-12-03T18:15:43.559742+0000 | compress | METRIC - error 0.08\n",
            "2025-12-03T18:15:43.559998+0000 | compress | METRIC - GPU 0 | usage: 11.27% | total memory: 24 GB\n",
            "2025-12-03T18:15:43.560169+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:15:43.560411+0000 | compress_modules | INFO - Quantizing model.language_model.layers.21.mlp.gate_proj using 32 samples\n",
            "2025-12-03T18:15:45.904833+0000 | compress | METRIC - time 2.34s\n",
            "2025-12-03T18:15:45.905969+0000 | compress | METRIC - error 12.85\n",
            "2025-12-03T18:15:45.906201+0000 | compress | METRIC - GPU 0 | usage: 11.98% | total memory: 24 GB\n",
            "2025-12-03T18:15:45.906371+0000 | compress | METRIC - Compressed module size: 90.21056 MB\n",
            "2025-12-03T18:15:45.906614+0000 | compress_modules | INFO - Quantizing model.language_model.layers.21.mlp.up_proj using 32 samples\n",
            "2025-12-03T18:15:48.208035+0000 | compress | METRIC - time 2.30s\n",
            "2025-12-03T18:15:48.209201+0000 | compress | METRIC - error 10.99\n",
            "2025-12-03T18:15:48.209444+0000 | compress | METRIC - GPU 0 | usage: 11.98% | total memory: 24 GB\n",
            "2025-12-03T18:15:48.209603+0000 | compress | METRIC - Compressed module size: 90.21056 MB\n",
            "2025-12-03T18:15:48.209860+0000 | compress_modules | INFO - Quantizing model.language_model.layers.21.mlp.down_proj using 32 samples\n",
            "2025-12-03T18:15:54.314122+0000 | compress | METRIC - time 6.10s\n",
            "2025-12-03T18:15:54.316528+0000 | compress | METRIC - error 1.42\n",
            "2025-12-03T18:15:54.316776+0000 | compress | METRIC - GPU 0 | usage: 16.01% | total memory: 24 GB\n",
            "2025-12-03T18:15:54.316960+0000 | compress | METRIC - Compressed module size: 90.189824 MB\n",
            "(22/33): Propagating: 100% 32/32 [00:00<00:00, 83.10it/s]\n",
            "(23/33): Calibrating: 100% 32/32 [00:02<00:00, 15.33it/s]\n",
            "2025-12-03T18:15:57.100986+0000 | compress_modules | INFO - Quantizing model.language_model.layers.22.self_attn.q_proj using 32 samples\n",
            "2025-12-03T18:15:59.255000+0000 | compress | METRIC - time 2.15s\n",
            "2025-12-03T18:15:59.256115+0000 | compress | METRIC - error 19.17\n",
            "2025-12-03T18:15:59.256361+0000 | compress | METRIC - GPU 0 | usage: 11.27% | total memory: 24 GB\n",
            "2025-12-03T18:15:59.256520+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:15:59.256768+0000 | compress_modules | INFO - Quantizing model.language_model.layers.22.self_attn.k_proj using 32 samples\n",
            "2025-12-03T18:16:01.354036+0000 | compress | METRIC - time 2.10s\n",
            "2025-12-03T18:16:01.355184+0000 | compress | METRIC - error 15.35\n",
            "2025-12-03T18:16:01.355425+0000 | compress | METRIC - GPU 0 | usage: 11.27% | total memory: 24 GB\n",
            "2025-12-03T18:16:01.355587+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:16:01.355852+0000 | compress_modules | INFO - Quantizing model.language_model.layers.22.self_attn.v_proj using 32 samples\n",
            "2025-12-03T18:16:03.432897+0000 | compress | METRIC - time 2.08s\n",
            "2025-12-03T18:16:03.434111+0000 | compress | METRIC - error 7.19\n",
            "2025-12-03T18:16:03.434389+0000 | compress | METRIC - GPU 0 | usage: 11.27% | total memory: 24 GB\n",
            "2025-12-03T18:16:03.434580+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:16:03.434866+0000 | compress_modules | INFO - Quantizing model.language_model.layers.22.self_attn.o_proj using 32 samples\n",
            "2025-12-03T18:16:05.500586+0000 | compress | METRIC - time 2.07s\n",
            "2025-12-03T18:16:05.501704+0000 | compress | METRIC - error 0.21\n",
            "2025-12-03T18:16:05.501951+0000 | compress | METRIC - GPU 0 | usage: 11.27% | total memory: 24 GB\n",
            "2025-12-03T18:16:05.502115+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:16:05.502362+0000 | compress_modules | INFO - Quantizing model.language_model.layers.22.mlp.gate_proj using 32 samples\n",
            "2025-12-03T18:16:07.828331+0000 | compress | METRIC - time 2.33s\n",
            "2025-12-03T18:16:07.829462+0000 | compress | METRIC - error 14.03\n",
            "2025-12-03T18:16:07.829688+0000 | compress | METRIC - GPU 0 | usage: 11.98% | total memory: 24 GB\n",
            "2025-12-03T18:16:07.829862+0000 | compress | METRIC - Compressed module size: 90.21056 MB\n",
            "2025-12-03T18:16:07.830108+0000 | compress_modules | INFO - Quantizing model.language_model.layers.22.mlp.up_proj using 32 samples\n",
            "2025-12-03T18:16:10.146815+0000 | compress | METRIC - time 2.32s\n",
            "2025-12-03T18:16:10.147992+0000 | compress | METRIC - error 11.83\n",
            "2025-12-03T18:16:10.148232+0000 | compress | METRIC - GPU 0 | usage: 11.98% | total memory: 24 GB\n",
            "2025-12-03T18:16:10.148393+0000 | compress | METRIC - Compressed module size: 90.21056 MB\n",
            "2025-12-03T18:16:10.148656+0000 | compress_modules | INFO - Quantizing model.language_model.layers.22.mlp.down_proj using 32 samples\n",
            "2025-12-03T18:16:16.443282+0000 | compress | METRIC - time 6.29s\n",
            "2025-12-03T18:16:16.445874+0000 | compress | METRIC - error 1.57\n",
            "2025-12-03T18:16:16.446152+0000 | compress | METRIC - GPU 0 | usage: 16.01% | total memory: 24 GB\n",
            "2025-12-03T18:16:16.446340+0000 | compress | METRIC - Compressed module size: 90.189824 MB\n",
            "(23/33): Propagating: 100% 32/32 [00:00<00:00, 83.48it/s]\n",
            "(24/33): Calibrating: 100% 32/32 [00:02<00:00, 15.36it/s]\n",
            "2025-12-03T18:16:19.222722+0000 | compress_modules | INFO - Quantizing model.language_model.layers.23.self_attn.q_proj using 32 samples\n",
            "2025-12-03T18:16:21.402549+0000 | compress | METRIC - time 2.18s\n",
            "2025-12-03T18:16:21.403753+0000 | compress | METRIC - error 21.68\n",
            "2025-12-03T18:16:21.404029+0000 | compress | METRIC - GPU 0 | usage: 11.27% | total memory: 24 GB\n",
            "2025-12-03T18:16:21.404209+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:16:21.404469+0000 | compress_modules | INFO - Quantizing model.language_model.layers.23.self_attn.k_proj using 32 samples\n",
            "2025-12-03T18:16:23.517275+0000 | compress | METRIC - time 2.11s\n",
            "2025-12-03T18:16:23.518443+0000 | compress | METRIC - error 16.37\n",
            "2025-12-03T18:16:23.518690+0000 | compress | METRIC - GPU 0 | usage: 11.27% | total memory: 24 GB\n",
            "2025-12-03T18:16:23.518870+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:16:23.519130+0000 | compress_modules | INFO - Quantizing model.language_model.layers.23.self_attn.v_proj using 32 samples\n",
            "2025-12-03T18:16:25.608468+0000 | compress | METRIC - time 2.09s\n",
            "2025-12-03T18:16:25.609645+0000 | compress | METRIC - error 8.72\n",
            "2025-12-03T18:16:25.609896+0000 | compress | METRIC - GPU 0 | usage: 11.27% | total memory: 24 GB\n",
            "2025-12-03T18:16:25.610066+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:16:25.610325+0000 | compress_modules | INFO - Quantizing model.language_model.layers.23.self_attn.o_proj using 32 samples\n",
            "2025-12-03T18:16:27.704439+0000 | compress | METRIC - time 2.09s\n",
            "2025-12-03T18:16:27.705606+0000 | compress | METRIC - error 0.19\n",
            "2025-12-03T18:16:27.705854+0000 | compress | METRIC - GPU 0 | usage: 11.27% | total memory: 24 GB\n",
            "2025-12-03T18:16:27.706023+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:16:27.706282+0000 | compress_modules | INFO - Quantizing model.language_model.layers.23.mlp.gate_proj using 32 samples\n",
            "2025-12-03T18:16:29.969103+0000 | compress | METRIC - time 2.26s\n",
            "2025-12-03T18:16:29.970260+0000 | compress | METRIC - error 15.29\n",
            "2025-12-03T18:16:29.970496+0000 | compress | METRIC - GPU 0 | usage: 11.98% | total memory: 24 GB\n",
            "2025-12-03T18:16:29.970667+0000 | compress | METRIC - Compressed module size: 90.21056 MB\n",
            "2025-12-03T18:16:29.970928+0000 | compress_modules | INFO - Quantizing model.language_model.layers.23.mlp.up_proj using 32 samples\n",
            "2025-12-03T18:16:32.318461+0000 | compress | METRIC - time 2.35s\n",
            "2025-12-03T18:16:32.319632+0000 | compress | METRIC - error 13.09\n",
            "2025-12-03T18:16:32.319877+0000 | compress | METRIC - GPU 0 | usage: 11.98% | total memory: 24 GB\n",
            "2025-12-03T18:16:32.320048+0000 | compress | METRIC - Compressed module size: 90.21056 MB\n",
            "2025-12-03T18:16:32.320302+0000 | compress_modules | INFO - Quantizing model.language_model.layers.23.mlp.down_proj using 32 samples\n",
            "2025-12-03T18:16:38.506224+0000 | compress | METRIC - time 6.19s\n",
            "2025-12-03T18:16:38.508593+0000 | compress | METRIC - error 1.93\n",
            "2025-12-03T18:16:38.508849+0000 | compress | METRIC - GPU 0 | usage: 16.01% | total memory: 24 GB\n",
            "2025-12-03T18:16:38.509021+0000 | compress | METRIC - Compressed module size: 90.189824 MB\n",
            "(24/33): Propagating: 100% 32/32 [00:00<00:00, 83.85it/s]\n",
            "(25/33): Calibrating: 100% 32/32 [00:02<00:00, 15.33it/s]\n",
            "2025-12-03T18:16:41.280699+0000 | compress_modules | INFO - Quantizing model.language_model.layers.24.self_attn.q_proj using 32 samples\n",
            "2025-12-03T18:16:43.437244+0000 | compress | METRIC - time 2.16s\n",
            "2025-12-03T18:16:43.438393+0000 | compress | METRIC - error 20.13\n",
            "2025-12-03T18:16:43.438630+0000 | compress | METRIC - GPU 0 | usage: 11.27% | total memory: 24 GB\n",
            "2025-12-03T18:16:43.438804+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:16:43.439109+0000 | compress_modules | INFO - Quantizing model.language_model.layers.24.self_attn.k_proj using 32 samples\n",
            "2025-12-03T18:16:45.528107+0000 | compress | METRIC - time 2.09s\n",
            "2025-12-03T18:16:45.529263+0000 | compress | METRIC - error 16.72\n",
            "2025-12-03T18:16:45.529511+0000 | compress | METRIC - GPU 0 | usage: 11.27% | total memory: 24 GB\n",
            "2025-12-03T18:16:45.529678+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:16:45.529951+0000 | compress_modules | INFO - Quantizing model.language_model.layers.24.self_attn.v_proj using 32 samples\n",
            "2025-12-03T18:16:47.641479+0000 | compress | METRIC - time 2.11s\n",
            "2025-12-03T18:16:47.642647+0000 | compress | METRIC - error 8.77\n",
            "2025-12-03T18:16:47.642899+0000 | compress | METRIC - GPU 0 | usage: 11.27% | total memory: 24 GB\n",
            "2025-12-03T18:16:47.643073+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:16:47.643337+0000 | compress_modules | INFO - Quantizing model.language_model.layers.24.self_attn.o_proj using 32 samples\n",
            "2025-12-03T18:16:49.726046+0000 | compress | METRIC - time 2.08s\n",
            "2025-12-03T18:16:49.727229+0000 | compress | METRIC - error 0.21\n",
            "2025-12-03T18:16:49.727471+0000 | compress | METRIC - GPU 0 | usage: 11.27% | total memory: 24 GB\n",
            "2025-12-03T18:16:49.727639+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:16:49.727908+0000 | compress_modules | INFO - Quantizing model.language_model.layers.24.mlp.gate_proj using 32 samples\n",
            "2025-12-03T18:16:51.991210+0000 | compress | METRIC - time 2.26s\n",
            "2025-12-03T18:16:51.992374+0000 | compress | METRIC - error 16.34\n",
            "2025-12-03T18:16:51.992620+0000 | compress | METRIC - GPU 0 | usage: 11.98% | total memory: 24 GB\n",
            "2025-12-03T18:16:51.992782+0000 | compress | METRIC - Compressed module size: 90.21056 MB\n",
            "2025-12-03T18:16:51.993053+0000 | compress_modules | INFO - Quantizing model.language_model.layers.24.mlp.up_proj using 32 samples\n",
            "2025-12-03T18:16:54.296302+0000 | compress | METRIC - time 2.30s\n",
            "2025-12-03T18:16:54.297531+0000 | compress | METRIC - error 13.90\n",
            "2025-12-03T18:16:54.297767+0000 | compress | METRIC - GPU 0 | usage: 11.98% | total memory: 24 GB\n",
            "2025-12-03T18:16:54.297943+0000 | compress | METRIC - Compressed module size: 90.21056 MB\n",
            "2025-12-03T18:16:54.298194+0000 | compress_modules | INFO - Quantizing model.language_model.layers.24.mlp.down_proj using 32 samples\n",
            "2025-12-03T18:17:00.613533+0000 | compress | METRIC - time 6.32s\n",
            "2025-12-03T18:17:00.616027+0000 | compress | METRIC - error 1.98\n",
            "2025-12-03T18:17:00.616289+0000 | compress | METRIC - GPU 0 | usage: 16.01% | total memory: 24 GB\n",
            "2025-12-03T18:17:00.616461+0000 | compress | METRIC - Compressed module size: 90.189824 MB\n",
            "(25/33): Propagating: 100% 32/32 [00:00<00:00, 83.78it/s]\n",
            "(26/33): Calibrating: 100% 32/32 [00:02<00:00, 15.36it/s]\n",
            "2025-12-03T18:17:03.388301+0000 | compress_modules | INFO - Quantizing model.language_model.layers.25.self_attn.q_proj using 32 samples\n",
            "2025-12-03T18:17:05.546476+0000 | compress | METRIC - time 2.16s\n",
            "2025-12-03T18:17:05.547670+0000 | compress | METRIC - error 23.57\n",
            "2025-12-03T18:17:05.547955+0000 | compress | METRIC - GPU 0 | usage: 11.27% | total memory: 24 GB\n",
            "2025-12-03T18:17:05.548140+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:17:05.548405+0000 | compress_modules | INFO - Quantizing model.language_model.layers.25.self_attn.k_proj using 32 samples\n",
            "2025-12-03T18:17:07.709881+0000 | compress | METRIC - time 2.16s\n",
            "2025-12-03T18:17:07.711057+0000 | compress | METRIC - error 18.45\n",
            "2025-12-03T18:17:07.711310+0000 | compress | METRIC - GPU 0 | usage: 11.27% | total memory: 24 GB\n",
            "2025-12-03T18:17:07.711498+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:17:07.711751+0000 | compress_modules | INFO - Quantizing model.language_model.layers.25.self_attn.v_proj using 32 samples\n",
            "2025-12-03T18:17:09.792586+0000 | compress | METRIC - time 2.08s\n",
            "2025-12-03T18:17:09.793774+0000 | compress | METRIC - error 10.81\n",
            "2025-12-03T18:17:09.794027+0000 | compress | METRIC - GPU 0 | usage: 11.27% | total memory: 24 GB\n",
            "2025-12-03T18:17:09.794184+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:17:09.794440+0000 | compress_modules | INFO - Quantizing model.language_model.layers.25.self_attn.o_proj using 32 samples\n",
            "2025-12-03T18:17:11.885208+0000 | compress | METRIC - time 2.09s\n",
            "2025-12-03T18:17:11.886339+0000 | compress | METRIC - error 0.08\n",
            "2025-12-03T18:17:11.886580+0000 | compress | METRIC - GPU 0 | usage: 11.27% | total memory: 24 GB\n",
            "2025-12-03T18:17:11.886740+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:17:11.887001+0000 | compress_modules | INFO - Quantizing model.language_model.layers.25.mlp.gate_proj using 32 samples\n",
            "2025-12-03T18:17:14.166267+0000 | compress | METRIC - time 2.28s\n",
            "2025-12-03T18:17:14.167433+0000 | compress | METRIC - error 18.11\n",
            "2025-12-03T18:17:14.167671+0000 | compress | METRIC - GPU 0 | usage: 11.98% | total memory: 24 GB\n",
            "2025-12-03T18:17:14.167844+0000 | compress | METRIC - Compressed module size: 90.21056 MB\n",
            "2025-12-03T18:17:14.168099+0000 | compress_modules | INFO - Quantizing model.language_model.layers.25.mlp.up_proj using 32 samples\n",
            "2025-12-03T18:17:16.475698+0000 | compress | METRIC - time 2.31s\n",
            "2025-12-03T18:17:16.476922+0000 | compress | METRIC - error 15.41\n",
            "2025-12-03T18:17:16.477202+0000 | compress | METRIC - GPU 0 | usage: 11.98% | total memory: 24 GB\n",
            "2025-12-03T18:17:16.477379+0000 | compress | METRIC - Compressed module size: 90.21056 MB\n",
            "2025-12-03T18:17:16.477657+0000 | compress_modules | INFO - Quantizing model.language_model.layers.25.mlp.down_proj using 32 samples\n",
            "2025-12-03T18:17:22.645897+0000 | compress | METRIC - time 6.17s\n",
            "2025-12-03T18:17:22.648356+0000 | compress | METRIC - error 2.29\n",
            "2025-12-03T18:17:22.648596+0000 | compress | METRIC - GPU 0 | usage: 16.01% | total memory: 24 GB\n",
            "2025-12-03T18:17:22.648759+0000 | compress | METRIC - Compressed module size: 90.189824 MB\n",
            "(26/33): Propagating: 100% 32/32 [00:00<00:00, 83.97it/s]\n",
            "(27/33): Calibrating: 100% 32/32 [00:02<00:00, 15.36it/s]\n",
            "2025-12-03T18:17:25.416701+0000 | compress_modules | INFO - Quantizing model.language_model.layers.26.self_attn.q_proj using 32 samples\n",
            "2025-12-03T18:17:27.586539+0000 | compress | METRIC - time 2.17s\n",
            "2025-12-03T18:17:27.587682+0000 | compress | METRIC - error 26.21\n",
            "2025-12-03T18:17:27.587940+0000 | compress | METRIC - GPU 0 | usage: 11.27% | total memory: 24 GB\n",
            "2025-12-03T18:17:27.588109+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:17:27.588370+0000 | compress_modules | INFO - Quantizing model.language_model.layers.26.self_attn.k_proj using 32 samples\n",
            "2025-12-03T18:17:29.712167+0000 | compress | METRIC - time 2.12s\n",
            "2025-12-03T18:17:29.713338+0000 | compress | METRIC - error 18.83\n",
            "2025-12-03T18:17:29.713600+0000 | compress | METRIC - GPU 0 | usage: 11.27% | total memory: 24 GB\n",
            "2025-12-03T18:17:29.713768+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:17:29.714046+0000 | compress_modules | INFO - Quantizing model.language_model.layers.26.self_attn.v_proj using 32 samples\n",
            "2025-12-03T18:17:31.808566+0000 | compress | METRIC - time 2.09s\n",
            "2025-12-03T18:17:31.809733+0000 | compress | METRIC - error 10.88\n",
            "2025-12-03T18:17:31.809986+0000 | compress | METRIC - GPU 0 | usage: 11.27% | total memory: 24 GB\n",
            "2025-12-03T18:17:31.810148+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:17:31.810408+0000 | compress_modules | INFO - Quantizing model.language_model.layers.26.self_attn.o_proj using 32 samples\n",
            "2025-12-03T18:17:33.878003+0000 | compress | METRIC - time 2.07s\n",
            "2025-12-03T18:17:33.879168+0000 | compress | METRIC - error 0.52\n",
            "2025-12-03T18:17:33.879406+0000 | compress | METRIC - GPU 0 | usage: 11.27% | total memory: 24 GB\n",
            "2025-12-03T18:17:33.879568+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:17:33.879825+0000 | compress_modules | INFO - Quantizing model.language_model.layers.26.mlp.gate_proj using 32 samples\n",
            "2025-12-03T18:17:36.168311+0000 | compress | METRIC - time 2.29s\n",
            "2025-12-03T18:17:36.169502+0000 | compress | METRIC - error 18.56\n",
            "2025-12-03T18:17:36.169741+0000 | compress | METRIC - GPU 0 | usage: 11.98% | total memory: 24 GB\n",
            "2025-12-03T18:17:36.169914+0000 | compress | METRIC - Compressed module size: 90.21056 MB\n",
            "2025-12-03T18:17:36.170204+0000 | compress_modules | INFO - Quantizing model.language_model.layers.26.mlp.up_proj using 32 samples\n",
            "2025-12-03T18:17:38.490924+0000 | compress | METRIC - time 2.32s\n",
            "2025-12-03T18:17:38.492135+0000 | compress | METRIC - error 15.97\n",
            "2025-12-03T18:17:38.492402+0000 | compress | METRIC - GPU 0 | usage: 11.98% | total memory: 24 GB\n",
            "2025-12-03T18:17:38.492577+0000 | compress | METRIC - Compressed module size: 90.21056 MB\n",
            "2025-12-03T18:17:38.492879+0000 | compress_modules | INFO - Quantizing model.language_model.layers.26.mlp.down_proj using 32 samples\n",
            "2025-12-03T18:17:44.766669+0000 | compress | METRIC - time 6.27s\n",
            "2025-12-03T18:17:44.769025+0000 | compress | METRIC - error 2.07\n",
            "2025-12-03T18:17:44.769286+0000 | compress | METRIC - GPU 0 | usage: 16.01% | total memory: 24 GB\n",
            "2025-12-03T18:17:44.769477+0000 | compress | METRIC - Compressed module size: 90.189824 MB\n",
            "(27/33): Propagating: 100% 32/32 [00:00<00:00, 84.17it/s]\n",
            "(28/33): Calibrating: 100% 32/32 [00:02<00:00, 15.30it/s]\n",
            "2025-12-03T18:17:47.547721+0000 | compress_modules | INFO - Quantizing model.language_model.layers.27.self_attn.q_proj using 32 samples\n",
            "2025-12-03T18:17:49.708578+0000 | compress | METRIC - time 2.16s\n",
            "2025-12-03T18:17:49.709755+0000 | compress | METRIC - error 29.71\n",
            "2025-12-03T18:17:49.710011+0000 | compress | METRIC - GPU 0 | usage: 11.27% | total memory: 24 GB\n",
            "2025-12-03T18:17:49.710178+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:17:49.710445+0000 | compress_modules | INFO - Quantizing model.language_model.layers.27.self_attn.k_proj using 32 samples\n",
            "2025-12-03T18:17:51.845173+0000 | compress | METRIC - time 2.13s\n",
            "2025-12-03T18:17:51.846349+0000 | compress | METRIC - error 18.61\n",
            "2025-12-03T18:17:51.846587+0000 | compress | METRIC - GPU 0 | usage: 11.27% | total memory: 24 GB\n",
            "2025-12-03T18:17:51.846776+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:17:51.847057+0000 | compress_modules | INFO - Quantizing model.language_model.layers.27.self_attn.v_proj using 32 samples\n",
            "2025-12-03T18:17:53.987225+0000 | compress | METRIC - time 2.14s\n",
            "2025-12-03T18:17:53.988416+0000 | compress | METRIC - error 11.04\n",
            "2025-12-03T18:17:53.988685+0000 | compress | METRIC - GPU 0 | usage: 11.27% | total memory: 24 GB\n",
            "2025-12-03T18:17:53.988882+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:17:53.989130+0000 | compress_modules | INFO - Quantizing model.language_model.layers.27.self_attn.o_proj using 32 samples\n",
            "2025-12-03T18:17:56.083890+0000 | compress | METRIC - time 2.09s\n",
            "2025-12-03T18:17:56.085043+0000 | compress | METRIC - error 0.26\n",
            "2025-12-03T18:17:56.085312+0000 | compress | METRIC - GPU 0 | usage: 11.27% | total memory: 24 GB\n",
            "2025-12-03T18:17:56.085480+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:17:56.085725+0000 | compress_modules | INFO - Quantizing model.language_model.layers.27.mlp.gate_proj using 32 samples\n",
            "2025-12-03T18:17:58.367458+0000 | compress | METRIC - time 2.28s\n",
            "2025-12-03T18:17:58.368619+0000 | compress | METRIC - error 20.11\n",
            "2025-12-03T18:17:58.368888+0000 | compress | METRIC - GPU 0 | usage: 11.98% | total memory: 24 GB\n",
            "2025-12-03T18:17:58.369059+0000 | compress | METRIC - Compressed module size: 90.21056 MB\n",
            "2025-12-03T18:17:58.369309+0000 | compress_modules | INFO - Quantizing model.language_model.layers.27.mlp.up_proj using 32 samples\n",
            "2025-12-03T18:18:00.658412+0000 | compress | METRIC - time 2.29s\n",
            "2025-12-03T18:18:00.659595+0000 | compress | METRIC - error 17.46\n",
            "2025-12-03T18:18:00.659848+0000 | compress | METRIC - GPU 0 | usage: 11.98% | total memory: 24 GB\n",
            "2025-12-03T18:18:00.660022+0000 | compress | METRIC - Compressed module size: 90.21056 MB\n",
            "2025-12-03T18:18:00.660314+0000 | compress_modules | INFO - Quantizing model.language_model.layers.27.mlp.down_proj using 32 samples\n",
            "2025-12-03T18:18:06.746173+0000 | compress | METRIC - time 6.09s\n",
            "2025-12-03T18:18:06.748681+0000 | compress | METRIC - error 2.33\n",
            "2025-12-03T18:18:06.748987+0000 | compress | METRIC - GPU 0 | usage: 16.01% | total memory: 24 GB\n",
            "2025-12-03T18:18:06.749169+0000 | compress | METRIC - Compressed module size: 90.189824 MB\n",
            "(28/33): Propagating: 100% 32/32 [00:00<00:00, 83.16it/s]\n",
            "(29/33): Calibrating: 100% 32/32 [00:02<00:00, 15.36it/s]\n",
            "2025-12-03T18:18:09.523296+0000 | compress_modules | INFO - Quantizing model.language_model.layers.28.self_attn.q_proj using 32 samples\n",
            "2025-12-03T18:18:11.656944+0000 | compress | METRIC - time 2.13s\n",
            "2025-12-03T18:18:11.658143+0000 | compress | METRIC - error 34.85\n",
            "2025-12-03T18:18:11.658394+0000 | compress | METRIC - GPU 0 | usage: 11.27% | total memory: 24 GB\n",
            "2025-12-03T18:18:11.658613+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:18:11.658887+0000 | compress_modules | INFO - Quantizing model.language_model.layers.28.self_attn.k_proj using 32 samples\n",
            "2025-12-03T18:18:13.741808+0000 | compress | METRIC - time 2.08s\n",
            "2025-12-03T18:18:13.742979+0000 | compress | METRIC - error 19.42\n",
            "2025-12-03T18:18:13.743229+0000 | compress | METRIC - GPU 0 | usage: 11.27% | total memory: 24 GB\n",
            "2025-12-03T18:18:13.743425+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:18:13.743680+0000 | compress_modules | INFO - Quantizing model.language_model.layers.28.self_attn.v_proj using 32 samples\n",
            "2025-12-03T18:18:15.805125+0000 | compress | METRIC - time 2.06s\n",
            "2025-12-03T18:18:15.806303+0000 | compress | METRIC - error 12.32\n",
            "2025-12-03T18:18:15.806539+0000 | compress | METRIC - GPU 0 | usage: 11.27% | total memory: 24 GB\n",
            "2025-12-03T18:18:15.806736+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:18:15.807020+0000 | compress_modules | INFO - Quantizing model.language_model.layers.28.self_attn.o_proj using 32 samples\n",
            "2025-12-03T18:18:17.904321+0000 | compress | METRIC - time 2.10s\n",
            "2025-12-03T18:18:17.905564+0000 | compress | METRIC - error 0.34\n",
            "2025-12-03T18:18:17.905825+0000 | compress | METRIC - GPU 0 | usage: 11.27% | total memory: 24 GB\n",
            "2025-12-03T18:18:17.906034+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:18:17.906292+0000 | compress_modules | INFO - Quantizing model.language_model.layers.28.mlp.gate_proj using 32 samples\n",
            "2025-12-03T18:18:20.173216+0000 | compress | METRIC - time 2.27s\n",
            "2025-12-03T18:18:20.174384+0000 | compress | METRIC - error 20.74\n",
            "2025-12-03T18:18:20.174639+0000 | compress | METRIC - GPU 0 | usage: 11.98% | total memory: 24 GB\n",
            "2025-12-03T18:18:20.174818+0000 | compress | METRIC - Compressed module size: 90.21056 MB\n",
            "2025-12-03T18:18:20.175080+0000 | compress_modules | INFO - Quantizing model.language_model.layers.28.mlp.up_proj using 32 samples\n",
            "2025-12-03T18:18:22.432677+0000 | compress | METRIC - time 2.26s\n",
            "2025-12-03T18:18:22.433866+0000 | compress | METRIC - error 18.57\n",
            "2025-12-03T18:18:22.434129+0000 | compress | METRIC - GPU 0 | usage: 11.98% | total memory: 24 GB\n",
            "2025-12-03T18:18:22.434308+0000 | compress | METRIC - Compressed module size: 90.21056 MB\n",
            "2025-12-03T18:18:22.434563+0000 | compress_modules | INFO - Quantizing model.language_model.layers.28.mlp.down_proj using 32 samples\n",
            "2025-12-03T18:18:28.619597+0000 | compress | METRIC - time 6.18s\n",
            "2025-12-03T18:18:28.621961+0000 | compress | METRIC - error 2.69\n",
            "2025-12-03T18:18:28.622245+0000 | compress | METRIC - GPU 0 | usage: 16.01% | total memory: 24 GB\n",
            "2025-12-03T18:18:28.622442+0000 | compress | METRIC - Compressed module size: 90.189824 MB\n",
            "(29/33): Propagating: 100% 32/32 [00:00<00:00, 84.80it/s]\n",
            "(30/33): Calibrating: 100% 32/32 [00:02<00:00, 15.35it/s]\n",
            "2025-12-03T18:18:31.395662+0000 | compress_modules | INFO - Quantizing model.language_model.layers.29.self_attn.q_proj using 32 samples\n",
            "2025-12-03T18:18:33.540903+0000 | compress | METRIC - time 2.14s\n",
            "2025-12-03T18:18:33.542122+0000 | compress | METRIC - error 33.69\n",
            "2025-12-03T18:18:33.542412+0000 | compress | METRIC - GPU 0 | usage: 11.27% | total memory: 24 GB\n",
            "2025-12-03T18:18:33.542602+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:18:33.542903+0000 | compress_modules | INFO - Quantizing model.language_model.layers.29.self_attn.k_proj using 32 samples\n",
            "2025-12-03T18:18:35.645097+0000 | compress | METRIC - time 2.10s\n",
            "2025-12-03T18:18:35.646346+0000 | compress | METRIC - error 18.63\n",
            "2025-12-03T18:18:35.646672+0000 | compress | METRIC - GPU 0 | usage: 11.27% | total memory: 24 GB\n",
            "2025-12-03T18:18:35.646856+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:18:35.647111+0000 | compress_modules | INFO - Quantizing model.language_model.layers.29.self_attn.v_proj using 32 samples\n",
            "2025-12-03T18:18:37.726619+0000 | compress | METRIC - time 2.08s\n",
            "2025-12-03T18:18:37.727905+0000 | compress | METRIC - error 11.49\n",
            "2025-12-03T18:18:37.728150+0000 | compress | METRIC - GPU 0 | usage: 11.27% | total memory: 24 GB\n",
            "2025-12-03T18:18:37.728319+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:18:37.728568+0000 | compress_modules | INFO - Quantizing model.language_model.layers.29.self_attn.o_proj using 32 samples\n",
            "2025-12-03T18:18:39.867451+0000 | compress | METRIC - time 2.14s\n",
            "2025-12-03T18:18:39.868654+0000 | compress | METRIC - error 0.38\n",
            "2025-12-03T18:18:39.868911+0000 | compress | METRIC - GPU 0 | usage: 11.27% | total memory: 24 GB\n",
            "2025-12-03T18:18:39.869082+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:18:39.869327+0000 | compress_modules | INFO - Quantizing model.language_model.layers.29.mlp.gate_proj using 32 samples\n",
            "2025-12-03T18:18:42.187362+0000 | compress | METRIC - time 2.32s\n",
            "2025-12-03T18:18:42.188549+0000 | compress | METRIC - error 21.70\n",
            "2025-12-03T18:18:42.188802+0000 | compress | METRIC - GPU 0 | usage: 11.98% | total memory: 24 GB\n",
            "2025-12-03T18:18:42.188981+0000 | compress | METRIC - Compressed module size: 90.21056 MB\n",
            "2025-12-03T18:18:42.189250+0000 | compress_modules | INFO - Quantizing model.language_model.layers.29.mlp.up_proj using 32 samples\n",
            "2025-12-03T18:18:44.508217+0000 | compress | METRIC - time 2.32s\n",
            "2025-12-03T18:18:44.509480+0000 | compress | METRIC - error 19.67\n",
            "2025-12-03T18:18:44.509744+0000 | compress | METRIC - GPU 0 | usage: 11.98% | total memory: 24 GB\n",
            "2025-12-03T18:18:44.509937+0000 | compress | METRIC - Compressed module size: 90.21056 MB\n",
            "2025-12-03T18:18:44.510233+0000 | compress_modules | INFO - Quantizing model.language_model.layers.29.mlp.down_proj using 32 samples\n",
            "2025-12-03T18:18:50.649071+0000 | compress | METRIC - time 6.14s\n",
            "2025-12-03T18:18:50.651465+0000 | compress | METRIC - error 3.31\n",
            "2025-12-03T18:18:50.651700+0000 | compress | METRIC - GPU 0 | usage: 16.01% | total memory: 24 GB\n",
            "2025-12-03T18:18:50.651876+0000 | compress | METRIC - Compressed module size: 90.189824 MB\n",
            "(30/33): Propagating: 100% 32/32 [00:00<00:00, 82.72it/s]\n",
            "(31/33): Calibrating: 100% 32/32 [00:02<00:00, 15.28it/s]\n",
            "2025-12-03T18:18:53.440142+0000 | compress_modules | INFO - Quantizing model.language_model.layers.30.self_attn.q_proj using 32 samples\n",
            "2025-12-03T18:18:55.549563+0000 | compress | METRIC - time 2.11s\n",
            "2025-12-03T18:18:55.550758+0000 | compress | METRIC - error 45.21\n",
            "2025-12-03T18:18:55.551040+0000 | compress | METRIC - GPU 0 | usage: 11.27% | total memory: 24 GB\n",
            "2025-12-03T18:18:55.551269+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:18:55.551565+0000 | compress_modules | INFO - Quantizing model.language_model.layers.30.self_attn.k_proj using 32 samples\n",
            "2025-12-03T18:18:57.648172+0000 | compress | METRIC - time 2.10s\n",
            "2025-12-03T18:18:57.649414+0000 | compress | METRIC - error 19.29\n",
            "2025-12-03T18:18:57.649679+0000 | compress | METRIC - GPU 0 | usage: 11.27% | total memory: 24 GB\n",
            "2025-12-03T18:18:57.649894+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:18:57.650196+0000 | compress_modules | INFO - Quantizing model.language_model.layers.30.self_attn.v_proj using 32 samples\n",
            "2025-12-03T18:18:59.716576+0000 | compress | METRIC - time 2.07s\n",
            "2025-12-03T18:18:59.717812+0000 | compress | METRIC - error 13.31\n",
            "2025-12-03T18:18:59.718077+0000 | compress | METRIC - GPU 0 | usage: 11.27% | total memory: 24 GB\n",
            "2025-12-03T18:18:59.718279+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:18:59.718577+0000 | compress_modules | INFO - Quantizing model.language_model.layers.30.self_attn.o_proj using 32 samples\n",
            "2025-12-03T18:19:01.778250+0000 | compress | METRIC - time 2.06s\n",
            "2025-12-03T18:19:01.779466+0000 | compress | METRIC - error 0.54\n",
            "2025-12-03T18:19:01.779705+0000 | compress | METRIC - GPU 0 | usage: 11.27% | total memory: 24 GB\n",
            "2025-12-03T18:19:01.779879+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:19:01.780129+0000 | compress_modules | INFO - Quantizing model.language_model.layers.30.mlp.gate_proj using 32 samples\n",
            "2025-12-03T18:19:04.068824+0000 | compress | METRIC - time 2.29s\n",
            "2025-12-03T18:19:04.070056+0000 | compress | METRIC - error 23.46\n",
            "2025-12-03T18:19:04.070332+0000 | compress | METRIC - GPU 0 | usage: 11.98% | total memory: 24 GB\n",
            "2025-12-03T18:19:04.070533+0000 | compress | METRIC - Compressed module size: 90.21056 MB\n",
            "2025-12-03T18:19:04.070835+0000 | compress_modules | INFO - Quantizing model.language_model.layers.30.mlp.up_proj using 32 samples\n",
            "2025-12-03T18:19:06.367937+0000 | compress | METRIC - time 2.30s\n",
            "2025-12-03T18:19:06.369496+0000 | compress | METRIC - error 20.61\n",
            "2025-12-03T18:19:06.369822+0000 | compress | METRIC - GPU 0 | usage: 11.98% | total memory: 24 GB\n",
            "2025-12-03T18:19:06.370012+0000 | compress | METRIC - Compressed module size: 90.21056 MB\n",
            "2025-12-03T18:19:06.370311+0000 | compress_modules | INFO - Quantizing model.language_model.layers.30.mlp.down_proj using 32 samples\n",
            "2025-12-03T18:19:12.576920+0000 | compress | METRIC - time 6.21s\n",
            "2025-12-03T18:19:12.579350+0000 | compress | METRIC - error 6.28\n",
            "2025-12-03T18:19:12.579592+0000 | compress | METRIC - GPU 0 | usage: 16.01% | total memory: 24 GB\n",
            "2025-12-03T18:19:12.579754+0000 | compress | METRIC - Compressed module size: 90.189824 MB\n",
            "(31/33): Propagating: 100% 32/32 [00:00<00:00, 83.77it/s]\n",
            "(32/33): Calibrating: 100% 32/32 [00:02<00:00, 15.19it/s]\n",
            "2025-12-03T18:19:15.377649+0000 | compress_modules | INFO - Quantizing model.language_model.layers.31.self_attn.q_proj using 32 samples\n",
            "2025-12-03T18:19:17.611705+0000 | compress | METRIC - time 2.23s\n",
            "2025-12-03T18:19:17.612955+0000 | compress | METRIC - error 19.37\n",
            "2025-12-03T18:19:17.613214+0000 | compress | METRIC - GPU 0 | usage: 11.27% | total memory: 24 GB\n",
            "2025-12-03T18:19:17.613379+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:19:17.613631+0000 | compress_modules | INFO - Quantizing model.language_model.layers.31.self_attn.k_proj using 32 samples\n",
            "2025-12-03T18:19:19.791411+0000 | compress | METRIC - time 2.18s\n",
            "2025-12-03T18:19:19.792649+0000 | compress | METRIC - error 16.55\n",
            "2025-12-03T18:19:19.792904+0000 | compress | METRIC - GPU 0 | usage: 11.27% | total memory: 24 GB\n",
            "2025-12-03T18:19:19.793075+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:19:19.793334+0000 | compress_modules | INFO - Quantizing model.language_model.layers.31.self_attn.v_proj using 32 samples\n",
            "2025-12-03T18:19:21.897541+0000 | compress | METRIC - time 2.10s\n",
            "2025-12-03T18:19:21.898759+0000 | compress | METRIC - error 8.27\n",
            "2025-12-03T18:19:21.899012+0000 | compress | METRIC - GPU 0 | usage: 11.27% | total memory: 24 GB\n",
            "2025-12-03T18:19:21.899171+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:19:21.899429+0000 | compress_modules | INFO - Quantizing model.language_model.layers.31.self_attn.o_proj using 32 samples\n",
            "2025-12-03T18:19:24.011460+0000 | compress | METRIC - time 2.11s\n",
            "2025-12-03T18:19:24.012684+0000 | compress | METRIC - error 1.41\n",
            "2025-12-03T18:19:24.012937+0000 | compress | METRIC - GPU 0 | usage: 11.27% | total memory: 24 GB\n",
            "2025-12-03T18:19:24.013489+0000 | compress | METRIC - Compressed module size: 33.56672 MB\n",
            "2025-12-03T18:19:24.013770+0000 | compress_modules | INFO - Quantizing model.language_model.layers.31.mlp.gate_proj using 32 samples\n",
            "2025-12-03T18:19:26.380875+0000 | compress | METRIC - time 2.37s\n",
            "2025-12-03T18:19:26.382140+0000 | compress | METRIC - error 21.36\n",
            "2025-12-03T18:19:26.382388+0000 | compress | METRIC - GPU 0 | usage: 11.98% | total memory: 24 GB\n",
            "2025-12-03T18:19:26.382550+0000 | compress | METRIC - Compressed module size: 90.21056 MB\n",
            "2025-12-03T18:19:26.382810+0000 | compress_modules | INFO - Quantizing model.language_model.layers.31.mlp.up_proj using 32 samples\n",
            "2025-12-03T18:19:28.777388+0000 | compress | METRIC - time 2.39s\n",
            "2025-12-03T18:19:28.778721+0000 | compress | METRIC - error 19.24\n",
            "2025-12-03T18:19:28.779023+0000 | compress | METRIC - GPU 0 | usage: 11.98% | total memory: 24 GB\n",
            "2025-12-03T18:19:28.779228+0000 | compress | METRIC - Compressed module size: 90.21056 MB\n",
            "2025-12-03T18:19:28.779498+0000 | compress_modules | INFO - Quantizing model.language_model.layers.31.mlp.down_proj using 32 samples\n",
            "2025-12-03T18:19:34.934893+0000 | compress | METRIC - time 6.16s\n",
            "2025-12-03T18:19:34.937382+0000 | compress | METRIC - error 17.62\n",
            "2025-12-03T18:19:34.937622+0000 | compress | METRIC - GPU 0 | usage: 16.01% | total memory: 24 GB\n",
            "2025-12-03T18:19:34.937796+0000 | compress | METRIC - Compressed module size: 90.189824 MB\n",
            "(32/33): Propagating: 100% 32/32 [00:00<00:00, 83.90it/s]\n",
            "(33/33): Calibrating: 100% 32/32 [00:00<00:00, 134.46it/s]\n",
            "(33/33): Propagating: 100% 32/32 [00:00<00:00, 175.78it/s]\n",
            "2025-12-03T18:19:36.240220+0000 | finalize | INFO - Compression lifecycle finalized for 1 modifiers\n",
            "2025-12-03T18:19:36.317542+0000 | get_model_compressor | INFO - skip_sparsity_compression_stats set to True. Skipping sparsity compression statistic calculations. No sparsity compressor will be applied.\n",
            "Compressing model: 224it [00:13, 16.37it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install vllm==0.6.3"
      ],
      "metadata": {
        "id": "TJTVW5r_qwjE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa27bd22-4b7d-4d21-d700-11ccb7b88ed9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: vllm==0.6.3 in /usr/local/lib/python3.12/dist-packages (0.6.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from vllm==0.6.3) (7.1.3)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (from vllm==0.6.3) (0.2.1)\n",
            "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.12/dist-packages (from vllm==0.6.3) (1.26.4)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from vllm==0.6.3) (2.32.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from vllm==0.6.3) (4.67.1)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.12/dist-packages (from vllm==0.6.3) (9.0.0)\n",
            "Requirement already satisfied: transformers>=4.45.0 in /usr/local/lib/python3.12/dist-packages (from vllm==0.6.3) (4.57.3)\n",
            "Requirement already satisfied: tokenizers>=0.19.1 in /usr/local/lib/python3.12/dist-packages (from vllm==0.6.3) (0.22.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from vllm==0.6.3) (6.33.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from vllm==0.6.3) (3.13.2)\n",
            "Requirement already satisfied: openai>=1.40.0 in /usr/local/lib/python3.12/dist-packages (from vllm==0.6.3) (2.8.1)\n",
            "Requirement already satisfied: uvicorn[standard] in /usr/local/lib/python3.12/dist-packages (from vllm==0.6.3) (0.38.0)\n",
            "Requirement already satisfied: pydantic>=2.9 in /usr/local/lib/python3.12/dist-packages (from vllm==0.6.3) (2.12.5)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from vllm==0.6.3) (12.0.0)\n",
            "Requirement already satisfied: prometheus-client>=0.18.0 in /usr/local/lib/python3.12/dist-packages (from vllm==0.6.3) (0.23.1)\n",
            "Requirement already satisfied: prometheus-fastapi-instrumentator>=7.0.0 in /usr/local/lib/python3.12/dist-packages (from vllm==0.6.3) (7.1.0)\n",
            "Requirement already satisfied: tiktoken>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from vllm==0.6.3) (0.12.0)\n",
            "Requirement already satisfied: lm-format-enforcer==0.10.6 in /usr/local/lib/python3.12/dist-packages (from vllm==0.6.3) (0.10.6)\n",
            "Requirement already satisfied: outlines<0.1,>=0.0.43 in /usr/local/lib/python3.12/dist-packages (from vllm==0.6.3) (0.0.46)\n",
            "Requirement already satisfied: typing-extensions>=4.10 in /usr/local/lib/python3.12/dist-packages (from vllm==0.6.3) (4.15.0)\n",
            "Requirement already satisfied: filelock>=3.10.4 in /usr/local/lib/python3.12/dist-packages (from vllm==0.6.3) (3.20.0)\n",
            "Requirement already satisfied: partial-json-parser in /usr/local/lib/python3.12/dist-packages (from vllm==0.6.3) (0.2.1.1.post7)\n",
            "Requirement already satisfied: pyzmq in /usr/local/lib/python3.12/dist-packages (from vllm==0.6.3) (27.1.0)\n",
            "Requirement already satisfied: msgspec in /usr/local/lib/python3.12/dist-packages (from vllm==0.6.3) (0.20.0)\n",
            "Requirement already satisfied: gguf==0.10.0 in /usr/local/lib/python3.12/dist-packages (from vllm==0.6.3) (0.10.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.12/dist-packages (from vllm==0.6.3) (8.7.0)\n",
            "Requirement already satisfied: mistral-common>=1.4.4 in /usr/local/lib/python3.12/dist-packages (from mistral-common[opencv]>=1.4.4->vllm==0.6.3) (1.8.6)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from vllm==0.6.3) (6.0.3)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (from vllm==0.6.3) (0.8.1)\n",
            "Requirement already satisfied: ray>=2.9 in /usr/local/lib/python3.12/dist-packages (from vllm==0.6.3) (2.52.1)\n",
            "Requirement already satisfied: nvidia-ml-py in /usr/local/lib/python3.12/dist-packages (from vllm==0.6.3) (13.580.82)\n",
            "Requirement already satisfied: torch==2.4.0 in /usr/local/lib/python3.12/dist-packages (from vllm==0.6.3) (2.4.0)\n",
            "Requirement already satisfied: torchvision==0.19 in /usr/local/lib/python3.12/dist-packages (from vllm==0.6.3) (0.19.0)\n",
            "Requirement already satisfied: xformers==0.0.27.post2 in /usr/local/lib/python3.12/dist-packages (from vllm==0.6.3) (0.0.27.post2)\n",
            "Requirement already satisfied: six>=1.16.0 in /usr/local/lib/python3.12/dist-packages (from vllm==0.6.3) (1.17.0)\n",
            "Requirement already satisfied: setuptools>=74.1.1 in /usr/local/lib/python3.12/dist-packages (from vllm==0.6.3) (80.9.0)\n",
            "Requirement already satisfied: fastapi!=0.113.*,!=0.114.0,>=0.107.0 in /usr/local/lib/python3.12/dist-packages (from vllm==0.6.3) (0.123.5)\n",
            "Requirement already satisfied: interegular>=0.3.2 in /usr/local/lib/python3.12/dist-packages (from lm-format-enforcer==0.10.6->vllm==0.6.3) (0.3.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from lm-format-enforcer==0.10.6->vllm==0.6.3) (25.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0->vllm==0.6.3) (1.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0->vllm==0.6.3) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0->vllm==0.6.3) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0->vllm==0.6.3) (2025.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0->vllm==0.6.3) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0->vllm==0.6.3) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0->vllm==0.6.3) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0->vllm==0.6.3) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0->vllm==0.6.3) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0->vllm==0.6.3) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0->vllm==0.6.3) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0->vllm==0.6.3) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0->vllm==0.6.3) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0->vllm==0.6.3) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0->vllm==0.6.3) (12.1.105)\n",
            "Requirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0->vllm==0.6.3) (3.0.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.4.0->vllm==0.6.3) (12.9.86)\n",
            "Requirement already satisfied: starlette<0.51.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from fastapi!=0.113.*,!=0.114.0,>=0.107.0->vllm==0.6.3) (0.50.0)\n",
            "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from fastapi!=0.113.*,!=0.114.0,>=0.107.0->vllm==0.6.3) (0.0.4)\n",
            "Requirement already satisfied: jsonschema>=4.21.1 in /usr/local/lib/python3.12/dist-packages (from mistral-common>=1.4.4->mistral-common[opencv]>=1.4.4->vllm==0.6.3) (4.25.1)\n",
            "Requirement already satisfied: pydantic-extra-types>=2.10.5 in /usr/local/lib/python3.12/dist-packages (from pydantic-extra-types[pycountry]>=2.10.5->mistral-common>=1.4.4->mistral-common[opencv]>=1.4.4->vllm==0.6.3) (2.10.6)\n",
            "Requirement already satisfied: opencv-python-headless>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from mistral-common[opencv]>=1.4.4->vllm==0.6.3) (4.11.0.86)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.40.0->vllm==0.6.3) (4.12.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.40.0->vllm==0.6.3) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.40.0->vllm==0.6.3) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.40.0->vllm==0.6.3) (0.12.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai>=1.40.0->vllm==0.6.3) (1.3.1)\n",
            "Requirement already satisfied: lark in /usr/local/lib/python3.12/dist-packages (from outlines<0.1,>=0.0.43->vllm==0.6.3) (1.3.1)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.12/dist-packages (from outlines<0.1,>=0.0.43->vllm==0.6.3) (1.6.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.12/dist-packages (from outlines<0.1,>=0.0.43->vllm==0.6.3) (3.1.2)\n",
            "Requirement already satisfied: diskcache in /usr/local/lib/python3.12/dist-packages (from outlines<0.1,>=0.0.43->vllm==0.6.3) (5.6.3)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.12/dist-packages (from outlines<0.1,>=0.0.43->vllm==0.6.3) (0.62.1)\n",
            "Requirement already satisfied: referencing in /usr/local/lib/python3.12/dist-packages (from outlines<0.1,>=0.0.43->vllm==0.6.3) (0.37.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (from outlines<0.1,>=0.0.43->vllm==0.6.3) (4.4.1)\n",
            "Requirement already satisfied: pycountry in /usr/local/lib/python3.12/dist-packages (from outlines<0.1,>=0.0.43->vllm==0.6.3) (24.6.1)\n",
            "Requirement already satisfied: pyairports in /usr/local/lib/python3.12/dist-packages (from outlines<0.1,>=0.0.43->vllm==0.6.3) (0.0.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9->vllm==0.6.3) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.5 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9->vllm==0.6.3) (2.41.5)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9->vllm==0.6.3) (0.4.2)\n",
            "Requirement already satisfied: click!=8.3.*,>=7.0 in /usr/local/lib/python3.12/dist-packages (from ray>=2.9->vllm==0.6.3) (8.2.1)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from ray>=2.9->vllm==0.6.3) (1.1.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->vllm==0.6.3) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->vllm==0.6.3) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->vllm==0.6.3) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->vllm==0.6.3) (2025.11.12)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken>=0.6.0->vllm==0.6.3) (2025.11.3)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in /usr/local/lib/python3.12/dist-packages (from tokenizers>=0.19.1->vllm==0.6.3) (0.36.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.45.0->vllm==0.6.3) (0.7.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm==0.6.3) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm==0.6.3) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm==0.6.3) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm==0.6.3) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm==0.6.3) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm==0.6.3) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->vllm==0.6.3) (1.22.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata->vllm==0.6.3) (3.23.0)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]->vllm==0.6.3) (0.16.0)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]->vllm==0.6.3) (0.7.1)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]->vllm==0.6.3) (1.2.1)\n",
            "Requirement already satisfied: uvloop>=0.15.1 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]->vllm==0.6.3) (0.22.1)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]->vllm==0.6.3) (1.1.1)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]->vllm==0.6.3) (15.0.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai>=1.40.0->vllm==0.6.3) (1.0.9)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.19.1->vllm==0.6.3) (1.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.21.1->mistral-common>=1.4.4->mistral-common[opencv]>=1.4.4->vllm==0.6.3) (2025.9.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.21.1->mistral-common>=1.4.4->mistral-common[opencv]>=1.4.4->vllm==0.6.3) (0.30.0)\n",
            "Requirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets->outlines<0.1,>=0.0.43->vllm==0.6.3) (22.0.0)\n",
            "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets->outlines<0.1,>=0.0.43->vllm==0.6.3) (0.4.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets->outlines<0.1,>=0.0.43->vllm==0.6.3) (2.3.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets->outlines<0.1,>=0.0.43->vllm==0.6.3) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.19 in /usr/local/lib/python3.12/dist-packages (from datasets->outlines<0.1,>=0.0.43->vllm==0.6.3) (0.70.18)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.4.0->vllm==0.6.3) (3.0.3)\n",
            "Requirement already satisfied: llvmlite<0.46,>=0.45.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba->outlines<0.1,>=0.0.43->vllm==0.6.3) (0.45.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->torch==2.4.0->vllm==0.6.3) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets->outlines<0.1,>=0.0.43->vllm==0.6.3) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets->outlines<0.1,>=0.0.43->vllm==0.6.3) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets->outlines<0.1,>=0.0.43->vllm==0.6.3) (2025.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python test_llmcompressor_awq_vllm.py"
      ],
      "metadata": {
        "id": "WBtL9gRWqoaK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a996304-fef4-47e3-d62b-ad4385dc7a4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-03 18:36:41.463245: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-03 18:36:41.481020: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764787001.502464    9982 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764787001.509102    9982 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764787001.525654    9982 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764787001.525686    9982 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764787001.525689    9982 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764787001.525691    9982 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-03 18:36:41.531502: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
            "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
            "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
            "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
            "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
            "/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
            "  warnings.warn(\n",
            "Loading sample image...\n",
            "Image size: (1000, 667)\n",
            "\n",
            "/usr/local/lib/python3.12/dist-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:\n",
            "No module named 'vllm._version'\n",
            "  from vllm.version import __version__ as VLLM_VERSION\n",
            "\n",
            "AWQ quantization complete. Loading quantized model...\n",
            "\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "ERROR 12-03 18:37:08 registry.py:264] Error in inspecting model architecture 'LlavaForConditionalGeneration'\n",
            "ERROR 12-03 18:37:08 registry.py:264] Traceback (most recent call last):\n",
            "ERROR 12-03 18:37:08 registry.py:264]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/registry.py\", line 426, in _run_in_subprocess\n",
            "ERROR 12-03 18:37:08 registry.py:264]     returned.check_returncode()\n",
            "ERROR 12-03 18:37:08 registry.py:264]   File \"/usr/lib/python3.12/subprocess.py\", line 502, in check_returncode\n",
            "ERROR 12-03 18:37:08 registry.py:264]     raise CalledProcessError(self.returncode, self.args, self.stdout,\n",
            "ERROR 12-03 18:37:08 registry.py:264] subprocess.CalledProcessError: Command '['/usr/bin/python3', '-m', 'vllm.model_executor.models.registry']' returned non-zero exit status 1.\n",
            "ERROR 12-03 18:37:08 registry.py:264] \n",
            "ERROR 12-03 18:37:08 registry.py:264] The above exception was the direct cause of the following exception:\n",
            "ERROR 12-03 18:37:08 registry.py:264] \n",
            "ERROR 12-03 18:37:08 registry.py:264] Traceback (most recent call last):\n",
            "ERROR 12-03 18:37:08 registry.py:264]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/registry.py\", line 262, in _try_inspect_model_cls\n",
            "ERROR 12-03 18:37:08 registry.py:264]     return model.inspect_model_cls()\n",
            "ERROR 12-03 18:37:08 registry.py:264]            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "ERROR 12-03 18:37:08 registry.py:264]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/registry.py\", line 224, in inspect_model_cls\n",
            "ERROR 12-03 18:37:08 registry.py:264]     return _run_in_subprocess(\n",
            "ERROR 12-03 18:37:08 registry.py:264]            ^^^^^^^^^^^^^^^^^^^\n",
            "ERROR 12-03 18:37:08 registry.py:264]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/registry.py\", line 429, in _run_in_subprocess\n",
            "ERROR 12-03 18:37:08 registry.py:264]     raise RuntimeError(f\"Error raised in subprocess:\\n\"\n",
            "ERROR 12-03 18:37:08 registry.py:264] RuntimeError: Error raised in subprocess:\n",
            "ERROR 12-03 18:37:08 registry.py:264] 2025-12-03 18:36:59.781304: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "ERROR 12-03 18:37:08 registry.py:264] WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "ERROR 12-03 18:37:08 registry.py:264] E0000 00:00:1764787019.802809   10137 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "ERROR 12-03 18:37:08 registry.py:264] E0000 00:00:1764787019.809291   10137 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "ERROR 12-03 18:37:08 registry.py:264] W0000 00:00:1764787019.825159   10137 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "ERROR 12-03 18:37:08 registry.py:264] W0000 00:00:1764787019.825184   10137 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "ERROR 12-03 18:37:08 registry.py:264] W0000 00:00:1764787019.825186   10137 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "ERROR 12-03 18:37:08 registry.py:264] W0000 00:00:1764787019.825188   10137 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "ERROR 12-03 18:37:08 registry.py:264] AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
            "ERROR 12-03 18:37:08 registry.py:264] AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
            "ERROR 12-03 18:37:08 registry.py:264] AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
            "ERROR 12-03 18:37:08 registry.py:264] AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
            "ERROR 12-03 18:37:08 registry.py:264] AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
            "ERROR 12-03 18:37:08 registry.py:264] /usr/local/lib/python3.12/dist-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:\n",
            "ERROR 12-03 18:37:08 registry.py:264] No module named 'vllm._version'\n",
            "ERROR 12-03 18:37:08 registry.py:264]   from vllm.version import __version__ as VLLM_VERSION\n",
            "ERROR 12-03 18:37:08 registry.py:264] <frozen runpy>:128: RuntimeWarning: 'vllm.model_executor.models.registry' found in sys.modules after import of package 'vllm.model_executor.models', but prior to execution of 'vllm.model_executor.models.registry'; this may result in unpredictable behaviour\n",
            "ERROR 12-03 18:37:08 registry.py:264] Traceback (most recent call last):\n",
            "ERROR 12-03 18:37:08 registry.py:264]   File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "ERROR 12-03 18:37:08 registry.py:264]   File \"<frozen runpy>\", line 88, in _run_code\n",
            "ERROR 12-03 18:37:08 registry.py:264]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/registry.py\", line 450, in <module>\n",
            "ERROR 12-03 18:37:08 registry.py:264]     _run()\n",
            "ERROR 12-03 18:37:08 registry.py:264]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/registry.py\", line 443, in _run\n",
            "ERROR 12-03 18:37:08 registry.py:264]     result = fn()\n",
            "ERROR 12-03 18:37:08 registry.py:264]              ^^^^\n",
            "ERROR 12-03 18:37:08 registry.py:264]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/registry.py\", line 225, in <lambda>\n",
            "ERROR 12-03 18:37:08 registry.py:264]     lambda: _ModelInfo.from_model_cls(self.load_model_cls()))\n",
            "ERROR 12-03 18:37:08 registry.py:264]                                       ^^^^^^^^^^^^^^^^^^^^^\n",
            "ERROR 12-03 18:37:08 registry.py:264]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/registry.py\", line 228, in load_model_cls\n",
            "ERROR 12-03 18:37:08 registry.py:264]     mod = importlib.import_module(self.module_name)\n",
            "ERROR 12-03 18:37:08 registry.py:264]           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "ERROR 12-03 18:37:08 registry.py:264]   File \"/usr/lib/python3.12/importlib/__init__.py\", line 90, in import_module\n",
            "ERROR 12-03 18:37:08 registry.py:264]     return _bootstrap._gcd_import(name[level:], package, level)\n",
            "ERROR 12-03 18:37:08 registry.py:264]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "ERROR 12-03 18:37:08 registry.py:264]   File \"<frozen importlib._bootstrap>\", line 1387, in _gcd_import\n",
            "ERROR 12-03 18:37:08 registry.py:264]   File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n",
            "ERROR 12-03 18:37:08 registry.py:264]   File \"<frozen importlib._bootstrap>\", line 1331, in _find_and_load_unlocked\n",
            "ERROR 12-03 18:37:08 registry.py:264]   File \"<frozen importlib._bootstrap>\", line 935, in _load_unlocked\n",
            "ERROR 12-03 18:37:08 registry.py:264]   File \"<frozen importlib._bootstrap_external>\", line 999, in exec_module\n",
            "ERROR 12-03 18:37:08 registry.py:264]   File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
            "ERROR 12-03 18:37:08 registry.py:264]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/llava.py\", line 21, in <module>\n",
            "ERROR 12-03 18:37:08 registry.py:264]     from .clip import (CLIPVisionModel, dummy_image_for_clip,\n",
            "ERROR 12-03 18:37:08 registry.py:264]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/clip.py\", line 10, in <module>\n",
            "ERROR 12-03 18:37:08 registry.py:264]     from transformers.models.clip.modeling_clip import CLIPSdpaAttention\n",
            "ERROR 12-03 18:37:08 registry.py:264] ImportError: cannot import name 'CLIPSdpaAttention' from 'transformers.models.clip.modeling_clip' (/usr/local/lib/python3.12/dist-packages/transformers/models/clip/modeling_clip.py). Did you mean: 'CLIPAttention'?\n",
            "ERROR 12-03 18:37:08 registry.py:264] \n",
            "Traceback (most recent call last):\n",
            "  File \"/content/test_llmcompressor_awq_vllm.py\", line 201, in <module>\n",
            "    LLM_inference(image)\n",
            "  File \"/content/test_llmcompressor_awq_vllm.py\", line 174, in LLM_inference\n",
            "    llm = LLM(MODEL_ID)\n",
            "          ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/llm.py\", line 177, in __init__\n",
            "    self.llm_engine = LLMEngine.from_engine_args(\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/llm_engine.py\", line 571, in from_engine_args\n",
            "    engine_config = engine_args.create_engine_config()\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/arg_utils.py\", line 900, in create_engine_config\n",
            "    model_config = self.create_model_config()\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/arg_utils.py\", line 837, in create_model_config\n",
            "    return ModelConfig(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/vllm/config.py\", line 194, in __init__\n",
            "    self.multimodal_config = self._init_multimodal_config(\n",
            "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/vllm/config.py\", line 213, in _init_multimodal_config\n",
            "    if ModelRegistry.is_multimodal_model(architectures):\n",
            "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/registry.py\", line 384, in is_multimodal_model\n",
            "    return self.inspect_model_cls(architectures).supports_multimodal\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/registry.py\", line 353, in inspect_model_cls\n",
            "    return self._raise_for_unsupported(architectures)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/registry.py\", line 314, in _raise_for_unsupported\n",
            "    raise ValueError(\n",
            "ValueError: Model architectures ['LlavaForConditionalGeneration'] are not supported for now. Supported architectures: ['AquilaModel', 'AquilaForCausalLM', 'ArcticForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BloomForCausalLM', 'CohereForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'ExaoneForCausalLM', 'FalconForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'JAISLMHeadModel', 'JambaForCausalLM', 'LlamaForCausalLM', 'LLaMAForCausalLM', 'MambaForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'QuantMixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiniCPMForCausalLM', 'MiniCPM3ForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'Phi3SmallForCausalLM', 'PhiMoEForCausalLM', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RWForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'SolarForCausalLM', 'XverseForCausalLM', 'BartModel', 'BartForConditionalGeneration', 'MistralModel', 'Qwen2ForRewardModel', 'Gemma2Model', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'FuyuForCausalLM', 'InternVLChatModel', 'LlavaForConditionalGeneration', 'LlavaNextForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'LlavaOnevisionForConditionalGeneration', 'MiniCPMV', 'MolmoForCausalLM', 'NVLM_D', 'PaliGemmaForConditionalGeneration', 'Phi3VForCausalLM', 'PixtralForConditionalGeneration', 'QWenLMHeadModel', 'Qwen2VLForConditionalGeneration', 'UltravoxModel', 'MllamaForConditionalGeneration', 'EAGLEModel', 'MedusaModel', 'MLPSpeculatorPreTrainedModel']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Batch Test"
      ],
      "metadata": {
        "id": "BOpk631vetP2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python test_baseline_batch.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6AVaFYccexo2",
        "outputId": "ed0a4dba-25d8-4b2b-fac7-3cdf49c9615c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-29 04:35:33.272756: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-29 04:35:33.292752: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764390933.314527   52889 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764390933.320985   52889 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764390933.338333   52889 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764390933.338370   52889 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764390933.338373   52889 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764390933.338375   52889 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-29 04:35:33.343392: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Loading sample image...\n",
            "Image size: (1000, 667)\n",
            "\n",
            "================================================================================\n",
            "Baseline_without_kv_cache Test for LLaVA\n",
            "================================================================================\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Loading checkpoint shards: 100% 3/3 [00:04<00:00,  1.41s/it]\n",
            "Model loaded.\n",
            "\n",
            "==============================\n",
            "  Batch Size Stress Test (true batch forward)\n",
            "==============================\n",
            "\n",
            "--- Batch Size = 4 (1 forward pass with 4 images) ---\n",
            " ==> BS=4: latency=8.535s, throughput=325.23 tok/s, max mem=14.57 GB\n",
            "\n",
            "--- Batch Size = 8 (1 forward pass with 8 images) ---\n",
            " ==> BS=8: latency=10.620s, throughput=522.77 tok/s, max mem=15.98 GB\n",
            "\n",
            "--- Batch Size = 16 (1 forward pass with 16 images) ---\n",
            " ==> BS=16: latency=15.916s, throughput=697.68 tok/s, max mem=18.79 GB\n",
            "\n",
            "--- Batch Size = 32 (1 forward pass with 32 images) ---\n",
            "[ERROR] BS=32 failed: CUDA out of memory. Tried to allocate 400.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 203.38 MiB is free. Process 767154 has 21.96 GiB memory in use. Of the allocated memory 19.66 GiB is allocated by PyTorch, and 2.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python test_flash_attn_batch.py"
      ],
      "metadata": {
        "id": "VjY4U_Ug5m-9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ae7a812-610e-4d81-9248-aaa9268cd3b5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-05 17:33:44.772429: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-05 17:33:44.789606: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764956024.810999    7928 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764956024.817466    7928 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764956024.833652    7928 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764956024.833680    7928 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764956024.833683    7928 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764956024.833686    7928 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-05 17:33:44.838445: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Loading sample image...\n",
            "Image size: (1000, 667)\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "TEST 2: With Flash Attention 2 (Batch Test)\n",
            "--------------------------------------------------------------------------------\n",
            "processor_config.json: 100% 173/173 [00:00<00:00, 1.26MB/s]\n",
            "chat_template.json: 100% 701/701 [00:00<00:00, 7.72MB/s]\n",
            "chat_template.jinja: 100% 674/674 [00:00<00:00, 7.40MB/s]\n",
            "preprocessor_config.json: 100% 505/505 [00:00<00:00, 5.42MB/s]\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "tokenizer_config.json: 1.45kB [00:00, 9.17MB/s]\n",
            "tokenizer.model: 100% 500k/500k [00:01<00:00, 253kB/s]\n",
            "tokenizer.json: 3.62MB [00:00, 177MB/s]\n",
            "added_tokens.json: 100% 41.0/41.0 [00:00<00:00, 487kB/s]\n",
            "special_tokens_map.json: 100% 552/552 [00:00<00:00, 6.65MB/s]\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "config.json: 100% 950/950 [00:00<00:00, 11.0MB/s]\n",
            "model.safetensors.index.json: 70.1kB [00:00, 198MB/s]\n",
            "Fetching 3 files:   0% 0/3 [00:00<?, ?it/s]\n",
            "model-00003-of-00003.safetensors:   0% 0.00/4.18G [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:   0% 0.00/4.99G [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:   0% 0.00/4.96G [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:   0% 546k/4.18G [00:01<3:35:29, 323kB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:   0% 599k/4.96G [00:02<5:02:34, 273kB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:   0% 1.74M/4.18G [00:03<2:07:18, 546kB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:   3% 136M/4.18G [00:05<02:18, 29.2MB/s]  \u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:   0% 8.13k/4.99G [00:06<1030:37:25, 1.35kB/s]\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  10% 404M/4.18G [00:06<00:36, 104MB/s] \u001b[A\n",
            "model-00003-of-00003.safetensors:  14% 605M/4.18G [00:06<00:20, 177MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  19% 807M/4.18G [00:06<00:14, 228MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:   1% 67.6M/4.96G [00:07<08:42, 9.36MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  24% 1.01G/4.18G [00:13<00:43, 72.1MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:   3% 135M/4.96G [00:14<07:58, 10.1MB/s] \u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  26% 1.07G/4.18G [00:14<00:44, 69.3MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:   5% 269M/4.96G [00:14<02:57, 26.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:   8% 403M/4.96G [00:14<01:38, 46.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  29% 1.21G/4.18G [00:14<00:33, 87.7MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  11% 537M/4.96G [00:14<00:59, 74.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  31% 1.28G/4.18G [00:15<00:28, 100MB/s] \u001b[A\n",
            "model-00003-of-00003.safetensors:  35% 1.48G/4.18G [00:15<00:16, 159MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:   0% 51.4k/4.99G [00:15<386:54:28, 3.58kB/s] \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  12% 604M/4.96G [00:15<00:53, 81.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  15% 738M/4.96G [00:16<00:38, 110MB/s] \u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  39% 1.61G/4.18G [00:16<00:15, 161MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  16% 806M/4.96G [00:18<01:03, 65.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  19% 940M/4.96G [00:18<00:39, 103MB/s] \u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  43% 1.81G/4.18G [00:18<00:21, 112MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  47% 1.95G/4.18G [00:18<00:15, 146MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  22% 1.07G/4.96G [00:19<00:34, 111MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:   0% 75.1k/4.99G [00:20<344:27:11, 4.03kB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  23% 1.14G/4.96G [00:20<00:42, 90.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  51% 2.15G/4.18G [00:21<00:19, 106MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  24% 1.21G/4.96G [00:22<00:45, 82.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:   3% 134M/4.99G [00:22<07:21, 11.0MB/s]     \u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  53% 2.21G/4.18G [00:22<00:19, 98.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  27% 1.34G/4.96G [00:22<00:33, 108MB/s] \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:   4% 201M/4.99G [00:22<04:24, 18.1MB/s]\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  55% 2.28G/4.18G [00:23<00:19, 96.5MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  28% 1.41G/4.96G [00:23<00:34, 102MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  56% 2.35G/4.18G [00:23<00:15, 116MB/s] \u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  31% 1.54G/4.96G [00:23<00:22, 151MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:   5% 268M/4.99G [00:23<03:10, 24.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:   6% 307M/4.99G [00:27<03:56, 19.8MB/s]\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  58% 2.42G/4.18G [00:27<00:37, 47.5MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:   7% 335M/4.99G [00:28<03:50, 20.2MB/s]\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  59% 2.48G/4.18G [00:28<00:30, 55.4MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:   8% 403M/4.99G [00:28<02:21, 32.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  33% 1.61G/4.96G [00:28<01:08, 48.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:   9% 470M/4.99G [00:30<02:29, 30.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  34% 1.68G/4.96G [00:31<01:22, 39.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  61% 2.55G/4.18G [00:31<00:39, 41.5MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  11% 531M/4.99G [00:31<01:47, 41.4MB/s]\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  63% 2.62G/4.18G [00:31<00:28, 54.4MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  37% 1.81G/4.96G [00:31<00:51, 60.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  39% 1.95G/4.96G [00:31<00:33, 89.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  67% 2.82G/4.18G [00:31<00:13, 100MB/s] \u001b[A\n",
            "model-00003-of-00003.safetensors:  71% 2.95G/4.18G [00:32<00:10, 113MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  12% 595M/4.99G [00:33<01:50, 40.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  41% 2.01G/4.96G [00:33<00:36, 80.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  74% 3.09G/4.18G [00:33<00:07, 152MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  42% 2.08G/4.96G [00:33<00:31, 90.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  75% 3.15G/4.18G [00:33<00:06, 154MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  45% 2.21G/4.96G [00:34<00:26, 103MB/s] \u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  77% 3.22G/4.18G [00:34<00:09, 105MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  46% 2.28G/4.96G [00:35<00:25, 106MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  79% 3.29G/4.18G [00:35<00:07, 113MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  13% 662M/4.99G [00:35<02:02, 35.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  47% 2.35G/4.96G [00:35<00:22, 118MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  15% 729M/4.99G [00:36<01:42, 41.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  49% 2.42G/4.96G [00:37<00:30, 82.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  80% 3.36G/4.18G [00:37<00:12, 64.7MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  51% 2.55G/4.96G [00:37<00:22, 106MB/s] \u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  82% 3.42G/4.18G [00:38<00:09, 81.0MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  53% 2.62G/4.96G [00:38<00:20, 113MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  15% 737M/4.99G [00:38<02:32, 28.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  54% 2.68G/4.96G [00:38<00:20, 109MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  84% 3.49G/4.18G [00:38<00:08, 77.9MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  16% 804M/4.99G [00:38<01:45, 39.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  55% 2.75G/4.96G [00:39<00:19, 116MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  85% 3.56G/4.18G [00:41<00:10, 56.5MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  87% 3.62G/4.18G [00:43<00:12, 43.7MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  57% 2.82G/4.96G [00:43<00:51, 42.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  87% 3.64G/4.18G [00:43<00:12, 43.8MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  90% 3.77G/4.18G [00:43<00:04, 87.0MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  58% 2.89G/4.96G [00:44<00:41, 49.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  92% 3.84G/4.18G [00:44<00:03, 89.7MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  64% 3.16G/4.96G [00:44<00:15, 113MB/s] \u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  94% 3.91G/4.18G [00:45<00:03, 88.2MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  65% 3.22G/4.96G [00:45<00:15, 109MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  95% 3.97G/4.18G [00:45<00:02, 97.2MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  97% 4.04G/4.18G [00:46<00:01, 124MB/s] \u001b[A\n",
            "model-00003-of-00003.safetensors:  98% 4.11G/4.18G [00:46<00:00, 140MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  66% 3.29G/4.96G [00:46<00:15, 107MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  17% 874M/4.99G [00:46<03:49, 17.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  68% 3.36G/4.96G [00:46<00:13, 121MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors: 100% 4.18G/4.18G [00:47<00:00, 114MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors: 100% 4.18G/4.18G [00:47<00:00, 88.4MB/s]\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  19% 965M/4.99G [00:47<02:28, 27.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  70% 3.49G/4.96G [00:47<00:11, 123MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  72% 3.56G/4.96G [00:47<00:09, 145MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  74% 3.69G/4.96G [00:48<00:06, 194MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  76% 3.76G/4.96G [00:49<00:08, 150MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  79% 3.89G/4.96G [00:49<00:06, 161MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  80% 3.96G/4.96G [00:50<00:08, 123MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  81% 4.03G/4.96G [00:51<00:08, 115MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  21% 1.04G/4.99G [00:52<03:00, 21.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  83% 4.09G/4.96G [00:52<00:08, 102MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  84% 4.16G/4.96G [00:52<00:06, 119MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  85% 4.23G/4.96G [00:53<00:05, 123MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  87% 4.29G/4.96G [00:54<00:06, 101MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  22% 1.08G/4.99G [00:54<03:12, 20.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  23% 1.15G/4.99G [00:56<02:32, 25.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  24% 1.21G/4.99G [00:57<02:02, 31.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  26% 1.28G/4.99G [00:57<01:33, 39.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  27% 1.35G/4.99G [00:57<01:06, 55.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  30% 1.48G/4.99G [00:58<00:35, 99.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  88% 4.36G/4.96G [00:58<00:14, 41.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  31% 1.55G/4.99G [00:58<00:29, 115MB/s] \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  89% 4.43G/4.96G [00:58<00:10, 52.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  34% 1.68G/4.99G [00:58<00:21, 157MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  91% 4.49G/4.96G [00:58<00:06, 66.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  35% 1.75G/4.99G [00:59<00:21, 151MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  92% 4.56G/4.96G [00:59<00:05, 69.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  36% 1.82G/4.99G [01:00<00:29, 108MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  93% 4.62G/4.96G [01:00<00:04, 74.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  38% 1.88G/4.99G [01:00<00:25, 124MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  40% 2.02G/4.99G [01:01<00:17, 169MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  95% 4.69G/4.96G [01:01<00:03, 78.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  97% 4.82G/4.96G [01:01<00:01, 122MB/s] \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  42% 2.09G/4.99G [01:01<00:21, 133MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  43% 2.15G/4.99G [01:02<00:25, 111MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  99% 4.89G/4.96G [01:03<00:00, 87.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors: 100% 4.96G/4.96G [01:03<00:00, 78.2MB/s]\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  44% 2.22G/4.99G [01:03<00:24, 113MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  46% 2.29G/4.99G [01:03<00:21, 129MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  47% 2.35G/4.99G [01:04<00:17, 149MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  48% 2.42G/4.99G [01:04<00:14, 174MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  50% 2.49G/4.99G [01:04<00:13, 189MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  51% 2.55G/4.99G [01:05<00:14, 167MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  53% 2.62G/4.99G [01:05<00:12, 196MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  54% 2.69G/4.99G [01:05<00:14, 159MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  55% 2.76G/4.99G [01:06<00:14, 155MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  57% 2.82G/4.99G [01:07<00:18, 115MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  58% 2.88G/4.99G [01:07<00:16, 125MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  60% 3.01G/4.99G [01:08<00:11, 176MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  64% 3.22G/4.99G [01:08<00:05, 301MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  66% 3.28G/4.99G [01:08<00:06, 281MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  67% 3.35G/4.99G [01:09<00:09, 179MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  68% 3.42G/4.99G [01:09<00:09, 165MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  70% 3.49G/4.99G [01:10<00:08, 186MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  71% 3.55G/4.99G [01:10<00:07, 188MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  73% 3.62G/4.99G [01:11<00:09, 138MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  74% 3.69G/4.99G [01:12<00:11, 115MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  75% 3.76G/4.99G [01:12<00:10, 122MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  77% 3.83G/4.99G [01:12<00:08, 146MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  78% 3.89G/4.99G [01:13<00:07, 145MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  79% 3.96G/4.99G [01:13<00:05, 173MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  81% 4.03G/4.99G [01:13<00:04, 199MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  82% 4.09G/4.99G [01:13<00:04, 225MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  83% 4.16G/4.99G [01:14<00:03, 245MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  85% 4.23G/4.99G [01:14<00:02, 299MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  86% 4.30G/4.99G [01:14<00:02, 339MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  88% 4.39G/4.99G [01:14<00:01, 368MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  89% 4.46G/4.99G [01:14<00:01, 370MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  91% 4.53G/4.99G [01:14<00:01, 396MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  92% 4.59G/4.99G [01:15<00:01, 389MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  93% 4.66G/4.99G [01:15<00:00, 386MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  95% 4.72G/4.99G [01:15<00:00, 387MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  96% 4.79G/4.99G [01:15<00:00, 393MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  97% 4.86G/4.99G [01:15<00:00, 390MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  99% 4.93G/4.99G [01:16<00:00, 393MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors: 100% 4.99G/4.99G [01:16<00:00, 65.5MB/s]\n",
            "Fetching 3 files: 100% 3/3 [01:16<00:00, 25.56s/it]\n",
            "Loading checkpoint shards: 100% 3/3 [00:04<00:00,  1.41s/it]\n",
            "generation_config.json: 100% 141/141 [00:00<00:00, 1.59MB/s]\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "\n",
            "==============================\n",
            "  Batch Size Stress Test (true batch forward)\n",
            "==============================\n",
            "\n",
            "--- Batch Size = 4 (1 forward pass with 4 images) ---\n",
            " ==> BS=4: latency=10.081s, throughput=275.38 tok/s, max mem=14.57 GB\n",
            "\n",
            "--- Batch Size = 8 (1 forward pass with 8 images) ---\n",
            " ==> BS=8: latency=10.559s, throughput=525.81 tok/s, max mem=15.98 GB\n",
            "\n",
            "--- Batch Size = 16 (1 forward pass with 16 images) ---\n",
            " ==> BS=16: latency=15.678s, throughput=708.26 tok/s, max mem=18.79 GB\n",
            "\n",
            "--- Batch Size = 32 (1 forward pass with 32 images) ---\n",
            "[ERROR] BS=32 failed: CUDA out of memory. Tried to allocate 400.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 203.38 MiB is free. Process 113921 has 21.96 GiB memory in use. Of the allocated memory 19.66 GiB is allocated by PyTorch, and 2.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n",
            "Flash Attention 2 Batch Results:\n",
            "  BS=4: Latency=10.081s, Throughput=275.38 tok/s, Memory=14.57 GB\n",
            "  BS=8: Latency=10.559s, Throughput=525.81 tok/s, Memory=15.98 GB\n",
            "  BS=16: Latency=15.678s, Throughput=708.26 tok/s, Memory=18.79 GB\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python test_kv_batch.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ed8pCHFDwlXy",
        "outputId": "63f89a1c-efca-4663-a218-e6ced9404baf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-29 19:44:04.630936: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-29 19:44:04.648911: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764445444.670993   30709 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764445444.677609   30709 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764445444.694920   30709 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764445444.694957   30709 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764445444.694960   30709 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764445444.694962   30709 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-29 19:44:04.699861: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\n",
            "Using quantile for Pruning:\n",
            "Loading sample image...\n",
            "Image size: (1000, 667)\n",
            "\n",
            "================================================================================\n",
            "Baseline KV_cache Batch Test for LLaVA\n",
            "================================================================================\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Loading checkpoint shards: 100% 3/3 [00:04<00:00,  1.40s/it]\n",
            "Model loaded.\n",
            "\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "\n",
            "==============================\n",
            "  Batch Size Stress Test (tensorwise)\n",
            "==============================\n",
            "\n",
            "--- Batch Size = 4 ---\n",
            " ==> BS=4, latency=7.666s, throughput=361.09 tok/s, max_mem=14.71 GB\n",
            "\n",
            "--- Batch Size = 8 ---\n",
            " ==> BS=8, latency=9.447s, throughput=586.00 tok/s, max_mem=16.24 GB\n",
            "\n",
            "--- Batch Size = 16 ---\n",
            " ==> BS=16, latency=13.069s, throughput=847.20 tok/s, max_mem=19.32 GB\n",
            "\n",
            "--- Batch Size = 32 ---\n",
            "[ERROR] BS=32 failed: CUDA out of memory. Tried to allocate 400.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 75.38 MiB is free. Process 441373 has 22.08 GiB memory in use. Of the allocated memory 19.63 GiB is allocated by PyTorch, and 2.23 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n",
            "Baseline KV Cache Batch Results:\n",
            "  BS=4: Latency=7.666s, Throughput=361.09 tok/s, Memory=14.71 GB\n",
            "  BS=8: Latency=9.447s, Throughput=586.00 tok/s, Memory=16.24 GB\n",
            "  BS=16: Latency=13.069s, Throughput=847.20 tok/s, Memory=19.32 GB\n",
            "\n",
            "\n",
            "Using Topk for Pruning:\n",
            "Loading sample image...\n",
            "Image size: (1000, 667)\n",
            "\n",
            "================================================================================\n",
            "Baseline KV_cache Batch Test for LLaVA\n",
            "================================================================================\n",
            "Loading checkpoint shards: 100% 3/3 [00:04<00:00,  1.41s/it]\n",
            "Model loaded.\n",
            "\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "\n",
            "==============================\n",
            "  Batch Size Stress Test (tensorwise)\n",
            "==============================\n",
            "\n",
            "--- Batch Size = 4 ---\n",
            " ==> BS=4, latency=7.601s, throughput=364.17 tok/s, max_mem=14.71 GB\n",
            "\n",
            "--- Batch Size = 8 ---\n",
            " ==> BS=8, latency=9.484s, throughput=583.71 tok/s, max_mem=16.24 GB\n",
            "\n",
            "--- Batch Size = 16 ---\n",
            " ==> BS=16, latency=13.097s, throughput=845.39 tok/s, max_mem=19.32 GB\n",
            "\n",
            "--- Batch Size = 32 ---\n",
            "[ERROR] BS=32 failed: CUDA out of memory. Tried to allocate 400.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 235.38 MiB is free. Process 441373 has 21.93 GiB memory in use. Of the allocated memory 19.63 GiB is allocated by PyTorch, and 2.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n",
            "Baseline KV Cache Batch Results:\n",
            "  BS=4: Latency=7.601s, Throughput=364.17 tok/s, Memory=14.71 GB\n",
            "  BS=8: Latency=9.484s, Throughput=583.71 tok/s, Memory=16.24 GB\n",
            "  BS=16: Latency=13.097s, Throughput=845.39 tok/s, Memory=19.32 GB\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python test_kv_prune_batch.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G98BTrrIxdAg",
        "outputId": "e3eae8c5-637e-49b0-81a1-5cc92ebbfd36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-29 19:48:00.423898: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-29 19:48:00.441177: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764445680.462307   31755 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764445680.468659   31755 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764445680.484706   31755 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764445680.484734   31755 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764445680.484737   31755 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764445680.484739   31755 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-29 19:48:00.489402: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\n",
            "Using quantile for Pruning:\n",
            "Loading sample image...\n",
            "Image size: (1000, 667)\n",
            "\n",
            "================================================================================\n",
            "Tensor-wise PRUNING Only KV_cache Batch Test for LLaVA\n",
            "================================================================================\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Loading checkpoint shards: 100% 3/3 [00:04<00:00,  1.39s/it]\n",
            "Model loaded.\n",
            "\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "\n",
            "==============================\n",
            "  Batch Size Stress Test (tensorwise)\n",
            "==============================\n",
            "\n",
            "--- Batch Size = 4 ---\n",
            " ==> BS=4, latency=7.511s, throughput=367.48 tok/s, max_mem=17.04 GB\n",
            "\n",
            "--- Batch Size = 8 ---\n",
            "[ERROR] BS=8 failed: quantile() input tensor is too large\n",
            "\n",
            "--- Batch Size = 16 ---\n",
            "[ERROR] BS=16 failed: CUDA out of memory. Tried to allocate 458.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 205.38 MiB is free. Process 455548 has 21.96 GiB memory in use. Of the allocated memory 19.47 GiB is allocated by PyTorch, and 2.26 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n",
            "--- Batch Size = 32 ---\n",
            "[ERROR] BS=32 failed: CUDA out of memory. Tried to allocate 400.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 75.38 MiB is free. Process 455548 has 22.08 GiB memory in use. Of the allocated memory 19.63 GiB is allocated by PyTorch, and 2.23 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n",
            "Tensor-wise Pruning KV Cache Batch Results:\n",
            "  BS=4: Latency=7.511s, Throughput=367.48 tok/s, Memory=17.04 GB\n",
            "\n",
            "\n",
            "Using Topk for Pruning:\n",
            "Loading sample image...\n",
            "Image size: (1000, 667)\n",
            "\n",
            "================================================================================\n",
            "Tensor-wise PRUNING Only KV_cache Batch Test for LLaVA\n",
            "================================================================================\n",
            "Loading checkpoint shards: 100% 3/3 [00:04<00:00,  1.40s/it]\n",
            "Model loaded.\n",
            "\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "\n",
            "==============================\n",
            "  Batch Size Stress Test (tensorwise)\n",
            "==============================\n",
            "\n",
            "--- Batch Size = 4 ---\n",
            " ==> BS=4, latency=7.487s, throughput=368.62 tok/s, max_mem=17.04 GB\n",
            "\n",
            "--- Batch Size = 8 ---\n",
            "[ERROR] BS=8 failed: CUDA out of memory. Tried to allocate 38.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 33.38 MiB is free. Process 455548 has 22.12 GiB memory in use. Of the allocated memory 19.16 GiB is allocated by PyTorch, and 2.73 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n",
            "--- Batch Size = 16 ---\n",
            "[ERROR] BS=16 failed: CUDA out of memory. Tried to allocate 138.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 51.38 MiB is free. Process 455548 has 22.11 GiB memory in use. Of the allocated memory 19.99 GiB is allocated by PyTorch, and 1.89 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n",
            "--- Batch Size = 32 ---\n",
            "[ERROR] BS=32 failed: CUDA out of memory. Tried to allocate 400.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 85.38 MiB is free. Process 455548 has 22.07 GiB memory in use. Of the allocated memory 19.63 GiB is allocated by PyTorch, and 2.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n",
            "Tensor-wise Pruning KV Cache Batch Results:\n",
            "  BS=4: Latency=7.487s, Throughput=368.62 tok/s, Memory=17.04 GB\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python test_kv_prune_quant_batch.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i6DnoWQxxux2",
        "outputId": "bf8fb0c0-a9ba-49a2-998f-67ba6af600ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-29 19:54:17.089615: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-29 19:54:17.107095: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764446057.128221   33395 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764446057.134711   33395 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764446057.151065   33395 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764446057.151098   33395 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764446057.151101   33395 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764446057.151104   33395 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-29 19:54:17.156102: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\n",
            "Using quantile for Pruning:\n",
            "Loading sample image...\n",
            "Image size: (1000, 667)\n",
            "\n",
            "================================================================================\n",
            "Tensor-wise KV Pruning + Quantization Batch Test for LLaVA\n",
            "================================================================================\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Loading checkpoint shards: 100% 3/3 [00:04<00:00,  1.40s/it]\n",
            "Model loaded.\n",
            "\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "\n",
            "==============================\n",
            "  Batch Size Stress Test (tensorwise)\n",
            "==============================\n",
            "\n",
            "--- Batch Size = 4 ---\n",
            " ==> BS=4, latency=7.599s, throughput=364.27 tok/s, max_mem=17.66 GB\n",
            "\n",
            "--- Batch Size = 8 ---\n",
            "[ERROR] BS=8 failed: quantile() input tensor is too large\n",
            "\n",
            "--- Batch Size = 16 ---\n",
            "[ERROR] BS=16 failed: CUDA out of memory. Tried to allocate 458.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 205.38 MiB is free. Process 478002 has 21.96 GiB memory in use. Of the allocated memory 19.47 GiB is allocated by PyTorch, and 2.26 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n",
            "--- Batch Size = 32 ---\n",
            "[ERROR] BS=32 failed: CUDA out of memory. Tried to allocate 400.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 75.38 MiB is free. Process 478002 has 22.08 GiB memory in use. Of the allocated memory 19.63 GiB is allocated by PyTorch, and 2.23 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n",
            "Tensor-wise Prune + Quant KV Batch Results:\n",
            "  BS=4: Latency=7.599s, Throughput=364.27 tok/s, Memory=17.66 GB\n",
            "\n",
            "\n",
            "Using Topk for Pruning:\n",
            "Loading sample image...\n",
            "Image size: (1000, 667)\n",
            "\n",
            "================================================================================\n",
            "Tensor-wise KV Pruning + Quantization Batch Test for LLaVA\n",
            "================================================================================\n",
            "Loading checkpoint shards: 100% 3/3 [00:04<00:00,  1.40s/it]\n",
            "Model loaded.\n",
            "\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "\n",
            "==============================\n",
            "  Batch Size Stress Test (tensorwise)\n",
            "==============================\n",
            "\n",
            "--- Batch Size = 4 ---\n",
            " ==> BS=4, latency=7.618s, throughput=363.34 tok/s, max_mem=17.66 GB\n",
            "\n",
            "--- Batch Size = 8 ---\n",
            "[ERROR] BS=8 failed: CUDA out of memory. Tried to allocate 38.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 29.38 MiB is free. Process 478002 has 22.13 GiB memory in use. Of the allocated memory 20.25 GiB is allocated by PyTorch, and 1.65 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n",
            "--- Batch Size = 16 ---\n",
            "[ERROR] BS=16 failed: CUDA out of memory. Tried to allocate 138.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 51.38 MiB is free. Process 478002 has 22.11 GiB memory in use. Of the allocated memory 19.99 GiB is allocated by PyTorch, and 1.89 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n",
            "--- Batch Size = 32 ---\n",
            "[ERROR] BS=32 failed: CUDA out of memory. Tried to allocate 400.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 85.38 MiB is free. Process 478002 has 22.07 GiB memory in use. Of the allocated memory 19.63 GiB is allocated by PyTorch, and 2.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n",
            "Tensor-wise Prune + Quant KV Batch Results:\n",
            "  BS=4: Latency=7.618s, Throughput=363.34 tok/s, Memory=17.66 GB\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python test_kv_quant_batch.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRQqn2xh_EgX",
        "outputId": "37680386-884c-4a1a-8515-353907982692"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-29 19:56:57.273263: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-29 19:56:57.290730: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764446217.311830   34117 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764446217.318255   34117 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764446217.334522   34117 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764446217.334551   34117 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764446217.334554   34117 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764446217.334556   34117 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-29 19:56:57.339383: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\n",
            "Using quantile for Pruning:\n",
            "Loading sample image...\n",
            "Image size: (1000, 667)\n",
            "\n",
            "================================================================================\n",
            "Tensor-wise Quantization-Only KV_cache Batch Test for LLaVA\n",
            "================================================================================\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Loading checkpoint shards: 100% 3/3 [00:04<00:00,  1.40s/it]\n",
            "Model loaded.\n",
            "\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "\n",
            "==============================\n",
            "  Batch Size Stress Test (tensorwise)\n",
            "==============================\n",
            "\n",
            "--- Batch Size = 4 ---\n",
            " ==> BS=4, latency=7.739s, throughput=357.67 tok/s, max_mem=17.65 GB\n",
            "\n",
            "--- Batch Size = 8 ---\n",
            "[ERROR] BS=8 failed: CUDA out of memory. Tried to allocate 38.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 33.38 MiB is free. Process 488225 has 22.12 GiB memory in use. Of the allocated memory 21.59 GiB is allocated by PyTorch, and 309.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n",
            "--- Batch Size = 16 ---\n",
            "[ERROR] BS=16 failed: CUDA out of memory. Tried to allocate 150.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 57.38 MiB is free. Process 488225 has 22.10 GiB memory in use. Of the allocated memory 20.92 GiB is allocated by PyTorch, and 978.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n",
            "--- Batch Size = 32 ---\n",
            "[ERROR] BS=32 failed: CUDA out of memory. Tried to allocate 400.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 225.38 MiB is free. Process 488225 has 21.94 GiB memory in use. Of the allocated memory 19.63 GiB is allocated by PyTorch, and 2.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n",
            "Tensor-wise KV Quantization Batch Results:\n",
            "  BS=4: Latency=7.739s, Throughput=357.67 tok/s, Memory=17.65 GB\n",
            "\n",
            "\n",
            "Using Topk for Pruning:\n",
            "Loading sample image...\n",
            "Image size: (1000, 667)\n",
            "\n",
            "================================================================================\n",
            "Tensor-wise Quantization-Only KV_cache Batch Test for LLaVA\n",
            "================================================================================\n",
            "Loading checkpoint shards: 100% 3/3 [00:04<00:00,  1.41s/it]\n",
            "Model loaded.\n",
            "\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "\n",
            "==============================\n",
            "  Batch Size Stress Test (tensorwise)\n",
            "==============================\n",
            "\n",
            "--- Batch Size = 4 ---\n",
            " ==> BS=4, latency=7.666s, throughput=361.06 tok/s, max_mem=17.65 GB\n",
            "\n",
            "--- Batch Size = 8 ---\n",
            "[ERROR] BS=8 failed: CUDA out of memory. Tried to allocate 38.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 5.38 MiB is free. Process 488225 has 22.15 GiB memory in use. Of the allocated memory 21.63 GiB is allocated by PyTorch, and 299.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n",
            "--- Batch Size = 16 ---\n",
            "[ERROR] BS=16 failed: CUDA out of memory. Tried to allocate 150.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 67.38 MiB is free. Process 488225 has 22.09 GiB memory in use. Of the allocated memory 20.92 GiB is allocated by PyTorch, and 967.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n",
            "--- Batch Size = 32 ---\n",
            "[ERROR] BS=32 failed: CUDA out of memory. Tried to allocate 400.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 235.38 MiB is free. Process 488225 has 21.93 GiB memory in use. Of the allocated memory 19.63 GiB is allocated by PyTorch, and 2.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n",
            "Tensor-wise KV Quantization Batch Results:\n",
            "  BS=4: Latency=7.666s, Throughput=361.06 tok/s, Memory=17.65 GB\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python int8_tensor_prune_quant_batch.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m9otAnG4An_M",
        "outputId": "fe54c7ac-69bd-46dc-c738-ea74b380dbb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-29 19:58:21.044983: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-29 19:58:21.062224: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764446301.083540   34525 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764446301.089912   34525 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764446301.105898   34525 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764446301.105925   34525 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764446301.105928   34525 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764446301.105931   34525 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-29 19:58:21.110621: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\n",
            "Using quantile for Pruning:\n",
            "Loading sample image...\n",
            "Image size: (1000, 667)\n",
            "\n",
            "================================================================================\n",
            "Method 1: INT8 + Tensor-wise KV Pruning + KV Quantization (Batch Test)\n",
            "================================================================================\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "Loading checkpoint shards: 100% 3/3 [00:16<00:00,  5.51s/it]\n",
            "INT8 model loaded (bitsandbytes).\n",
            "\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "\n",
            "==============================\n",
            "  Batch Size Stress Test (tensorwise)\n",
            "==============================\n",
            "\n",
            "--- Batch Size = 4 ---\n",
            " ==> BS=4, latency=2.308s, throughput=1048.53 tok/s, max_mem=11.16 GB\n",
            "\n",
            "--- Batch Size = 8 ---\n",
            "[ERROR] BS=8 failed: quantile() input tensor is too large\n",
            "\n",
            "--- Batch Size = 16 ---\n",
            "[ERROR] BS=16 failed: quantile() input tensor is too large\n",
            "\n",
            "--- Batch Size = 32 ---\n",
            "[ERROR] BS=32 failed: CUDA out of memory. Tried to allocate 594.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 449.38 MiB is free. Process 493634 has 21.72 GiB memory in use. Of the allocated memory 18.31 GiB is allocated by PyTorch, and 3.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n",
            "Method 1 Batch Results (INT8 + Tensor-wise Prune + KV Quant):\n",
            "  BS=4: Latency=2.308s, Throughput=1048.53 tok/s, Memory=11.16 GB\n",
            "\n",
            "\n",
            "Using Topk for Pruning:\n",
            "Loading sample image...\n",
            "Image size: (1000, 667)\n",
            "\n",
            "================================================================================\n",
            "Method 1: INT8 + Tensor-wise KV Pruning + KV Quantization (Batch Test)\n",
            "================================================================================\n",
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "Loading checkpoint shards: 100% 3/3 [00:16<00:00,  5.47s/it]\n",
            "INT8 model loaded (bitsandbytes).\n",
            "\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "\n",
            "==============================\n",
            "  Batch Size Stress Test (tensorwise)\n",
            "==============================\n",
            "\n",
            "--- Batch Size = 4 ---\n",
            " ==> BS=4, latency=2.286s, throughput=1058.70 tok/s, max_mem=11.16 GB\n",
            "\n",
            "--- Batch Size = 8 ---\n",
            " ==> BS=8, latency=11.707s, throughput=445.54 tok/s, max_mem=15.64 GB\n",
            "\n",
            "--- Batch Size = 16 ---\n",
            "[ERROR] BS=16 failed: CUDA out of memory. Tried to allocate 76.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 71.38 MiB is free. Process 493634 has 22.09 GiB memory in use. Of the allocated memory 19.60 GiB is allocated by PyTorch, and 2.25 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n",
            "--- Batch Size = 32 ---\n",
            "[ERROR] BS=32 failed: CUDA out of memory. Tried to allocate 276.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 159.38 MiB is free. Process 493634 has 22.00 GiB memory in use. Of the allocated memory 19.35 GiB is allocated by PyTorch, and 2.41 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n",
            "Method 1 Batch Results (INT8 + Tensor-wise Prune + KV Quant):\n",
            "  BS=4: Latency=2.286s, Throughput=1058.70 tok/s, Memory=11.16 GB\n",
            "  BS=8: Latency=11.707s, Throughput=445.54 tok/s, Memory=15.64 GB\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python int4_tensor_prune_quant_batch.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dO0cJsJcBIGb",
        "outputId": "e8cb127c-08b5-4302-9ab7-0833140306d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-29 20:01:43.438314: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-29 20:01:43.455898: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764446503.477069   35498 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764446503.483449   35498 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764446503.499772   35498 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764446503.499800   35498 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764446503.499803   35498 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764446503.499805   35498 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-29 20:01:43.504602: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\n",
            "Using quantile for Pruning:\n",
            "================================================================================\n",
            "Method 1: INT4 + Tensor-wise KV Pruning + KV Quantization (Batch Test)\n",
            "================================================================================\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "Loading checkpoint shards: 100% 3/3 [00:16<00:00,  5.51s/it]\n",
            "INT4 model loaded (bitsandbytes).\n",
            "\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "\n",
            "==============================\n",
            "  Batch Size Stress Test (tensorwise)\n",
            "==============================\n",
            "\n",
            "--- Batch Size = 4 ---\n",
            " ==> BS=4, latency=35.215s, throughput=78.60 tok/s, max_mem=8.79 GB\n",
            "\n",
            "--- Batch Size = 8 ---\n",
            "[ERROR] BS=8 failed: quantile() input tensor is too large\n",
            "\n",
            "--- Batch Size = 16 ---\n",
            "[ERROR] BS=16 failed: quantile() input tensor is too large\n",
            "\n",
            "--- Batch Size = 32 ---\n",
            "[ERROR] BS=32 failed: quantile() input tensor is too large\n",
            "\n",
            "Method 1 Batch Results (INT4 + Tensor-wise Prune + KV Quant):\n",
            "  BS=4: Latency=35.215s, Throughput=78.60 tok/s, Memory=8.79 GB\n",
            "\n",
            "\n",
            "Using Topk for Pruning:\n",
            "================================================================================\n",
            "Method 1: INT4 + Tensor-wise KV Pruning + KV Quantization (Batch Test)\n",
            "================================================================================\n",
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "Loading checkpoint shards: 100% 3/3 [00:16<00:00,  5.63s/it]\n",
            "INT4 model loaded (bitsandbytes).\n",
            "\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "\n",
            "==============================\n",
            "  Batch Size Stress Test (tensorwise)\n",
            "==============================\n",
            "\n",
            "--- Batch Size = 4 ---\n",
            " ==> BS=4, latency=35.214s, throughput=78.60 tok/s, max_mem=8.78 GB\n",
            "\n",
            "--- Batch Size = 8 ---\n",
            " ==> BS=8, latency=38.420s, throughput=144.09 tok/s, max_mem=13.24 GB\n",
            "\n",
            "--- Batch Size = 16 ---\n",
            "[ERROR] BS=16 failed: CUDA out of memory. Tried to allocate 76.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 71.38 MiB is free. Process 508161 has 22.09 GiB memory in use. Of the allocated memory 19.94 GiB is allocated by PyTorch, and 1.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n",
            "--- Batch Size = 32 ---\n",
            "[ERROR] BS=32 failed: CUDA out of memory. Tried to allocate 276.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 35.38 MiB is free. Process 508161 has 22.12 GiB memory in use. Of the allocated memory 19.18 GiB is allocated by PyTorch, and 2.72 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n",
            "Method 1 Batch Results (INT4 + Tensor-wise Prune + KV Quant):\n",
            "  BS=4: Latency=35.214s, Throughput=78.60 tok/s, Memory=8.78 GB\n",
            "  BS=8: Latency=38.420s, Throughput=144.09 tok/s, Memory=13.24 GB\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python int4_channel_prune_quant_batch.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mXPS6YqdBrP4",
        "outputId": "5d6e506c-25b4-485a-fb63-04d2b904b419"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-29 20:21:33.255872: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-29 20:21:33.273460: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764447693.295113   40954 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764447693.301517   40954 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764447693.317776   40954 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764447693.317807   40954 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764447693.317811   40954 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764447693.317813   40954 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-29 20:21:33.322579: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\n",
            "Using unstructure for Pruning:\n",
            "================================================================================\n",
            "Method 2: INT4 + Channel-wise KV Pruning + KV Quantization (Batch Test)\n",
            "================================================================================\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "Loading checkpoint shards: 100% 3/3 [00:16<00:00,  5.57s/it]\n",
            "INT4 model loaded (bitsandbytes).\n",
            "\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "\n",
            "==============================\n",
            "  Batch Size Stress Test (channelwise)\n",
            "==============================\n",
            "\n",
            "--- Batch Size = 4 ---\n",
            " ==> BS=4, latency=20.359s, throughput=127.71 tok/s, max_mem=8.71 GB\n",
            "\n",
            "--- Batch Size = 8 ---\n",
            " ==> BS=8, latency=22.041s, throughput=235.92 tok/s, max_mem=13.10 GB\n",
            "\n",
            "--- Batch Size = 16 ---\n",
            "[ERROR] BS=16 failed: CUDA out of memory. Tried to allocate 86.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 15.38 MiB is free. Process 594687 has 22.14 GiB memory in use. Of the allocated memory 21.10 GiB is allocated by PyTorch, and 831.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n",
            "--- Batch Size = 32 ---\n",
            "[ERROR] BS=32 failed: CUDA out of memory. Tried to allocate 298.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 91.38 MiB is free. Process 594687 has 22.07 GiB memory in use. Of the allocated memory 20.16 GiB is allocated by PyTorch, and 1.67 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n",
            "Method 2 Batch Results (INT4 + Channel-wise Prune + KV Quant):\n",
            "  BS=4: Latency=20.359s, Throughput=127.71 tok/s, Memory=8.71 GB\n",
            "  BS=8: Latency=22.041s, Throughput=235.92 tok/s, Memory=13.10 GB\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python int8_channel_prune_quant_batch.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LEvKrie0E-e2",
        "outputId": "1412c7ea-ee3d-47bf-f3b5-4ec0f20bf19a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-29 20:23:58.864033: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-29 20:23:58.881592: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764447838.902902   41659 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764447838.909355   41659 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764447838.925952   41659 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764447838.925984   41659 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764447838.925987   41659 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764447838.925990   41659 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-29 20:23:58.930898: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\n",
            "Using unstructure for Pruning:\n",
            "================================================================================\n",
            "Method 2: INT8 + Channel-wise KV Pruning + KV Quantization (Batch Test)\n",
            "================================================================================\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "Loading checkpoint shards: 100% 3/3 [00:16<00:00,  5.63s/it]\n",
            "INT8 model loaded (bitsandbytes).\n",
            "\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "\n",
            "==============================\n",
            "  Batch Size Stress Test (channelwise)\n",
            "==============================\n",
            "\n",
            "--- Batch Size = 4 ---\n",
            " ==> BS=4, latency=17.437s, throughput=158.74 tok/s, max_mem=11.36 GB\n",
            "\n",
            "--- Batch Size = 8 ---\n",
            " ==> BS=8, latency=19.093s, throughput=289.94 tok/s, max_mem=15.86 GB\n",
            "\n",
            "--- Batch Size = 16 ---\n",
            "[ERROR] BS=16 failed: CUDA out of memory. Tried to allocate 76.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 27.38 MiB is free. Process 604800 has 22.13 GiB memory in use. Of the allocated memory 20.84 GiB is allocated by PyTorch, and 1.06 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n",
            "--- Batch Size = 32 ---\n",
            "[ERROR] BS=32 failed: CUDA out of memory. Tried to allocate 298.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 157.38 MiB is free. Process 604800 has 22.00 GiB memory in use. Of the allocated memory 19.17 GiB is allocated by PyTorch, and 2.59 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n",
            "Method 2 Batch Results (INT8 + Channel-wise Prune + KV Quant):\n",
            "  BS=4: Latency=17.437s, Throughput=158.74 tok/s, Memory=11.36 GB\n",
            "  BS=8: Latency=19.093s, Throughput=289.94 tok/s, Memory=15.86 GB\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python channel_prune_quant_batch.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UEiVgKw3FtwW",
        "outputId": "cb6121e9-027f-4e56-a0fd-468d32835b60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-29 20:25:37.708798: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-29 20:25:37.727046: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764447937.750537   42178 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764447937.757233   42178 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764447937.773945   42178 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764447937.773976   42178 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764447937.773979   42178 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764447937.773982   42178 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-29 20:25:37.778858: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\n",
            "Using unstructure for Pruning:\n",
            "Loading sample image...\n",
            "Image size: (1000, 667)\n",
            "\n",
            "================================================================================\n",
            "Method 2: Channel-wise KV Pruning + KV Quantization (Batch Test)\n",
            "================================================================================\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Loading checkpoint shards: 100% 3/3 [00:04<00:00,  1.43s/it]\n",
            "LLava fp16 model loaded.\n",
            "\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "\n",
            "==============================\n",
            "  Batch Size Stress Test (channelwise)\n",
            "==============================\n",
            "\n",
            "--- Batch Size = 4 ---\n",
            " ==> BS=4, latency=7.660s, throughput=361.35 tok/s, max_mem=17.67 GB\n",
            "\n",
            "--- Batch Size = 8 ---\n",
            "[ERROR] BS=8 failed: CUDA out of memory. Tried to allocate 38.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 33.38 MiB is free. Process 612402 has 22.12 GiB memory in use. Of the allocated memory 21.60 GiB is allocated by PyTorch, and 303.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n",
            "--- Batch Size = 16 ---\n",
            "[ERROR] BS=16 failed: CUDA out of memory. Tried to allocate 150.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 125.38 MiB is free. Process 612402 has 22.03 GiB memory in use. Of the allocated memory 20.05 GiB is allocated by PyTorch, and 1.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n",
            "--- Batch Size = 32 ---\n",
            "[ERROR] BS=32 failed: CUDA out of memory. Tried to allocate 400.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 75.38 MiB is free. Process 612402 has 22.08 GiB memory in use. Of the allocated memory 19.63 GiB is allocated by PyTorch, and 2.23 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n",
            "Method 2 Batch Results (Channel-wise Prune + KV Quant):\n",
            "  BS=4: Latency=7.660s, Throughput=361.35 tok/s, Memory=17.67 GB\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python test_bitsandbytes_batch.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lK48zSaRS74Q",
        "outputId": "389d43cf-b97a-4325-9457-3a50be4eac77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-29 20:36:02.108930: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-29 20:36:02.127078: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764448562.149085   45122 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764448562.155670   45122 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764448562.172461   45122 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764448562.172516   45122 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764448562.172520   45122 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764448562.172523   45122 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-29 20:36:02.177401: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Loading sample image...\n",
            "Image size: (1000, 667)\n",
            "\n",
            "================================================================================\n",
            "8bit batch Test for LLaVA\n",
            "================================================================================\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "Loading checkpoint shards: 100% 3/3 [00:16<00:00,  5.64s/it]\n",
            "Model loaded.\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "\n",
            "==============================\n",
            "  Batch Size Stress Test (true batch forward)\n",
            "==============================\n",
            "\n",
            "--- Batch Size = 4 (1 forward pass with 4 images) ---\n",
            " ==> BS=4: latency=17.827s, throughput=155.72 tok/s, max mem=8.32 GB\n",
            "\n",
            "--- Batch Size = 8 (1 forward pass with 8 images) ---\n",
            " ==> BS=8: latency=20.289s, throughput=273.64 tok/s, max mem=9.78 GB\n",
            "\n",
            "--- Batch Size = 16 (1 forward pass with 16 images) ---\n",
            " ==> BS=16: latency=25.494s, throughput=435.56 tok/s, max mem=12.71 GB\n",
            "\n",
            "--- Batch Size = 32 (1 forward pass with 32 images) ---\n",
            " ==> BS=32: latency=36.260s, throughput=612.47 tok/s, max mem=18.56 GB\n",
            "\"8bit Batch Results:\n",
            "  BS=4: Latency=17.827s, Throughput=155.72 tok/s, Memory=8.32 GB\n",
            "  BS=8: Latency=20.289s, Throughput=273.64 tok/s, Memory=9.78 GB\n",
            "  BS=16: Latency=25.494s, Throughput=435.56 tok/s, Memory=12.71 GB\n",
            "  BS=32: Latency=36.260s, Throughput=612.47 tok/s, Memory=18.56 GB\n"
          ]
        }
      ]
    }
  ]
}