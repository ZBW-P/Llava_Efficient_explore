{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "runtime_attributes": {
        "runtime_version": "2025.10"
      }
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers accelerate pillow torch\n",
        "!pip install -q flash-attn --no-build-isolation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCg-P6EKcHuQ",
        "outputId": "ba33b143-75b5-4343-f4d8-5bb9a52ab4c2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/8.4 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m7.7/8.4 MB\u001b[0m \u001b[31m230.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m139.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for flash-attn (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llmcompressor datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I6VrjHIVYFyZ",
        "outputId": "37ddceba-041f-44ff-bc32-238075a62e4c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llmcompressor\n",
            "  Downloading llmcompressor-0.8.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Collecting loguru<=0.7.3,>=0.7.2 (from llmcompressor)\n",
            "  Downloading loguru-0.7.3-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: pyyaml<=6.0.3,>=6.0.1 in /usr/local/lib/python3.12/dist-packages (from llmcompressor) (6.0.3)\n",
            "Requirement already satisfied: numpy<=2.3.3,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from llmcompressor) (2.0.2)\n",
            "Requirement already satisfied: requests<=2.32.5,>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from llmcompressor) (2.32.4)\n",
            "Requirement already satisfied: tqdm<=4.67.1,>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from llmcompressor) (4.67.1)\n",
            "Requirement already satisfied: torch<=2.8.0,>=2.7.0 in /usr/local/lib/python3.12/dist-packages (from llmcompressor) (2.8.0+cu126)\n",
            "Collecting transformers<=4.56.2,>=4.53.0 (from llmcompressor)\n",
            "  Downloading transformers-4.56.2-py3-none-any.whl.metadata (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.1/40.1 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: accelerate<=1.10.1,>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from llmcompressor) (1.10.1)\n",
            "Requirement already satisfied: nvidia-ml-py<=13.580.82,>=12.560.30 in /usr/local/lib/python3.12/dist-packages (from llmcompressor) (12.575.51)\n",
            "Requirement already satisfied: pillow<=11.3.0,>=10.4.0 in /usr/local/lib/python3.12/dist-packages (from llmcompressor) (11.3.0)\n",
            "Collecting compressed-tensors==0.12.2 (from llmcompressor)\n",
            "  Downloading compressed_tensors-0.12.2-py3-none-any.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.12/dist-packages (from compressed-tensors==0.12.2->llmcompressor) (2.11.10)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.35.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate<=1.10.1,>=1.6.0->llmcompressor) (5.9.5)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate<=1.10.1,>=1.6.0->llmcompressor) (0.6.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.10)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<=2.32.5,>=2.32.2->llmcompressor) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<=2.32.5,>=2.32.2->llmcompressor) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<=2.32.5,>=2.32.2->llmcompressor) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<=2.32.5,>=2.32.2->llmcompressor) (2025.10.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (3.4.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<=4.56.2,>=4.53.0->llmcompressor) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<=4.56.2,>=4.53.0->llmcompressor) (0.22.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0->compressed-tensors==0.12.2->llmcompressor) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0->compressed-tensors==0.12.2->llmcompressor) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0->compressed-tensors==0.12.2->llmcompressor) (0.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<=2.8.0,>=2.7.0->llmcompressor) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<=2.8.0,>=2.7.0->llmcompressor) (3.0.3)\n",
            "Downloading llmcompressor-0.8.1-py3-none-any.whl (273 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m273.2/273.2 kB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading compressed_tensors-0.12.2-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.0/183.0 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading loguru-0.7.3-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformers-4.56.2-py3-none-any.whl (11.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m172.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: loguru, transformers, compressed-tensors, llmcompressor\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.57.0\n",
            "    Uninstalling transformers-4.57.0:\n",
            "      Successfully uninstalled transformers-4.57.0\n",
            "Successfully installed compressed-tensors-0.12.2 llmcompressor-0.8.1 loguru-0.7.3 transformers-4.56.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U bitsandbytes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_tuG642YIRs",
        "outputId": "c42556ff-a70a-4e30-eb31-d45b08ecb512"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: torch<3,>=2.3 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.8.0+cu126)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (25.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.3->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.3)\n",
            "Downloading bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl (59.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bitsandbytes\n",
            "Successfully installed bitsandbytes-0.48.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6RPnK-KkbI1D",
        "outputId": "145898a3-6202-4e76-9a8c-329b8ca45c3c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.12.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python test_baseline.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZV_YISLiWAr0",
        "outputId": "c67870cf-058b-459e-d84f-42f779d74fbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-28 04:45:36.594803: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-28 04:45:36.611840: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764305136.633080    1764 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764305136.639397    1764 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764305136.655493    1764 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764305136.655518    1764 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764305136.655521    1764 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764305136.655524    1764 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-28 04:45:36.660195: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Loading sample image...\n",
            "Image size: (1000, 667)\n",
            "\n",
            "================================================================================\n",
            "Baseline_with_kv_cache Test for LLaVA\n",
            "================================================================================\n",
            "processor_config.json: 100% 173/173 [00:00<00:00, 1.66MB/s]\n",
            "chat_template.json: 100% 701/701 [00:00<00:00, 7.28MB/s]\n",
            "chat_template.jinja: 100% 674/674 [00:00<00:00, 7.18MB/s]\n",
            "preprocessor_config.json: 100% 505/505 [00:00<00:00, 5.76MB/s]\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "tokenizer_config.json: 1.45kB [00:00, 7.58MB/s]\n",
            "tokenizer.model: 100% 500k/500k [00:01<00:00, 285kB/s]\n",
            "tokenizer.json: 3.62MB [00:00, 185MB/s]\n",
            "added_tokens.json: 100% 41.0/41.0 [00:00<00:00, 520kB/s]\n",
            "special_tokens_map.json: 100% 552/552 [00:00<00:00, 5.42MB/s]\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "config.json: 100% 950/950 [00:00<00:00, 11.1MB/s]\n",
            "model.safetensors.index.json: 70.1kB [00:00, 196MB/s]\n",
            "Fetching 3 files:   0% 0/3 [00:00<?, ?it/s]\n",
            "model-00001-of-00003.safetensors:   0% 0.00/4.99G [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:   0% 0.00/4.96G [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:   0% 0.00/4.18G [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:   0% 24.6k/4.99G [00:01<67:13:45, 20.6kB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:   0% 599k/4.96G [00:01<4:04:17, 338kB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:   0% 119k/4.99G [00:01<18:51:07, 73.6kB/s] \u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:   0% 546k/4.18G [00:02<4:21:57, 266kB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:   5% 239M/4.99G [00:02<00:24, 191MB/s]    \u001b[A\n",
            "model-00001-of-00003.safetensors:   6% 306M/4.99G [00:04<01:08, 68.1MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:   8% 412M/4.99G [00:04<00:44, 102MB/s] \u001b[A\n",
            "model-00001-of-00003.safetensors:  14% 677M/4.99G [00:05<00:21, 201MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:   1% 67.7M/4.96G [00:05<05:55, 13.8MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  16% 811M/4.99G [00:05<00:20, 204MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:   3% 135M/4.96G [00:08<04:41, 17.2MB/s] \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:   2% 67.6M/4.18G [00:12<12:27, 5.50MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:   3% 135M/4.18G [00:13<05:08, 13.1MB/s] \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:   5% 269M/4.96G [00:13<03:23, 23.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:   6% 269M/4.18G [00:13<01:57, 33.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:   8% 403M/4.96G [00:13<01:46, 42.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:   8% 336M/4.18G [00:13<01:21, 47.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:   9% 470M/4.96G [00:13<01:22, 54.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  13% 537M/4.18G [00:13<00:36, 101MB/s] \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  14% 604M/4.18G [00:14<00:34, 103MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  12% 604M/4.96G [00:14<00:57, 75.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  18% 738M/4.18G [00:14<00:25, 134MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  18% 880M/4.99G [00:14<01:52, 36.6MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  19% 805M/4.18G [00:15<00:24, 140MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  19% 941M/4.99G [00:15<01:35, 42.6MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  15% 738M/4.96G [00:15<00:44, 94.2MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  20% 1.01G/4.99G [00:16<01:33, 42.8MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  16% 805M/4.96G [00:19<01:26, 48.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  22% 940M/4.18G [00:19<00:51, 63.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  22% 1.08G/4.99G [00:19<01:45, 37.3MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  19% 940M/4.96G [00:19<00:53, 75.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  24% 1.01G/4.18G [00:19<00:40, 77.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  20% 1.01G/4.96G [00:19<00:42, 92.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  27% 1.14G/4.18G [00:19<00:25, 118MB/s] \u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  23% 1.14G/4.99G [00:19<01:25, 45.2MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  22% 1.07G/4.96G [00:19<00:38, 100MB/s] \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  29% 1.21G/4.18G [00:20<00:26, 111MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  23% 1.14G/4.96G [00:20<00:39, 96.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  32% 1.34G/4.18G [00:20<00:19, 145MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  34% 1.41G/4.18G [00:21<00:21, 127MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  26% 1.27G/4.96G [00:21<00:32, 113MB/s] \u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  26% 1.28G/4.99G [00:21<01:10, 53.0MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  27% 1.34G/4.96G [00:22<00:31, 115MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  27% 1.34G/4.99G [00:25<01:34, 38.5MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  30% 1.48G/4.99G [00:26<01:09, 50.6MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  28% 1.41G/4.96G [00:26<01:21, 43.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  31% 1.54G/4.96G [00:27<00:52, 65.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  35% 1.48G/4.18G [00:27<01:09, 38.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  39% 1.61G/4.18G [00:27<00:44, 57.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  34% 1.68G/4.96G [00:28<00:44, 73.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  37% 1.81G/4.96G [00:30<00:39, 79.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  43% 1.81G/4.18G [00:30<00:34, 68.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  31% 1.54G/4.99G [00:30<01:37, 35.2MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  38% 1.88G/4.96G [00:30<00:34, 88.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  45% 1.88G/4.18G [00:31<00:33, 69.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  33% 1.67G/4.99G [00:31<01:05, 50.7MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  35% 1.74G/4.99G [00:32<01:04, 50.9MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  39% 1.94G/4.99G [00:35<00:53, 57.5MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  39% 1.95G/4.96G [00:35<01:16, 39.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  48% 2.01G/4.18G [00:35<00:45, 47.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  51% 2.15G/4.18G [00:35<00:29, 68.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  42% 2.08G/4.99G [00:35<00:36, 79.2MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  42% 2.08G/4.96G [00:35<00:47, 61.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  45% 2.21G/4.96G [00:36<00:32, 83.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  53% 2.21G/4.18G [00:36<00:26, 73.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  43% 2.15G/4.99G [00:39<00:56, 50.3MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  46% 2.28G/4.96G [00:40<00:55, 48.1MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  44% 2.21G/4.99G [00:40<00:50, 55.0MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  56% 2.35G/4.18G [00:40<00:34, 53.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  48% 2.42G/4.99G [00:40<00:25, 101MB/s] \u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  47% 2.35G/4.96G [00:40<00:44, 59.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  58% 2.42G/4.18G [00:41<00:35, 50.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  50% 2.48G/4.99G [00:42<00:35, 71.6MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  49% 2.42G/4.96G [00:42<00:52, 48.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  61% 2.55G/4.18G [00:42<00:23, 68.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  50% 2.48G/4.96G [00:43<00:49, 50.5MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  51% 2.55G/4.99G [00:44<00:39, 61.2MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  51% 2.55G/4.96G [00:44<00:39, 60.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  63% 2.62G/4.18G [00:44<00:26, 59.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  54% 2.68G/4.99G [00:44<00:24, 93.4MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  66% 2.75G/4.18G [00:44<00:15, 89.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  56% 2.78G/4.99G [00:44<00:20, 110MB/s] \u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  53% 2.62G/4.96G [00:45<00:36, 64.1MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  57% 2.85G/4.99G [00:45<00:18, 117MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  67% 2.82G/4.18G [00:45<00:16, 80.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  54% 2.68G/4.96G [00:45<00:34, 66.4MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  58% 2.92G/4.99G [00:46<00:20, 99.0MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  57% 2.82G/4.96G [00:47<00:29, 72.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  71% 2.95G/4.18G [00:51<00:28, 43.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  74% 3.09G/4.18G [00:51<00:18, 57.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  60% 2.98G/4.99G [00:52<00:58, 34.3MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  60% 2.95G/4.96G [00:52<00:44, 45.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  77% 3.22G/4.18G [00:52<00:12, 78.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  61% 3.05G/4.99G [00:52<00:45, 42.8MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  64% 3.18G/4.99G [00:52<00:25, 71.9MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  61% 3.02G/4.96G [00:52<00:35, 53.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  64% 3.15G/4.96G [00:52<00:22, 81.9MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  65% 3.25G/4.99G [00:53<00:22, 78.5MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  65% 3.22G/4.96G [00:53<00:18, 94.3MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  68% 3.38G/4.99G [00:53<00:13, 117MB/s] \u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  79% 3.29G/4.18G [00:53<00:12, 73.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  66% 3.29G/4.96G [00:54<00:19, 85.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  80% 3.35G/4.18G [00:54<00:11, 69.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  69% 3.42G/4.96G [00:54<00:12, 120MB/s] \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  82% 3.42G/4.18G [00:54<00:08, 86.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  69% 3.45G/4.99G [00:55<00:21, 71.5MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  85% 3.55G/4.18G [00:59<00:13, 45.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  70% 3.52G/4.99G [01:00<00:41, 35.5MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  70% 3.49G/4.96G [01:01<00:40, 36.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  87% 3.62G/4.18G [01:01<00:11, 46.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  72% 3.59G/4.99G [01:01<00:33, 42.4MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  73% 3.62G/4.96G [01:01<00:24, 54.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  90% 3.76G/4.18G [01:01<00:06, 69.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  73% 3.65G/4.99G [01:01<00:26, 51.3MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  93% 3.89G/4.18G [01:01<00:02, 100MB/s] \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  76% 3.76G/4.96G [01:02<00:15, 78.5MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  75% 3.72G/4.99G [01:02<00:19, 66.0MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  76% 3.79G/4.99G [01:03<00:20, 60.2MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  77% 3.85G/4.99G [01:05<00:20, 54.5MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  95% 3.96G/4.18G [01:05<00:03, 56.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  77% 3.82G/4.96G [01:05<00:23, 48.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  96% 4.02G/4.18G [01:06<00:02, 59.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  79% 3.92G/4.99G [01:06<00:19, 55.2MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  78% 3.89G/4.96G [01:06<00:20, 51.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  80% 3.96G/4.96G [01:06<00:16, 61.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  97% 4.04G/4.18G [01:07<00:02, 48.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  81% 4.03G/4.96G [01:08<00:16, 55.7MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  81% 4.05G/4.99G [01:08<00:17, 52.2MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  98% 4.11G/4.18G [01:09<00:01, 44.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors: 100% 4.18G/4.18G [01:09<00:00, 60.4MB/s]\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  83% 4.09G/4.96G [01:09<00:14, 61.5MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  85% 4.25G/4.99G [01:09<00:06, 107MB/s] \u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  85% 4.23G/4.96G [01:09<00:07, 99.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  87% 4.29G/4.96G [01:09<00:05, 122MB/s] \u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  88% 4.39G/4.99G [01:09<00:04, 136MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  88% 4.36G/4.96G [01:09<00:04, 136MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  89% 4.43G/4.96G [01:10<00:03, 152MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  89% 4.46G/4.99G [01:10<00:04, 132MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  91% 4.52G/4.99G [01:10<00:02, 158MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  91% 4.50G/4.96G [01:10<00:02, 156MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  92% 4.59G/4.99G [01:10<00:02, 157MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  93% 4.66G/4.99G [01:11<00:01, 189MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  92% 4.56G/4.96G [01:11<00:03, 118MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  95% 4.72G/4.99G [01:12<00:03, 87.3MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  93% 4.63G/4.96G [01:13<00:04, 77.9MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  96% 4.79G/4.99G [01:13<00:02, 94.5MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  95% 4.69G/4.96G [01:13<00:03, 88.7MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  97% 4.86G/4.99G [01:13<00:01, 109MB/s] \u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  96% 4.76G/4.96G [01:13<00:01, 107MB/s] \u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  99% 4.93G/4.99G [01:14<00:00, 128MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  97% 4.82G/4.96G [01:14<00:01, 127MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  99% 4.89G/4.96G [01:14<00:00, 143MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors: 100% 4.99G/4.99G [01:14<00:00, 67.0MB/s]\n",
            "Fetching 3 files:  33% 1/3 [01:15<02:30, 75.04s/it]\n",
            "\n",
            "model-00002-of-00003.safetensors: 100% 4.96G/4.96G [01:14<00:00, 66.2MB/s]\n",
            "Fetching 3 files: 100% 3/3 [01:15<00:00, 25.18s/it]\n",
            "Loading checkpoint shards: 100% 3/3 [00:04<00:00,  1.43s/it]\n",
            "generation_config.json: 100% 141/141 [00:00<00:00, 1.61MB/s]\n",
            "Model loaded.\n",
            "Running 3 inference passes...\n",
            " Run 1/3: 11.50 tokens/sec, 13.52 GB\n",
            " Run 2/3: 16.51 tokens/sec, 13.52 GB\n",
            " Run 3/3: 16.49 tokens/sec, 13.52 GB\n",
            "\n",
            "kv_cache_results Results:\n",
            "  Average Latency: 6.940 seconds\n",
            "  Average Throughput: 14.83 tokens/sec\n",
            "  Average Memory: 13.52 GB\n",
            "\n",
            "Sample Output:\n",
            "USER:  \n",
            "Describe this image in detail.\n",
            "ASSISTANT: The image features a pier extending out into a large body of water, likely a lake. The pier is made of wood and has a bench situated on it, providing ...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python test_flash_attn.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJZATiT0eCXB",
        "outputId": "cb18597c-2d01-43f3-adf5-aafe1d0be146"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-29 02:36:52.117531: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-29 02:36:52.137644: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764383812.158778   22475 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764383812.165324   22475 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764383812.181940   22475 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764383812.181968   22475 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764383812.181971   22475 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764383812.181973   22475 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-29 02:36:52.186951: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Loading sample image...\n",
            "Image size: (1000, 667)\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "TEST 2: With Flash Attention 2\n",
            "--------------------------------------------------------------------------------\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Loading checkpoint shards: 100% 3/3 [00:04<00:00,  1.40s/it]\n",
            "Running 3 inference passes...\n",
            " Run 1/3: 11.93 tokens/sec, 13.52 GB\n",
            " Run 2/3: 14.03 tokens/sec, 13.52 GB\n",
            " Run 3/3: 13.99 tokens/sec, 13.52 GB\n",
            "\n",
            "Flash Attention 2 Results:\n",
            "  Average Latency: 7.555 seconds\n",
            "  Average Throughput: 13.31 tokens/sec\n",
            "  Average Memory: 13.52 GB\n",
            "\n",
            "Sample Output:\n",
            "USER:  \n",
            "Describe this image in detail.\n",
            "ASSISTANT: The image features a pier extending out into a large body of water, likely a lake. The pier is made of wood and has a bench situated on it, providing ...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python test_bitsandbytes.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vW2O-BceeFvK",
        "outputId": "b3facea7-b489-4984-cfd0-ed152dcaa373"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-29 02:39:20.653179: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-29 02:39:20.673303: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764383960.694256   23143 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764383960.700634   23143 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764383960.717129   23143 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764383960.717154   23143 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764383960.717157   23143 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764383960.717160   23143 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-29 02:39:20.721990: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Loading sample image...\n",
            "Image size: (1000, 667)\n",
            "\n",
            "================================================================================\n",
            "bitsandbytes Quantization 4 bits Test for LLaVA\n",
            "================================================================================\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "Loading checkpoint shards: 100% 3/3 [00:16<00:00,  5.53s/it]\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "\n",
            "AWQ bitsandbytes quant 4 bit complete. Loading quantized model...\n",
            "\n",
            "Running 3 inference passes...\n",
            " Run 1/3: 12.25 tokens/sec, 4.69 GB\n",
            " Run 2/3: 13.91 tokens/sec, 4.69 GB\n",
            " Run 3/3: 13.62 tokens/sec, 4.69 GB\n",
            "\n",
            "bitsandbytes Results:\n",
            "  Avg Latency: 7.567 seconds\n",
            "  Avg Throughput: 13.26 tokens/sec\n",
            "  Avg Memory: 4.69 GB\n",
            "\n",
            "Sample Output:\n",
            "USER:  \n",
            "Describe this image in detail.\n",
            "ASSISTANT: The image features a wooden pier extending out over a body of water. The pier is situated in a serene, natural setting with a mountain in the backgrou...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python test_kv.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aik6Mkopek8e",
        "outputId": "808fc0c6-80ec-4735-9e78-d1193c4fb5ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-29 02:40:24.761685: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-29 02:40:24.781414: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764384024.802902   23502 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764384024.809458   23502 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764384024.826165   23502 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764384024.826193   23502 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764384024.826196   23502 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764384024.826199   23502 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-29 02:40:24.831232: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Loading sample image...\n",
            "Image size: (1000, 667)\n",
            "\n",
            "================================================================================\n",
            "KV_cache Test for LLaVA\n",
            "================================================================================\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Loading checkpoint shards: 100% 3/3 [00:04<00:00,  1.42s/it]\n",
            "Model loaded.\n",
            "Running 3 inference passes...\n",
            " Run 1/3: 106.07 tokens/sec, 13.55 GB\n",
            " Run 2/3: 108.03 tokens/sec, 13.55 GB\n",
            " Run 3/3: 107.89 tokens/sec, 13.55 GB\n",
            "\n",
            "kv_cache Results:\n",
            "  Average Latency: 6.467 seconds\n",
            "  Average Throughput: 107.33 tokens/sec\n",
            "  Average Memory: 13.55 GB\n",
            "\n",
            "Sample Output:\n",
            "USER:  \n",
            "Describe this image in detail.\n",
            "ASSISTANT: The image features a pier with a wooden walkway extending out into the water. The pier is situated near a mountain, providing a picturesque backdrop. ...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python test_kv_prune_quant.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H55M7Pi8yexw",
        "outputId": "62cdc10a-8021-443f-b413-d1435d11572a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-28 05:13:39.970014: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-28 05:13:39.987379: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764306820.009165    9258 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764306820.015633    9258 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764306820.031725    9258 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764306820.031754    9258 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764306820.031756    9258 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764306820.031759    9258 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-28 05:13:40.036494: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Loading sample image...\n",
            "Image size: (1000, 667)\n",
            "\n",
            "================================================================================\n",
            "Prun + Quantization KV_cache Test for LLaVA\n",
            "================================================================================\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Loading checkpoint shards: 100% 3/3 [00:04<00:00,  1.42s/it]\n",
            "Model loaded.\n",
            "Running 3 inference passes...\n",
            " Run 1/3: 202.79 tokens/sec, 14.26 GB\n",
            " Run 2/3: 205.27 tokens/sec, 14.26 GB\n",
            " Run 3/3: 204.80 tokens/sec, 14.26 GB\n",
            "\n",
            "Prun + Quant_kv_cache Results:\n",
            "  Average Latency: 3.148 seconds\n",
            "  Average Throughput: 204.29 tokens/sec\n",
            "  Average Memory: 14.26 GB\n",
            "\n",
            "Sample Output:\n",
            "USER:  \n",
            "Describe this image in detail.\n",
            "ASSISTANT:The image shows a pier extending out into the water. The pier is situated on the water's edge, and it is a long pier that extends out into the water. T...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python test_kv_prune.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YPBK2sapy0zb",
        "outputId": "462b4f20-9fa3-4fc6-e750-7bbf48fc2521"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-28 04:49:39.905786: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-28 04:49:39.923555: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764305379.945223    3001 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764305379.951672    3001 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764305379.968118    3001 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764305379.968152    3001 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764305379.968155    3001 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764305379.968158    3001 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-28 04:49:39.973023: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Loading sample image...\n",
            "Image size: (1000, 667)\n",
            "\n",
            "================================================================================\n",
            "Prun KV_cache Test for LLaVA\n",
            "================================================================================\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Loading checkpoint shards: 100% 3/3 [00:04<00:00,  1.42s/it]\n",
            "Model loaded.\n",
            "Running 3 inference passes...\n",
            " Run 1/3: 200.50 tokens/sec, 14.11 GB\n",
            " Run 2/3: 204.95 tokens/sec, 14.11 GB\n",
            " Run 3/3: 205.20 tokens/sec, 14.11 GB\n",
            "\n",
            "Prun_kv_cacheresults Results:\n",
            "  Average Latency: 3.159 seconds\n",
            "  Average Throughput: 203.55 tokens/sec\n",
            "  Average Memory: 14.11 GB\n",
            "\n",
            "Sample Output:\n",
            "USER:  \n",
            "Describe this image in detail.\n",
            "ASSISTANT:The image shows a pier extending out into the water. The pier is situated on the water's edge, and it is a long pier that extends out into the water. T...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python test_kv_quant.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afVotDivy6tQ",
        "outputId": "8e1953e9-379d-4ce7-a267-08b63efd8e4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-28 05:15:54.006989: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-28 05:15:54.024089: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764306954.046074    9861 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764306954.052725    9861 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764306954.069526    9861 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764306954.069558    9861 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764306954.069563    9861 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764306954.069567    9861 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-28 05:15:54.074279: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Loading sample image...\n",
            "Image size: (1000, 667)\n",
            "\n",
            "================================================================================\n",
            "Quantization KV_cache Test for LLaVA\n",
            "================================================================================\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Loading checkpoint shards: 100% 3/3 [00:04<00:00,  1.41s/it]\n",
            "Model loaded.\n",
            "Running 3 inference passes...\n",
            " Run 1/3: 106.46 tokens/sec, 14.28 GB\n",
            " Run 2/3: 108.11 tokens/sec, 14.28 GB\n",
            " Run 3/3: 108.13 tokens/sec, 14.28 GB\n",
            "\n",
            "Quant_kv_cache Results:\n",
            "  Average Latency: 6.452 seconds\n",
            "  Average Throughput: 107.56 tokens/sec\n",
            "  Average Memory: 14.28 GB\n",
            "\n",
            "Sample Output:\n",
            "USER:  \n",
            "Describe this image in detail.\n",
            "ASSISTANT: The image features a pier with a wooden walkway extending out into the water. The pier is situated in front of a beautiful mountain range, creating a ...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python int8_tensor_prune_quant.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5r5OpOrz5glA",
        "outputId": "1de74669-2378-44b9-e572-0362a6b0f39e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-29 02:24:00.293612: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-29 02:24:02.188570: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764383042.210425   18682 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764383042.216895   18682 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764383042.234012   18682 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764383042.234039   18682 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764383042.234042   18682 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764383042.234045   18682 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-29 02:24:02.238934: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Loading sample image...\n",
            "Image size: (1000, 667)\n",
            "\n",
            "================================================================================\n",
            "Method 1: INT8 + Tensor-wise KV Pruning + KV Quantization\n",
            "================================================================================\n",
            "processor_config.json: 100% 173/173 [00:00<00:00, 1.58MB/s]\n",
            "chat_template.json: 100% 701/701 [00:00<00:00, 5.73MB/s]\n",
            "chat_template.jinja: 100% 674/674 [00:00<00:00, 7.75MB/s]\n",
            "preprocessor_config.json: 100% 505/505 [00:00<00:00, 5.38MB/s]\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "tokenizer_config.json: 1.45kB [00:00, 7.35MB/s]\n",
            "tokenizer.model: 100% 500k/500k [00:01<00:00, 405kB/s]\n",
            "tokenizer.json: 3.62MB [00:00, 210MB/s]\n",
            "added_tokens.json: 100% 41.0/41.0 [00:00<00:00, 390kB/s]\n",
            "special_tokens_map.json: 100% 552/552 [00:00<00:00, 5.92MB/s]\n",
            "config.json: 100% 950/950 [00:00<00:00, 10.1MB/s]\n",
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "model.safetensors.index.json: 70.1kB [00:00, 193MB/s]\n",
            "Fetching 3 files:   0% 0/3 [00:00<?, ?it/s]\n",
            "model-00001-of-00003.safetensors:   0% 0.00/4.99G [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:   0% 0.00/4.96G [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:   0% 0.00/4.18G [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:   0% 13.5k/4.99G [00:01<122:44:48, 11.3kB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:   0% 114k/4.99G [00:02<21:50:35, 63.5kB/s]  \u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:   0% 546k/4.18G [00:02<4:41:54, 247kB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  10% 504M/4.99G [00:02<00:12, 372MB/s]    \u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:   0% 599k/4.96G [00:02<6:12:49, 222kB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:   2% 67.7M/4.18G [00:05<04:48, 14.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  14% 680M/4.99G [00:08<01:01, 70.7MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:   3% 135M/4.18G [00:12<06:20, 10.6MB/s] \u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  16% 808M/4.99G [00:12<01:19, 52.7MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:   5% 202M/4.18G [00:13<03:27, 19.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  18% 876M/4.99G [00:13<01:06, 61.7MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:   6% 269M/4.18G [00:13<02:06, 30.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:   1% 67.6M/4.96G [00:13<14:59, 5.44MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  10% 403M/4.18G [00:13<01:04, 58.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:   3% 135M/4.96G [00:13<06:25, 12.5MB/s] \u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  19% 943M/4.99G [00:14<01:04, 62.4MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:   5% 269M/4.96G [00:14<02:38, 29.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  11% 470M/4.18G [00:14<01:02, 59.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  20% 1.01G/4.99G [00:15<01:03, 62.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  13% 537M/4.18G [00:15<00:53, 67.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  22% 1.07G/4.99G [00:15<00:51, 75.7MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  16% 671M/4.18G [00:15<00:31, 111MB/s] \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:   9% 470M/4.96G [00:15<01:19, 56.7MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  24% 1.21G/4.99G [00:15<00:33, 114MB/s] \u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  12% 604M/4.96G [00:16<00:54, 80.1MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  26% 1.27G/4.99G [00:16<00:33, 111MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  18% 738M/4.18G [00:16<00:33, 103MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  14% 671M/4.96G [00:16<00:51, 83.6MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  27% 1.34G/4.99G [00:16<00:30, 122MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  19% 805M/4.18G [00:17<00:33, 100MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  15% 738M/4.96G [00:17<00:45, 92.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  21% 873M/4.18G [00:17<00:30, 109MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  18% 872M/4.96G [00:17<00:36, 111MB/s] \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  23% 940M/4.18G [00:18<00:32, 100MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  19% 940M/4.96G [00:18<00:34, 118MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  28% 1.41G/4.99G [00:18<00:50, 71.1MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  24% 1.01G/4.18G [00:18<00:28, 110MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  31% 1.54G/4.99G [00:19<00:33, 103MB/s] \u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  26% 1.07G/4.18G [00:19<00:29, 104MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  22% 1.07G/4.96G [00:20<00:41, 93.2MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  32% 1.61G/4.99G [00:20<00:41, 82.4MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  27% 1.14G/4.18G [00:20<00:36, 82.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  29% 1.21G/4.18G [00:22<00:42, 69.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  23% 1.14G/4.96G [00:25<01:31, 41.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  31% 1.27G/4.18G [00:25<01:13, 39.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  34% 1.68G/4.99G [00:27<01:49, 30.2MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  32% 1.34G/4.18G [00:27<01:10, 40.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  35% 1.75G/4.99G [00:29<01:40, 32.1MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  24% 1.21G/4.96G [00:29<02:00, 31.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  34% 1.41G/4.18G [00:29<01:13, 37.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  36% 1.82G/4.99G [00:29<01:15, 42.2MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  26% 1.28G/4.96G [00:29<01:33, 39.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  35% 1.48G/4.18G [00:29<00:55, 48.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  38% 1.89G/4.99G [00:29<01:00, 51.7MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  27% 1.34G/4.96G [00:30<01:15, 48.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  37% 1.54G/4.18G [00:30<00:44, 59.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  39% 1.96G/4.99G [00:30<00:46, 64.7MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  39% 1.61G/4.18G [00:30<00:36, 70.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  41% 2.02G/4.99G [00:30<00:39, 75.7MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  28% 1.41G/4.96G [00:30<01:04, 55.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  40% 1.68G/4.18G [00:30<00:25, 96.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  42% 2.09G/4.99G [00:31<00:32, 89.9MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  30% 1.48G/4.96G [00:31<00:51, 67.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  42% 1.75G/4.18G [00:31<00:24, 100MB/s] \u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  43% 2.16G/4.99G [00:32<00:33, 83.7MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  43% 1.81G/4.18G [00:32<00:24, 96.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  33% 1.61G/4.96G [00:33<00:49, 68.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  48% 2.01G/4.18G [00:33<00:15, 137MB/s] \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  34% 1.68G/4.96G [00:33<00:43, 75.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  50% 2.08G/4.18G [00:34<00:17, 120MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  45% 2.22G/4.99G [00:34<00:47, 58.8MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  37% 1.81G/4.96G [00:34<00:30, 102MB/s] \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  51% 2.15G/4.18G [00:34<00:16, 119MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  46% 2.28G/4.99G [00:34<00:40, 66.4MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  38% 1.88G/4.96G [00:35<00:31, 98.5MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  47% 2.35G/4.99G [00:35<00:36, 72.8MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  41% 2.01G/4.96G [00:35<00:20, 143MB/s] \u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  50% 2.48G/4.99G [00:35<00:23, 106MB/s] \u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  42% 2.08G/4.96G [00:36<00:24, 118MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  51% 2.55G/4.99G [00:37<00:29, 83.7MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  45% 2.21G/4.96G [00:37<00:21, 126MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  53% 2.21G/4.18G [00:39<00:47, 41.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  55% 2.28G/4.18G [00:39<00:36, 52.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  52% 2.62G/4.99G [00:41<01:01, 38.7MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  46% 2.28G/4.96G [00:42<00:58, 45.9MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  54% 2.68G/4.99G [00:43<01:02, 36.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  56% 2.35G/4.18G [00:46<01:13, 24.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  55% 2.75G/4.99G [00:46<01:08, 32.8MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  47% 2.35G/4.96G [00:46<01:21, 32.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  59% 2.48G/4.18G [00:46<00:38, 43.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  63% 2.62G/4.18G [00:46<00:23, 67.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  56% 2.82G/4.99G [00:46<00:50, 43.1MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  49% 2.42G/4.96G [00:46<01:03, 40.1MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  58% 2.88G/4.99G [00:47<00:40, 51.5MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  66% 2.75G/4.18G [00:47<00:17, 82.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  60% 3.02G/4.99G [00:47<00:23, 83.2MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  50% 2.48G/4.96G [00:48<01:01, 40.2MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  62% 3.08G/4.99G [00:48<00:23, 80.7MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  54% 2.68G/4.96G [00:49<00:30, 74.1MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  63% 3.15G/4.99G [00:49<00:21, 84.4MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  55% 2.75G/4.96G [00:49<00:26, 82.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  67% 2.82G/4.18G [00:49<00:22, 61.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  64% 3.22G/4.99G [00:49<00:18, 93.4MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  71% 2.95G/4.18G [00:50<00:14, 86.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  66% 3.29G/4.99G [00:50<00:17, 95.8MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  58% 2.89G/4.96G [00:50<00:21, 96.9MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  67% 3.35G/4.99G [00:50<00:15, 105MB/s] \u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  61% 3.02G/4.96G [00:50<00:15, 125MB/s] \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  72% 3.02G/4.18G [00:50<00:13, 85.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  69% 3.42G/4.99G [00:51<00:15, 103MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  62% 3.09G/4.96G [00:52<00:17, 105MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  70% 3.49G/4.99G [00:52<00:15, 99.6MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  64% 3.15G/4.96G [00:52<00:16, 108MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  76% 3.15G/4.18G [00:52<00:11, 85.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  71% 3.55G/4.99G [00:52<00:13, 108MB/s] \u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  77% 3.22G/4.18G [00:53<00:11, 83.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  79% 3.29G/4.18G [00:54<00:10, 85.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  65% 3.22G/4.96G [00:54<00:22, 78.8MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  73% 3.62G/4.99G [00:54<00:20, 66.4MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  66% 3.29G/4.96G [00:55<00:23, 72.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  80% 3.35G/4.18G [00:56<00:12, 63.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  82% 3.42G/4.18G [00:58<00:14, 51.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  68% 3.35G/4.96G [00:58<00:33, 48.0MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  74% 3.69G/4.99G [00:58<00:32, 39.7MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  75% 3.76G/4.99G [01:01<00:38, 32.5MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  84% 3.49G/4.18G [01:01<00:19, 34.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  77% 3.82G/4.99G [01:01<00:28, 40.5MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  69% 3.42G/4.96G [01:01<00:46, 33.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  87% 3.62G/4.18G [01:02<00:09, 56.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  78% 3.89G/4.99G [01:02<00:21, 51.4MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  88% 3.69G/4.18G [01:02<00:07, 66.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  79% 3.96G/4.99G [01:03<00:18, 54.9MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  70% 3.49G/4.96G [01:03<00:42, 34.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  72% 3.56G/4.96G [01:04<00:32, 43.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  73% 3.63G/4.96G [01:04<00:23, 57.7MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  81% 4.02G/4.99G [01:04<00:17, 54.3MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  82% 4.09G/4.99G [01:04<00:12, 72.9MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  90% 3.76G/4.18G [01:04<00:08, 51.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  74% 3.69G/4.96G [01:04<00:19, 65.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  92% 3.84G/4.18G [01:05<00:05, 65.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  83% 4.16G/4.99G [01:05<00:11, 73.9MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  95% 3.98G/4.18G [01:05<00:02, 94.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  77% 3.83G/4.96G [01:06<00:13, 83.6MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  85% 4.22G/4.99G [01:06<00:09, 78.5MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  79% 3.89G/4.96G [01:06<00:10, 99.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  80% 3.96G/4.96G [01:06<00:08, 122MB/s] \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  97% 4.04G/4.18G [01:06<00:01, 91.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  81% 4.03G/4.96G [01:06<00:06, 141MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  86% 4.29G/4.99G [01:07<00:08, 78.1MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  98% 4.11G/4.18G [01:07<00:00, 94.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  89% 4.43G/4.99G [01:07<00:04, 117MB/s] \u001b[A\n",
            "model-00001-of-00003.safetensors:  90% 4.49G/4.99G [01:08<00:04, 123MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors: 100% 4.18G/4.18G [01:08<00:00, 61.1MB/s]\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  83% 4.09G/4.96G [01:08<00:11, 77.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  84% 4.16G/4.96G [01:09<00:08, 92.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  85% 4.23G/4.96G [01:09<00:06, 117MB/s] \u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  91% 4.56G/4.99G [01:09<00:04, 94.0MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  93% 4.63G/4.99G [01:09<00:03, 116MB/s] \u001b[A\n",
            "model-00001-of-00003.safetensors:  94% 4.69G/4.99G [01:10<00:02, 120MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  87% 4.30G/4.96G [01:10<00:07, 87.0MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  95% 4.72G/4.99G [01:10<00:02, 90.8MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  88% 4.36G/4.96G [01:10<00:05, 103MB/s] \u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  96% 4.79G/4.99G [01:12<00:03, 61.9MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  89% 4.43G/4.96G [01:13<00:08, 59.2MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  97% 4.86G/4.99G [01:13<00:02, 63.9MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  91% 4.49G/4.96G [01:13<00:06, 69.6MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  99% 4.93G/4.99G [01:13<00:00, 88.1MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  93% 4.63G/4.96G [01:13<00:02, 122MB/s] \u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  95% 4.69G/4.96G [01:13<00:01, 150MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  96% 4.76G/4.96G [01:14<00:01, 178MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors: 100% 4.99G/4.99G [01:14<00:00, 67.2MB/s]\n",
            "Fetching 3 files:  33% 1/3 [01:14<02:29, 74.81s/it]\n",
            "\n",
            "model-00002-of-00003.safetensors:  97% 4.82G/4.96G [01:14<00:00, 179MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  99% 4.89G/4.96G [01:14<00:00, 217MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors: 100% 4.96G/4.96G [01:14<00:00, 66.3MB/s]\n",
            "Fetching 3 files: 100% 3/3 [01:15<00:00, 25.07s/it]\n",
            "Loading checkpoint shards: 100% 3/3 [00:16<00:00,  5.50s/it]\n",
            "generation_config.json: 100% 141/141 [00:00<00:00, 1.20MB/s]\n",
            "INT8 model loaded (bitsandbytes).\n",
            "Running 3 inference passes...\n",
            " Run 1/3: 75.72 tokens/sec, 7.96 GB\n",
            " Run 2/3: 83.29 tokens/sec, 7.96 GB\n",
            " Run 3/3: 84.08 tokens/sec, 7.96 GB\n",
            "\n",
            "Method 1 Results (INT8 + Tensor-wise Prune + KV Quant):\n",
            "  Average Latency: 7.928 seconds\n",
            "  Average Throughput: 81.03 tokens/sec\n",
            "  Average Memory: 7.96 GB\n",
            "\n",
            "Sample Output:\n",
            "USER:  \n",
            "Describe this image in detail.\n",
            "ASSISTANT: The image is a beautiful, serene, and picturesque view of a pier and a mountain in the distance. The image is a beautiful, serene, and picturesque vie...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python int4_tensor_prune_quant.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8meb3O5M6RGK",
        "outputId": "1865637f-704b-41e6-f402-7624d3e6f5ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-29 02:26:56.893150: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-29 02:26:56.913265: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764383216.935026   19669 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764383216.941908   19669 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764383216.959237   19669 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764383216.959274   19669 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764383216.959276   19669 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764383216.959279   19669 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-29 02:26:56.964221: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Loading sample image...\n",
            "Image size: (1000, 667)\n",
            "\n",
            "================================================================================\n",
            "Method 1: INT4 + Tensor-wise KV Pruning + KV Quantization\n",
            "================================================================================\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "Loading checkpoint shards: 100% 3/3 [00:17<00:00,  5.69s/it]\n",
            "INT4 model loaded (bitsandbytes).\n",
            "Running 3 inference passes...\n",
            " Run 1/3: 100.70 tokens/sec, 5.21 GB\n",
            " Run 2/3: 100.18 tokens/sec, 5.21 GB\n",
            " Run 3/3: 101.83 tokens/sec, 5.21 GB\n",
            "\n",
            "Method 1 Results (INT4 + Tensor-wise Prune + KV Quant):\n",
            "  Average Latency: 6.878 seconds\n",
            "  Average Throughput: 100.91 tokens/sec\n",
            "  Average Memory: 5.21 GB\n",
            "\n",
            "Sample Output:\n",
            "USER:  \n",
            "Describe this image in detail.\n",
            "ASSISTANT: The image shows a pier, which is a wooden pier with a pier, and a pier, and a pier, and a pier, and a pier, and a pier, and a pier, and a wooden pier ...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python int4_channel_prune_quant.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iCWqqDdq7zj4",
        "outputId": "9b2f1e9c-a784-4522-efd5-a82fd1054c6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-29 02:31:09.576002: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-29 02:31:09.596174: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764383469.617313   20852 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764383469.623792   20852 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764383469.640721   20852 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764383469.640752   20852 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764383469.640755   20852 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764383469.640757   20852 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-29 02:31:09.645619: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Loading sample image...\n",
            "Image size: (1000, 667)\n",
            "\n",
            "================================================================================\n",
            "Method 2: INT4 + Channel-wise KV Pruning + KV Quantization\n",
            "================================================================================\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "Loading checkpoint shards: 100% 3/3 [00:16<00:00,  5.59s/it]\n",
            "INT4 model loaded (bitsandbytes).\n",
            "Running 3 inference passes for Method 2...\n",
            " Run 1/3: 108.76 tokens/sec, 5.20 GB\n",
            " Run 2/3: 110.85 tokens/sec, 5.20 GB\n",
            " Run 3/3: 111.10 tokens/sec, 5.20 GB\n",
            "\n",
            "Method 2 Results (INT4 + Channel-wise Prune + KV Quant):\n",
            "  Average Latency: 6.224 seconds\n",
            "  Average Throughput: 110.23 tokens/sec\n",
            "  Average Memory: 5.20 GB\n",
            "\n",
            "Sample Output:\n",
            "USER:  \n",
            "Describe this image in detail.\n",
            "ASSISTANT: The image features a pier or dock extending out into the water. The pier is surrounded by a lush, green forest, creating a serene and picturesque scen...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python channel_prune_quant.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E6WzfMF_9c2U",
        "outputId": "aa02b698-827c-4d71-c054-f8bb4a4c18d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-29 02:32:48.300225: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-29 02:32:48.319456: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764383568.340172   21390 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764383568.346562   21390 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764383568.363720   21390 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764383568.363748   21390 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764383568.363751   21390 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764383568.363754   21390 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-29 02:32:48.368661: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Loading sample image...\n",
            "Image size: (1000, 667)\n",
            "\n",
            "================================================================================\n",
            "Method 2: Channel-wise KV Pruning + KV Quantization\n",
            "================================================================================\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Loading checkpoint shards: 100% 3/3 [00:04<00:00,  1.41s/it]\n",
            "LLava model loaded .\n",
            "Running 3 inference passes for Method 2...\n",
            " Run 1/3: 118.17 tokens/sec, 14.28 GB\n",
            " Run 2/3: 118.77 tokens/sec, 14.28 GB\n",
            " Run 3/3: 118.44 tokens/sec, 14.28 GB\n",
            "\n",
            "Method 2 Results (Channel-wise Prune + KV Quant):\n",
            "  Average Latency: 5.774 seconds\n",
            "  Average Throughput: 118.46 tokens/sec\n",
            "  Average Memory: 14.28 GB\n",
            "\n",
            "Sample Output:\n",
            "USER:  \n",
            "Describe this image in detail.\n",
            "ASSISTANT: The image features a pier extending out into a large body of water, likely a lake or a river. The pier is surrounded by a lush, green forest, creating...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install vllm"
      ],
      "metadata": {
        "id": "tOSTuyFvd3BY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python test_llmcompressor_awq.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6pPiSEpXKPlR",
        "outputId": "864bde01-147a-44c1-bd52-6a63e3ea9cc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-28 22:04:40.922647: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-28 22:04:40.941506: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764367480.964241   31943 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764367480.971767   31943 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764367480.990977   31943 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764367480.991003   31943 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764367480.991006   31943 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764367480.991009   31943 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-28 22:04:40.995914: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
            "  warnings.warn(\n",
            "Loading sample image...\n",
            "Image size: (1000, 667)\n",
            "\n",
            "\n",
            "AWQ quantization complete. Loading quantized model...\n",
            "\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Compressing model: 224it [00:00, 608.92it/s]\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "Running 3 inference passes...\n",
            " Run 1/3: 1.47 tokens/sec, 17.08 GB\n",
            " Run 2/3: 1.56 tokens/sec, 17.08 GB\n",
            " Run 3/3: 1.56 tokens/sec, 17.08 GB\n",
            "\n",
            "Baseline Results:\n",
            "  Avg Latency: 55.010 seconds\n",
            "  Avg Throughput: 1.53 tokens/sec\n",
            "  Avg Memory: 17.08 GB\n",
            "\n",
            "Sample Output:\n",
            "USER:  \n",
            "Please detail describe this image\n",
            " ASSISTANT: The image features a pier extending out over a large body of water, possibly a lake. The pier is made of wood and has a bench situated on it, prov...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python test_llmcompressor_awq_vllm.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iaoy3AakgThw",
        "outputId": "cd0cf80a-c056-4465-e44d-16e3c7a56a14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-28 22:21:10.063334: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-28 22:21:10.082333: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764368470.105693   36949 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764368470.113601   36949 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764368470.134817   36949 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764368470.134845   36949 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764368470.134848   36949 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764368470.134851   36949 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-28 22:21:10.139844: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
            "  warnings.warn(\n",
            "Loading sample image...\n",
            "Image size: (1000, 667)\n",
            "\n",
            "\n",
            "AWQ quantization complete. Loading quantized model...\n",
            "\n",
            "INFO 11-28 22:21:22 [utils.py:253] non-default args: {'disable_log_stats': True, 'model': 'llava-hf/llava-1.5-7b-hf'}\n",
            "INFO 11-28 22:21:23 [model.py:631] Resolved architecture: LlavaForConditionalGeneration\n",
            "INFO 11-28 22:21:23 [model.py:1745] Using max model len 4096\n",
            "INFO 11-28 22:21:25 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
            "WARNING 11-28 22:21:27 [system_utils.py:103] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: CUDA is initialized\n",
            "2025-11-28 22:21:31.810577: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764368491.834533   37094 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764368491.842143   37094 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764368491.865189   37094 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764368491.865221   37094 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764368491.865226   37094 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764368491.865230   37094 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
            "  warnings.warn(\n",
            "\u001b[1;36m(EngineCore_DP0 pid=37094)\u001b[0;0m INFO 11-28 22:21:41 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='llava-hf/llava-1.5-7b-hf', speculative_config=None, tokenizer='llava-hf/llava-1.5-7b-hf', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=llava-hf/llava-1.5-7b-hf, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}\n",
            "\u001b[1;36m(EngineCore_DP0 pid=37094)\u001b[0;0m INFO 11-28 22:21:43 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.30.0.2:55593 backend=nccl\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "\u001b[1;36m(EngineCore_DP0 pid=37094)\u001b[0;0m INFO 11-28 22:21:43 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
            "\u001b[1;36m(EngineCore_DP0 pid=37094)\u001b[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=37094)\u001b[0;0m INFO 11-28 22:21:49 [gpu_model_runner.py:3259] Starting to load model llava-hf/llava-1.5-7b-hf...\n",
            "\u001b[1;36m(EngineCore_DP0 pid=37094)\u001b[0;0m INFO 11-28 22:21:49 [layer.py:570] MultiHeadAttention attn_backend: AttentionBackendEnum.FLASH_ATTN, use_upstream_fa: False\n",
            "\u001b[1;36m(EngineCore_DP0 pid=37094)\u001b[0;0m INFO 11-28 22:21:50 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']\n",
            "\u001b[1;36m(EngineCore_DP0 pid=37094)\u001b[0;0m INFO 11-28 22:21:50 [cuda.py:427] Using FLASH_ATTN backend.\n",
            "Loading safetensors checkpoint shards: 100% 3/3 [00:03<00:00,  1.31s/it]\n",
            "\u001b[1;36m(EngineCore_DP0 pid=37094)\u001b[0;0m INFO 11-28 22:21:55 [default_loader.py:314] Loading weights took 4.02 seconds\n",
            "\u001b[1;36m(EngineCore_DP0 pid=37094)\u001b[0;0m INFO 11-28 22:21:56 [gpu_model_runner.py:3338] Model loading took 13.1343 GiB memory and 5.573395 seconds\n",
            "\u001b[1;36m(EngineCore_DP0 pid=37094)\u001b[0;0m INFO 11-28 22:21:56 [gpu_model_runner.py:4088] Encoder cache will be initialized with a budget of 8192 tokens, and profiled with 14 image items of the maximum feature size.\n",
            "\u001b[1;36m(EngineCore_DP0 pid=37094)\u001b[0;0m INFO 11-28 22:22:04 [backends.py:631] Using cache directory: /root/.cache/vllm/torch_compile_cache/d2b828ecec/rank_0_0/backbone for vLLM's torch.compile\n",
            "\u001b[1;36m(EngineCore_DP0 pid=37094)\u001b[0;0m INFO 11-28 22:22:04 [backends.py:647] Dynamo bytecode transform time: 7.40 s\n",
            "\u001b[1;36m(EngineCore_DP0 pid=37094)\u001b[0;0m INFO 11-28 22:22:11 [backends.py:210] Directly load the compiled graph(s) for dynamic shape from the cache, took 6.768 s\n",
            "\u001b[1;36m(EngineCore_DP0 pid=37094)\u001b[0;0m INFO 11-28 22:22:12 [monitor.py:34] torch.compile takes 14.17 s in total\n",
            "\u001b[1;36m(EngineCore_DP0 pid=37094)\u001b[0;0m INFO 11-28 22:22:15 [gpu_worker.py:359] Available KV cache memory: 5.13 GiB\n",
            "\u001b[1;36m(EngineCore_DP0 pid=37094)\u001b[0;0m INFO 11-28 22:22:16 [kv_cache_utils.py:1229] GPU KV cache size: 10,496 tokens\n",
            "\u001b[1;36m(EngineCore_DP0 pid=37094)\u001b[0;0m INFO 11-28 22:22:16 [kv_cache_utils.py:1234] Maximum concurrency for 4,096 tokens per request: 2.56x\n",
            "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100% 51/51 [00:05<00:00,  8.89it/s]\n",
            "Capturing CUDA graphs (decode, FULL): 100% 35/35 [00:03<00:00,  9.98it/s]\n",
            "\u001b[1;36m(EngineCore_DP0 pid=37094)\u001b[0;0m INFO 11-28 22:22:26 [gpu_model_runner.py:4244] Graph capturing finished in 10 secs, took 0.54 GiB\n",
            "\u001b[1;36m(EngineCore_DP0 pid=37094)\u001b[0;0m INFO 11-28 22:22:26 [core.py:250] init engine (profile, create kv cache, warmup model) took 30.30 seconds\n",
            "INFO 11-28 22:22:27 [llm.py:352] Supported tasks: ['generate']\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "Running 3 inference passes...\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/test_llmcompressor_awq_vllm.py\", line 198, in <module>\n",
            "    LLM_inference(image)\n",
            "  File \"/content/test_llmcompressor_awq_vllm.py\", line 175, in LLM_inference\n",
            "    results = benchmark_vllm(llm, processor, image)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/test_llmcompressor_awq_vllm.py\", line 144, in benchmark_vllm\n",
            "    result = run_inference_vllm(llm, processor, image)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/test_llmcompressor_awq_vllm.py\", line 110, in run_inference_vllm\n",
            "    outputs = llm.generate(\n",
            "              ^^^^^^^^^^^^^\n",
            "TypeError: LLM.generate() got an unexpected keyword argument 'multi_modal_data'\n",
            "[rank0]:[W1128 22:22:31.348204500 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
            "ERROR 11-28 22:22:32 [core_client.py:598] Engine core proc EngineCore_DP0 died unexpectedly, shutting down client.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Batch Test"
      ],
      "metadata": {
        "id": "BOpk631vetP2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python test_baseline_batch.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6AVaFYccexo2",
        "outputId": "ed0a4dba-25d8-4b2b-fac7-3cdf49c9615c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-29 04:35:33.272756: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-29 04:35:33.292752: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764390933.314527   52889 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764390933.320985   52889 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764390933.338333   52889 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764390933.338370   52889 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764390933.338373   52889 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764390933.338375   52889 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-29 04:35:33.343392: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Loading sample image...\n",
            "Image size: (1000, 667)\n",
            "\n",
            "================================================================================\n",
            "Baseline_without_kv_cache Test for LLaVA\n",
            "================================================================================\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Loading checkpoint shards: 100% 3/3 [00:04<00:00,  1.41s/it]\n",
            "Model loaded.\n",
            "\n",
            "==============================\n",
            "  Batch Size Stress Test (true batch forward)\n",
            "==============================\n",
            "\n",
            "--- Batch Size = 4 (1 forward pass with 4 images) ---\n",
            " ==> BS=4: latency=8.535s, throughput=325.23 tok/s, max mem=14.57 GB\n",
            "\n",
            "--- Batch Size = 8 (1 forward pass with 8 images) ---\n",
            " ==> BS=8: latency=10.620s, throughput=522.77 tok/s, max mem=15.98 GB\n",
            "\n",
            "--- Batch Size = 16 (1 forward pass with 16 images) ---\n",
            " ==> BS=16: latency=15.916s, throughput=697.68 tok/s, max mem=18.79 GB\n",
            "\n",
            "--- Batch Size = 32 (1 forward pass with 32 images) ---\n",
            "[ERROR] BS=32 failed: CUDA out of memory. Tried to allocate 400.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 203.38 MiB is free. Process 767154 has 21.96 GiB memory in use. Of the allocated memory 19.66 GiB is allocated by PyTorch, and 2.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python test_flash_attn_batch.py"
      ],
      "metadata": {
        "id": "VjY4U_Ug5m-9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d890b588-23ae-46fa-e12b-0a3a9afa3a30"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-29 17:56:21.097214: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-29 17:56:21.114397: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764438981.135954    1769 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764438981.142517    1769 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764438981.158750    1769 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764438981.158782    1769 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764438981.158785    1769 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764438981.158788    1769 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-29 17:56:21.163575: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Loading sample image...\n",
            "Image size: (1000, 667)\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "TEST 2: With Flash Attention 2 (Batch Test)\n",
            "--------------------------------------------------------------------------------\n",
            "processor_config.json: 100% 173/173 [00:00<00:00, 1.12MB/s]\n",
            "chat_template.json: 100% 701/701 [00:00<00:00, 7.56MB/s]\n",
            "chat_template.jinja: 100% 674/674 [00:00<00:00, 7.27MB/s]\n",
            "preprocessor_config.json: 100% 505/505 [00:00<00:00, 4.81MB/s]\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "tokenizer_config.json: 1.45kB [00:00, 7.91MB/s]\n",
            "tokenizer.model: 100% 500k/500k [00:01<00:00, 288kB/s]\n",
            "tokenizer.json: 3.62MB [00:00, 218MB/s]\n",
            "added_tokens.json: 100% 41.0/41.0 [00:00<00:00, 516kB/s]\n",
            "special_tokens_map.json: 100% 552/552 [00:00<00:00, 6.73MB/s]\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "config.json: 100% 950/950 [00:00<00:00, 9.84MB/s]\n",
            "model.safetensors.index.json: 70.1kB [00:00, 101MB/s]\n",
            "Fetching 3 files:   0% 0/3 [00:00<?, ?it/s]\n",
            "model-00003-of-00003.safetensors:   0% 0.00/4.18G [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:   0% 0.00/4.99G [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:   0% 0.00/4.96G [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:   0% 8.13k/4.99G [00:01<262:11:09, 5.29kB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:   0% 2.65M/4.99G [00:01<38:20, 2.17MB/s]    \u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:   0% 10.6M/4.99G [00:01<08:38, 9.61MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:   0% 599k/4.96G [00:01<4:35:04, 300kB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:   1% 44.1M/4.99G [00:04<07:44, 10.7MB/s]\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:   0% 546k/4.18G [00:04<10:27:55, 111kB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:   1% 67.7M/4.96G [00:04<05:14, 15.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  14% 604M/4.18G [00:05<00:21, 169MB/s]   \u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:   2% 86.2M/4.99G [00:05<03:37, 22.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:   4% 202M/4.96G [00:06<01:54, 41.5MB/s] \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:   2% 120M/4.99G [00:06<03:01, 26.9MB/s] \u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:   3% 149M/4.99G [00:06<02:29, 32.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:   5% 270M/4.96G [00:09<02:19, 33.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  22% 940M/4.18G [00:09<00:28, 113MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  10% 472M/4.96G [00:09<00:59, 75.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  11% 539M/4.96G [00:09<00:52, 83.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:   6% 283M/4.99G [00:10<02:02, 38.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:   8% 380M/4.99G [00:17<03:38, 21.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  12% 606M/4.96G [00:17<02:41, 27.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  16% 807M/4.96G [00:18<01:18, 52.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  10% 485M/4.99G [00:18<02:19, 32.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  11% 552M/4.99G [00:18<01:47, 41.3MB/s]\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  27% 1.14G/4.18G [00:18<00:58, 51.9MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  12% 613M/4.99G [00:19<01:31, 48.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  20% 1.01G/4.96G [00:19<00:56, 69.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  14% 680M/4.99G [00:19<01:19, 54.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  24% 1.21G/4.96G [00:20<00:39, 94.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  15% 747M/4.99G [00:20<01:11, 59.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  26% 1.28G/4.96G [00:21<00:37, 98.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  27% 1.34G/4.96G [00:21<00:35, 102MB/s] \u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  31% 1.27G/4.18G [00:21<00:57, 50.5MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  16% 814M/4.99G [00:21<01:04, 64.5MB/s]\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  32% 1.34G/4.18G [00:21<00:50, 55.8MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  19% 943M/4.99G [00:21<00:39, 102MB/s] \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  30% 1.48G/4.96G [00:22<00:28, 121MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  20% 1.01G/4.99G [00:22<00:40, 99.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  31% 1.54G/4.96G [00:22<00:26, 128MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  34% 1.41G/4.18G [00:23<00:48, 56.5MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  22% 1.07G/4.99G [00:23<00:35, 111MB/s] \u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  35% 1.48G/4.18G [00:23<00:43, 61.8MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  23% 1.14G/4.99G [00:23<00:38, 99.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  24% 1.21G/4.99G [00:24<00:32, 116MB/s] \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  35% 1.75G/4.96G [00:24<00:25, 127MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  39% 1.61G/4.18G [00:24<00:35, 71.4MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  27% 1.33G/4.99G [00:25<00:31, 116MB/s]\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  42% 1.74G/4.18G [00:25<00:27, 89.0MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  28% 1.40G/4.99G [00:29<01:22, 43.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  37% 1.81G/4.96G [00:29<01:07, 46.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  43% 1.81G/4.18G [00:30<00:50, 46.7MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  39% 1.95G/4.96G [00:30<00:44, 68.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  29% 1.47G/4.99G [00:30<01:05, 53.7MB/s]\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  45% 1.88G/4.18G [00:30<00:43, 52.2MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  31% 1.53G/4.99G [00:31<00:57, 59.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  41% 2.02G/4.96G [00:31<00:45, 64.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  48% 2.01G/4.18G [00:34<00:48, 44.9MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  42% 2.08G/4.96G [00:34<01:05, 43.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  32% 1.60G/4.99G [00:35<01:39, 34.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  36% 1.80G/4.99G [00:35<00:46, 69.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  43% 2.15G/4.96G [00:35<00:56, 49.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  51% 2.15G/4.18G [00:35<00:35, 57.5MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  38% 1.87G/4.99G [00:36<00:42, 73.6MB/s]\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  53% 2.21G/4.18G [00:36<00:33, 58.3MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  39% 1.94G/4.99G [00:37<00:40, 76.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  45% 2.22G/4.96G [00:37<00:57, 47.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  55% 2.28G/4.18G [00:37<00:30, 62.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  46% 2.28G/4.96G [00:38<00:52, 51.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  40% 2.01G/4.99G [00:38<00:43, 68.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  49% 2.42G/4.96G [00:38<00:30, 83.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  43% 2.14G/4.99G [00:38<00:26, 108MB/s] \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  51% 2.55G/4.96G [00:38<00:19, 125MB/s] \u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  56% 2.35G/4.18G [00:38<00:31, 57.5MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  44% 2.21G/4.99G [00:38<00:23, 118MB/s]\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  58% 2.41G/4.18G [00:39<00:27, 64.4MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  46% 2.27G/4.99G [00:39<00:25, 105MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  53% 2.62G/4.96G [00:39<00:24, 94.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  59% 2.48G/4.18G [00:40<00:22, 73.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  54% 2.69G/4.96G [00:40<00:21, 104MB/s] \u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  63% 2.62G/4.18G [00:41<00:17, 89.3MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  57% 2.82G/4.96G [00:42<00:28, 76.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  66% 2.75G/4.18G [00:43<00:19, 74.1MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  47% 2.34G/4.99G [00:46<01:25, 31.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  50% 2.48G/4.99G [00:46<00:47, 53.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  58% 2.89G/4.96G [00:46<00:45, 45.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  67% 2.82G/4.18G [00:46<00:27, 50.0MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  61% 3.02G/4.96G [00:46<00:27, 70.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  71% 2.95G/4.18G [00:46<00:16, 76.3MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  51% 2.54G/4.99G [00:46<00:40, 61.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  62% 3.09G/4.96G [00:46<00:22, 82.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  72% 3.02G/4.18G [00:47<00:13, 87.0MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  64% 3.15G/4.96G [00:49<00:31, 57.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  54% 2.68G/4.99G [00:52<00:59, 38.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  65% 3.22G/4.96G [00:52<00:43, 39.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  74% 3.09G/4.18G [00:52<00:30, 35.8MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  55% 2.74G/4.99G [00:52<00:47, 47.2MB/s]\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  75% 3.15G/4.18G [00:52<00:22, 45.9MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  56% 2.81G/4.99G [00:52<00:37, 58.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  66% 3.29G/4.96G [00:53<00:35, 47.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  58% 2.88G/4.99G [00:53<00:31, 67.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  59% 2.95G/4.99G [00:54<00:26, 76.4MB/s]\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  79% 3.29G/4.18G [00:54<00:15, 59.2MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  68% 3.36G/4.96G [00:54<00:31, 51.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  80% 3.35G/4.18G [00:54<00:12, 68.4MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  69% 3.42G/4.96G [00:54<00:24, 62.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  60% 3.02G/4.99G [00:54<00:24, 80.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  70% 3.49G/4.96G [00:54<00:18, 78.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  82% 3.42G/4.18G [00:55<00:09, 79.9MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  72% 3.56G/4.96G [00:55<00:13, 105MB/s] \u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  84% 3.49G/4.18G [00:55<00:07, 96.1MB/s]\u001b[A\n",
            "model-00003-of-00003.safetensors:  85% 3.55G/4.18G [00:55<00:05, 116MB/s] \u001b[A\n",
            "model-00003-of-00003.safetensors:  87% 3.62G/4.18G [00:56<00:04, 118MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  62% 3.08G/4.99G [00:56<00:27, 69.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  73% 3.62G/4.96G [00:56<00:15, 86.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  63% 3.15G/4.99G [00:56<00:20, 89.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  74% 3.69G/4.96G [00:56<00:12, 98.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  88% 3.69G/4.18G [00:56<00:04, 113MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  76% 3.76G/4.96G [00:56<00:09, 123MB/s] \u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  90% 3.76G/4.18G [00:57<00:03, 134MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  64% 3.22G/4.99G [00:57<00:19, 89.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  66% 3.29G/4.99G [00:57<00:19, 87.4MB/s]\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  92% 3.82G/4.18G [00:58<00:04, 79.7MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  79% 3.89G/4.96G [00:58<00:11, 91.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  67% 3.35G/4.99G [00:59<00:25, 64.4MB/s]\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  92% 3.84G/4.18G [01:00<00:06, 49.1MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  69% 3.42G/4.99G [01:02<00:39, 39.8MB/s]\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  94% 3.91G/4.18G [01:02<00:07, 37.9MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  70% 3.49G/4.99G [01:03<00:27, 54.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  80% 3.96G/4.96G [01:03<00:24, 41.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  74% 3.69G/4.99G [01:04<00:16, 79.0MB/s]\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  95% 3.97G/4.18G [01:05<00:06, 33.0MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  81% 4.03G/4.96G [01:06<00:27, 34.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  83% 4.09G/4.96G [01:06<00:19, 43.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  84% 4.16G/4.96G [01:06<00:14, 54.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  97% 4.04G/4.18G [01:07<00:03, 35.6MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  85% 4.23G/4.96G [01:07<00:10, 70.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors:  98% 4.11G/4.18G [01:07<00:01, 43.7MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  75% 3.76G/4.99G [01:07<00:24, 49.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  77% 3.82G/4.99G [01:08<00:19, 61.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  87% 4.29G/4.96G [01:08<00:09, 68.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors: 100% 4.18G/4.18G [01:08<00:00, 60.9MB/s]\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  79% 3.96G/4.99G [01:08<00:11, 88.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  81% 4.02G/4.99G [01:09<00:10, 95.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  82% 4.09G/4.99G [01:09<00:07, 115MB/s] \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  88% 4.36G/4.96G [01:09<00:09, 60.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  89% 4.42G/4.96G [01:09<00:06, 79.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  91% 4.49G/4.96G [01:09<00:04, 102MB/s] \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  92% 4.56G/4.96G [01:10<00:03, 127MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  93% 4.62G/4.96G [01:10<00:02, 155MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  83% 4.16G/4.99G [01:10<00:09, 87.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  95% 4.69G/4.96G [01:10<00:01, 156MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  96% 4.76G/4.96G [01:11<00:01, 184MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  85% 4.23G/4.99G [01:11<00:08, 88.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  97% 4.82G/4.96G [01:11<00:00, 167MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  99% 4.89G/4.96G [01:11<00:00, 193MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors: 100% 4.96G/4.96G [01:11<00:00, 68.9MB/s]\n",
            "\n",
            "\n",
            "model-00001-of-00003.safetensors:  87% 4.36G/4.99G [01:12<00:06, 105MB/s] \u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  90% 4.49G/4.99G [01:12<00:03, 155MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  93% 4.63G/4.99G [01:12<00:01, 208MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  94% 4.69G/4.99G [01:13<00:01, 223MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  95% 4.76G/4.99G [01:13<00:00, 238MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  97% 4.86G/4.99G [01:13<00:00, 220MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors:  99% 4.93G/4.99G [01:13<00:00, 238MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00003.safetensors: 100% 4.99G/4.99G [01:14<00:00, 67.3MB/s]\n",
            "Fetching 3 files: 100% 3/3 [01:14<00:00, 24.95s/it]\n",
            "Loading checkpoint shards: 100% 3/3 [00:04<00:00,  1.56s/it]\n",
            "generation_config.json: 100% 141/141 [00:00<00:00, 1.66MB/s]\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "\n",
            "==============================\n",
            "  Batch Size Stress Test (true batch forward)\n",
            "==============================\n",
            "\n",
            "--- Batch Size = 4 (1 forward pass with 4 images) ---\n",
            " ==> BS=4: latency=10.305s, throughput=269.39 tok/s, max mem=14.57 GB\n",
            "\n",
            "--- Batch Size = 8 (1 forward pass with 8 images) ---\n",
            " ==> BS=8: latency=10.537s, throughput=526.93 tok/s, max mem=15.98 GB\n",
            "\n",
            "--- Batch Size = 16 (1 forward pass with 16 images) ---\n",
            " ==> BS=16: latency=15.637s, throughput=710.11 tok/s, max mem=18.79 GB\n",
            "\n",
            "--- Batch Size = 32 (1 forward pass with 32 images) ---\n",
            "[ERROR] BS=32 failed: CUDA out of memory. Tried to allocate 400.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 203.38 MiB is free. Process 37535 has 21.96 GiB memory in use. Of the allocated memory 19.66 GiB is allocated by PyTorch, and 2.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n",
            "Flash Attention 2 Batch Results:\n",
            "  BS=4: Latency=10.305s, Throughput=269.39 tok/s, Memory=14.57 GB\n",
            "  BS=8: Latency=10.537s, Throughput=526.93 tok/s, Memory=15.98 GB\n",
            "  BS=16: Latency=15.637s, Throughput=710.11 tok/s, Memory=18.79 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python test_kv_batch.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ed8pCHFDwlXy",
        "outputId": "63f89a1c-efca-4663-a218-e6ced9404baf"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-29 19:44:04.630936: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-29 19:44:04.648911: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764445444.670993   30709 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764445444.677609   30709 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764445444.694920   30709 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764445444.694957   30709 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764445444.694960   30709 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764445444.694962   30709 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-29 19:44:04.699861: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\n",
            "Using quantile for Pruning:\n",
            "Loading sample image...\n",
            "Image size: (1000, 667)\n",
            "\n",
            "================================================================================\n",
            "Baseline KV_cache Batch Test for LLaVA\n",
            "================================================================================\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Loading checkpoint shards: 100% 3/3 [00:04<00:00,  1.40s/it]\n",
            "Model loaded.\n",
            "\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "\n",
            "==============================\n",
            "  Batch Size Stress Test (tensorwise)\n",
            "==============================\n",
            "\n",
            "--- Batch Size = 4 ---\n",
            " ==> BS=4, latency=7.666s, throughput=361.09 tok/s, max_mem=14.71 GB\n",
            "\n",
            "--- Batch Size = 8 ---\n",
            " ==> BS=8, latency=9.447s, throughput=586.00 tok/s, max_mem=16.24 GB\n",
            "\n",
            "--- Batch Size = 16 ---\n",
            " ==> BS=16, latency=13.069s, throughput=847.20 tok/s, max_mem=19.32 GB\n",
            "\n",
            "--- Batch Size = 32 ---\n",
            "[ERROR] BS=32 failed: CUDA out of memory. Tried to allocate 400.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 75.38 MiB is free. Process 441373 has 22.08 GiB memory in use. Of the allocated memory 19.63 GiB is allocated by PyTorch, and 2.23 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n",
            "Baseline KV Cache Batch Results:\n",
            "  BS=4: Latency=7.666s, Throughput=361.09 tok/s, Memory=14.71 GB\n",
            "  BS=8: Latency=9.447s, Throughput=586.00 tok/s, Memory=16.24 GB\n",
            "  BS=16: Latency=13.069s, Throughput=847.20 tok/s, Memory=19.32 GB\n",
            "\n",
            "\n",
            "Using Topk for Pruning:\n",
            "Loading sample image...\n",
            "Image size: (1000, 667)\n",
            "\n",
            "================================================================================\n",
            "Baseline KV_cache Batch Test for LLaVA\n",
            "================================================================================\n",
            "Loading checkpoint shards: 100% 3/3 [00:04<00:00,  1.41s/it]\n",
            "Model loaded.\n",
            "\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "\n",
            "==============================\n",
            "  Batch Size Stress Test (tensorwise)\n",
            "==============================\n",
            "\n",
            "--- Batch Size = 4 ---\n",
            " ==> BS=4, latency=7.601s, throughput=364.17 tok/s, max_mem=14.71 GB\n",
            "\n",
            "--- Batch Size = 8 ---\n",
            " ==> BS=8, latency=9.484s, throughput=583.71 tok/s, max_mem=16.24 GB\n",
            "\n",
            "--- Batch Size = 16 ---\n",
            " ==> BS=16, latency=13.097s, throughput=845.39 tok/s, max_mem=19.32 GB\n",
            "\n",
            "--- Batch Size = 32 ---\n",
            "[ERROR] BS=32 failed: CUDA out of memory. Tried to allocate 400.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 235.38 MiB is free. Process 441373 has 21.93 GiB memory in use. Of the allocated memory 19.63 GiB is allocated by PyTorch, and 2.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n",
            "Baseline KV Cache Batch Results:\n",
            "  BS=4: Latency=7.601s, Throughput=364.17 tok/s, Memory=14.71 GB\n",
            "  BS=8: Latency=9.484s, Throughput=583.71 tok/s, Memory=16.24 GB\n",
            "  BS=16: Latency=13.097s, Throughput=845.39 tok/s, Memory=19.32 GB\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python test_kv_prune_batch.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G98BTrrIxdAg",
        "outputId": "e3eae8c5-637e-49b0-81a1-5cc92ebbfd36"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-29 19:48:00.423898: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-29 19:48:00.441177: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764445680.462307   31755 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764445680.468659   31755 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764445680.484706   31755 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764445680.484734   31755 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764445680.484737   31755 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764445680.484739   31755 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-29 19:48:00.489402: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\n",
            "Using quantile for Pruning:\n",
            "Loading sample image...\n",
            "Image size: (1000, 667)\n",
            "\n",
            "================================================================================\n",
            "Tensor-wise PRUNING Only KV_cache Batch Test for LLaVA\n",
            "================================================================================\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Loading checkpoint shards: 100% 3/3 [00:04<00:00,  1.39s/it]\n",
            "Model loaded.\n",
            "\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "\n",
            "==============================\n",
            "  Batch Size Stress Test (tensorwise)\n",
            "==============================\n",
            "\n",
            "--- Batch Size = 4 ---\n",
            " ==> BS=4, latency=7.511s, throughput=367.48 tok/s, max_mem=17.04 GB\n",
            "\n",
            "--- Batch Size = 8 ---\n",
            "[ERROR] BS=8 failed: quantile() input tensor is too large\n",
            "\n",
            "--- Batch Size = 16 ---\n",
            "[ERROR] BS=16 failed: CUDA out of memory. Tried to allocate 458.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 205.38 MiB is free. Process 455548 has 21.96 GiB memory in use. Of the allocated memory 19.47 GiB is allocated by PyTorch, and 2.26 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n",
            "--- Batch Size = 32 ---\n",
            "[ERROR] BS=32 failed: CUDA out of memory. Tried to allocate 400.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 75.38 MiB is free. Process 455548 has 22.08 GiB memory in use. Of the allocated memory 19.63 GiB is allocated by PyTorch, and 2.23 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n",
            "Tensor-wise Pruning KV Cache Batch Results:\n",
            "  BS=4: Latency=7.511s, Throughput=367.48 tok/s, Memory=17.04 GB\n",
            "\n",
            "\n",
            "Using Topk for Pruning:\n",
            "Loading sample image...\n",
            "Image size: (1000, 667)\n",
            "\n",
            "================================================================================\n",
            "Tensor-wise PRUNING Only KV_cache Batch Test for LLaVA\n",
            "================================================================================\n",
            "Loading checkpoint shards: 100% 3/3 [00:04<00:00,  1.40s/it]\n",
            "Model loaded.\n",
            "\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "\n",
            "==============================\n",
            "  Batch Size Stress Test (tensorwise)\n",
            "==============================\n",
            "\n",
            "--- Batch Size = 4 ---\n",
            " ==> BS=4, latency=7.487s, throughput=368.62 tok/s, max_mem=17.04 GB\n",
            "\n",
            "--- Batch Size = 8 ---\n",
            "[ERROR] BS=8 failed: CUDA out of memory. Tried to allocate 38.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 33.38 MiB is free. Process 455548 has 22.12 GiB memory in use. Of the allocated memory 19.16 GiB is allocated by PyTorch, and 2.73 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n",
            "--- Batch Size = 16 ---\n",
            "[ERROR] BS=16 failed: CUDA out of memory. Tried to allocate 138.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 51.38 MiB is free. Process 455548 has 22.11 GiB memory in use. Of the allocated memory 19.99 GiB is allocated by PyTorch, and 1.89 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n",
            "--- Batch Size = 32 ---\n",
            "[ERROR] BS=32 failed: CUDA out of memory. Tried to allocate 400.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 85.38 MiB is free. Process 455548 has 22.07 GiB memory in use. Of the allocated memory 19.63 GiB is allocated by PyTorch, and 2.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n",
            "Tensor-wise Pruning KV Cache Batch Results:\n",
            "  BS=4: Latency=7.487s, Throughput=368.62 tok/s, Memory=17.04 GB\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python test_kv_prune_quant_batch.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i6DnoWQxxux2",
        "outputId": "bf8fb0c0-a9ba-49a2-998f-67ba6af600ad"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-29 19:54:17.089615: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-29 19:54:17.107095: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764446057.128221   33395 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764446057.134711   33395 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764446057.151065   33395 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764446057.151098   33395 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764446057.151101   33395 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764446057.151104   33395 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-29 19:54:17.156102: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\n",
            "Using quantile for Pruning:\n",
            "Loading sample image...\n",
            "Image size: (1000, 667)\n",
            "\n",
            "================================================================================\n",
            "Tensor-wise KV Pruning + Quantization Batch Test for LLaVA\n",
            "================================================================================\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Loading checkpoint shards: 100% 3/3 [00:04<00:00,  1.40s/it]\n",
            "Model loaded.\n",
            "\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "\n",
            "==============================\n",
            "  Batch Size Stress Test (tensorwise)\n",
            "==============================\n",
            "\n",
            "--- Batch Size = 4 ---\n",
            " ==> BS=4, latency=7.599s, throughput=364.27 tok/s, max_mem=17.66 GB\n",
            "\n",
            "--- Batch Size = 8 ---\n",
            "[ERROR] BS=8 failed: quantile() input tensor is too large\n",
            "\n",
            "--- Batch Size = 16 ---\n",
            "[ERROR] BS=16 failed: CUDA out of memory. Tried to allocate 458.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 205.38 MiB is free. Process 478002 has 21.96 GiB memory in use. Of the allocated memory 19.47 GiB is allocated by PyTorch, and 2.26 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n",
            "--- Batch Size = 32 ---\n",
            "[ERROR] BS=32 failed: CUDA out of memory. Tried to allocate 400.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 75.38 MiB is free. Process 478002 has 22.08 GiB memory in use. Of the allocated memory 19.63 GiB is allocated by PyTorch, and 2.23 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n",
            "Tensor-wise Prune + Quant KV Batch Results:\n",
            "  BS=4: Latency=7.599s, Throughput=364.27 tok/s, Memory=17.66 GB\n",
            "\n",
            "\n",
            "Using Topk for Pruning:\n",
            "Loading sample image...\n",
            "Image size: (1000, 667)\n",
            "\n",
            "================================================================================\n",
            "Tensor-wise KV Pruning + Quantization Batch Test for LLaVA\n",
            "================================================================================\n",
            "Loading checkpoint shards: 100% 3/3 [00:04<00:00,  1.40s/it]\n",
            "Model loaded.\n",
            "\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "\n",
            "==============================\n",
            "  Batch Size Stress Test (tensorwise)\n",
            "==============================\n",
            "\n",
            "--- Batch Size = 4 ---\n",
            " ==> BS=4, latency=7.618s, throughput=363.34 tok/s, max_mem=17.66 GB\n",
            "\n",
            "--- Batch Size = 8 ---\n",
            "[ERROR] BS=8 failed: CUDA out of memory. Tried to allocate 38.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 29.38 MiB is free. Process 478002 has 22.13 GiB memory in use. Of the allocated memory 20.25 GiB is allocated by PyTorch, and 1.65 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n",
            "--- Batch Size = 16 ---\n",
            "[ERROR] BS=16 failed: CUDA out of memory. Tried to allocate 138.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 51.38 MiB is free. Process 478002 has 22.11 GiB memory in use. Of the allocated memory 19.99 GiB is allocated by PyTorch, and 1.89 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n",
            "--- Batch Size = 32 ---\n",
            "[ERROR] BS=32 failed: CUDA out of memory. Tried to allocate 400.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 85.38 MiB is free. Process 478002 has 22.07 GiB memory in use. Of the allocated memory 19.63 GiB is allocated by PyTorch, and 2.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n",
            "Tensor-wise Prune + Quant KV Batch Results:\n",
            "  BS=4: Latency=7.618s, Throughput=363.34 tok/s, Memory=17.66 GB\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python test_kv_quant_batch.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRQqn2xh_EgX",
        "outputId": "37680386-884c-4a1a-8515-353907982692"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-29 19:56:57.273263: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-29 19:56:57.290730: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764446217.311830   34117 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764446217.318255   34117 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764446217.334522   34117 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764446217.334551   34117 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764446217.334554   34117 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764446217.334556   34117 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-29 19:56:57.339383: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\n",
            "Using quantile for Pruning:\n",
            "Loading sample image...\n",
            "Image size: (1000, 667)\n",
            "\n",
            "================================================================================\n",
            "Tensor-wise Quantization-Only KV_cache Batch Test for LLaVA\n",
            "================================================================================\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Loading checkpoint shards: 100% 3/3 [00:04<00:00,  1.40s/it]\n",
            "Model loaded.\n",
            "\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "\n",
            "==============================\n",
            "  Batch Size Stress Test (tensorwise)\n",
            "==============================\n",
            "\n",
            "--- Batch Size = 4 ---\n",
            " ==> BS=4, latency=7.739s, throughput=357.67 tok/s, max_mem=17.65 GB\n",
            "\n",
            "--- Batch Size = 8 ---\n",
            "[ERROR] BS=8 failed: CUDA out of memory. Tried to allocate 38.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 33.38 MiB is free. Process 488225 has 22.12 GiB memory in use. Of the allocated memory 21.59 GiB is allocated by PyTorch, and 309.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n",
            "--- Batch Size = 16 ---\n",
            "[ERROR] BS=16 failed: CUDA out of memory. Tried to allocate 150.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 57.38 MiB is free. Process 488225 has 22.10 GiB memory in use. Of the allocated memory 20.92 GiB is allocated by PyTorch, and 978.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n",
            "--- Batch Size = 32 ---\n",
            "[ERROR] BS=32 failed: CUDA out of memory. Tried to allocate 400.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 225.38 MiB is free. Process 488225 has 21.94 GiB memory in use. Of the allocated memory 19.63 GiB is allocated by PyTorch, and 2.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n",
            "Tensor-wise KV Quantization Batch Results:\n",
            "  BS=4: Latency=7.739s, Throughput=357.67 tok/s, Memory=17.65 GB\n",
            "\n",
            "\n",
            "Using Topk for Pruning:\n",
            "Loading sample image...\n",
            "Image size: (1000, 667)\n",
            "\n",
            "================================================================================\n",
            "Tensor-wise Quantization-Only KV_cache Batch Test for LLaVA\n",
            "================================================================================\n",
            "Loading checkpoint shards: 100% 3/3 [00:04<00:00,  1.41s/it]\n",
            "Model loaded.\n",
            "\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "\n",
            "==============================\n",
            "  Batch Size Stress Test (tensorwise)\n",
            "==============================\n",
            "\n",
            "--- Batch Size = 4 ---\n",
            " ==> BS=4, latency=7.666s, throughput=361.06 tok/s, max_mem=17.65 GB\n",
            "\n",
            "--- Batch Size = 8 ---\n",
            "[ERROR] BS=8 failed: CUDA out of memory. Tried to allocate 38.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 5.38 MiB is free. Process 488225 has 22.15 GiB memory in use. Of the allocated memory 21.63 GiB is allocated by PyTorch, and 299.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n",
            "--- Batch Size = 16 ---\n",
            "[ERROR] BS=16 failed: CUDA out of memory. Tried to allocate 150.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 67.38 MiB is free. Process 488225 has 22.09 GiB memory in use. Of the allocated memory 20.92 GiB is allocated by PyTorch, and 967.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n",
            "--- Batch Size = 32 ---\n",
            "[ERROR] BS=32 failed: CUDA out of memory. Tried to allocate 400.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 235.38 MiB is free. Process 488225 has 21.93 GiB memory in use. Of the allocated memory 19.63 GiB is allocated by PyTorch, and 2.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n",
            "Tensor-wise KV Quantization Batch Results:\n",
            "  BS=4: Latency=7.666s, Throughput=361.06 tok/s, Memory=17.65 GB\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python int8_tensor_prune_quant_batch.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m9otAnG4An_M",
        "outputId": "fe54c7ac-69bd-46dc-c738-ea74b380dbb6"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-29 19:58:21.044983: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-29 19:58:21.062224: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764446301.083540   34525 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764446301.089912   34525 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764446301.105898   34525 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764446301.105925   34525 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764446301.105928   34525 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764446301.105931   34525 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-29 19:58:21.110621: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\n",
            "Using quantile for Pruning:\n",
            "Loading sample image...\n",
            "Image size: (1000, 667)\n",
            "\n",
            "================================================================================\n",
            "Method 1: INT8 + Tensor-wise KV Pruning + KV Quantization (Batch Test)\n",
            "================================================================================\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "Loading checkpoint shards: 100% 3/3 [00:16<00:00,  5.51s/it]\n",
            "INT8 model loaded (bitsandbytes).\n",
            "\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "\n",
            "==============================\n",
            "  Batch Size Stress Test (tensorwise)\n",
            "==============================\n",
            "\n",
            "--- Batch Size = 4 ---\n",
            " ==> BS=4, latency=2.308s, throughput=1048.53 tok/s, max_mem=11.16 GB\n",
            "\n",
            "--- Batch Size = 8 ---\n",
            "[ERROR] BS=8 failed: quantile() input tensor is too large\n",
            "\n",
            "--- Batch Size = 16 ---\n",
            "[ERROR] BS=16 failed: quantile() input tensor is too large\n",
            "\n",
            "--- Batch Size = 32 ---\n",
            "[ERROR] BS=32 failed: CUDA out of memory. Tried to allocate 594.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 449.38 MiB is free. Process 493634 has 21.72 GiB memory in use. Of the allocated memory 18.31 GiB is allocated by PyTorch, and 3.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n",
            "Method 1 Batch Results (INT8 + Tensor-wise Prune + KV Quant):\n",
            "  BS=4: Latency=2.308s, Throughput=1048.53 tok/s, Memory=11.16 GB\n",
            "\n",
            "\n",
            "Using Topk for Pruning:\n",
            "Loading sample image...\n",
            "Image size: (1000, 667)\n",
            "\n",
            "================================================================================\n",
            "Method 1: INT8 + Tensor-wise KV Pruning + KV Quantization (Batch Test)\n",
            "================================================================================\n",
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "Loading checkpoint shards: 100% 3/3 [00:16<00:00,  5.47s/it]\n",
            "INT8 model loaded (bitsandbytes).\n",
            "\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "\n",
            "==============================\n",
            "  Batch Size Stress Test (tensorwise)\n",
            "==============================\n",
            "\n",
            "--- Batch Size = 4 ---\n",
            " ==> BS=4, latency=2.286s, throughput=1058.70 tok/s, max_mem=11.16 GB\n",
            "\n",
            "--- Batch Size = 8 ---\n",
            " ==> BS=8, latency=11.707s, throughput=445.54 tok/s, max_mem=15.64 GB\n",
            "\n",
            "--- Batch Size = 16 ---\n",
            "[ERROR] BS=16 failed: CUDA out of memory. Tried to allocate 76.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 71.38 MiB is free. Process 493634 has 22.09 GiB memory in use. Of the allocated memory 19.60 GiB is allocated by PyTorch, and 2.25 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n",
            "--- Batch Size = 32 ---\n",
            "[ERROR] BS=32 failed: CUDA out of memory. Tried to allocate 276.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 159.38 MiB is free. Process 493634 has 22.00 GiB memory in use. Of the allocated memory 19.35 GiB is allocated by PyTorch, and 2.41 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n",
            "Method 1 Batch Results (INT8 + Tensor-wise Prune + KV Quant):\n",
            "  BS=4: Latency=2.286s, Throughput=1058.70 tok/s, Memory=11.16 GB\n",
            "  BS=8: Latency=11.707s, Throughput=445.54 tok/s, Memory=15.64 GB\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python int4_tensor_prune_quant_batch.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dO0cJsJcBIGb",
        "outputId": "e8cb127c-08b5-4302-9ab7-0833140306d1"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-29 20:01:43.438314: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-29 20:01:43.455898: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764446503.477069   35498 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764446503.483449   35498 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764446503.499772   35498 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764446503.499800   35498 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764446503.499803   35498 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764446503.499805   35498 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-29 20:01:43.504602: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\n",
            "Using quantile for Pruning:\n",
            "================================================================================\n",
            "Method 1: INT4 + Tensor-wise KV Pruning + KV Quantization (Batch Test)\n",
            "================================================================================\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "Loading checkpoint shards: 100% 3/3 [00:16<00:00,  5.51s/it]\n",
            "INT4 model loaded (bitsandbytes).\n",
            "\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "\n",
            "==============================\n",
            "  Batch Size Stress Test (tensorwise)\n",
            "==============================\n",
            "\n",
            "--- Batch Size = 4 ---\n",
            " ==> BS=4, latency=35.215s, throughput=78.60 tok/s, max_mem=8.79 GB\n",
            "\n",
            "--- Batch Size = 8 ---\n",
            "[ERROR] BS=8 failed: quantile() input tensor is too large\n",
            "\n",
            "--- Batch Size = 16 ---\n",
            "[ERROR] BS=16 failed: quantile() input tensor is too large\n",
            "\n",
            "--- Batch Size = 32 ---\n",
            "[ERROR] BS=32 failed: quantile() input tensor is too large\n",
            "\n",
            "Method 1 Batch Results (INT4 + Tensor-wise Prune + KV Quant):\n",
            "  BS=4: Latency=35.215s, Throughput=78.60 tok/s, Memory=8.79 GB\n",
            "\n",
            "\n",
            "Using Topk for Pruning:\n",
            "================================================================================\n",
            "Method 1: INT4 + Tensor-wise KV Pruning + KV Quantization (Batch Test)\n",
            "================================================================================\n",
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "Loading checkpoint shards: 100% 3/3 [00:16<00:00,  5.63s/it]\n",
            "INT4 model loaded (bitsandbytes).\n",
            "\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "\n",
            "==============================\n",
            "  Batch Size Stress Test (tensorwise)\n",
            "==============================\n",
            "\n",
            "--- Batch Size = 4 ---\n",
            " ==> BS=4, latency=35.214s, throughput=78.60 tok/s, max_mem=8.78 GB\n",
            "\n",
            "--- Batch Size = 8 ---\n",
            " ==> BS=8, latency=38.420s, throughput=144.09 tok/s, max_mem=13.24 GB\n",
            "\n",
            "--- Batch Size = 16 ---\n",
            "[ERROR] BS=16 failed: CUDA out of memory. Tried to allocate 76.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 71.38 MiB is free. Process 508161 has 22.09 GiB memory in use. Of the allocated memory 19.94 GiB is allocated by PyTorch, and 1.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n",
            "--- Batch Size = 32 ---\n",
            "[ERROR] BS=32 failed: CUDA out of memory. Tried to allocate 276.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 35.38 MiB is free. Process 508161 has 22.12 GiB memory in use. Of the allocated memory 19.18 GiB is allocated by PyTorch, and 2.72 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n",
            "Method 1 Batch Results (INT4 + Tensor-wise Prune + KV Quant):\n",
            "  BS=4: Latency=35.214s, Throughput=78.60 tok/s, Memory=8.78 GB\n",
            "  BS=8: Latency=38.420s, Throughput=144.09 tok/s, Memory=13.24 GB\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python int4_channel_prune_quant_batch.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mXPS6YqdBrP4",
        "outputId": "5d6e506c-25b4-485a-fb63-04d2b904b419"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-29 20:21:33.255872: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-29 20:21:33.273460: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764447693.295113   40954 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764447693.301517   40954 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764447693.317776   40954 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764447693.317807   40954 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764447693.317811   40954 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764447693.317813   40954 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-29 20:21:33.322579: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\n",
            "Using unstructure for Pruning:\n",
            "================================================================================\n",
            "Method 2: INT4 + Channel-wise KV Pruning + KV Quantization (Batch Test)\n",
            "================================================================================\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "Loading checkpoint shards: 100% 3/3 [00:16<00:00,  5.57s/it]\n",
            "INT4 model loaded (bitsandbytes).\n",
            "\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "\n",
            "==============================\n",
            "  Batch Size Stress Test (channelwise)\n",
            "==============================\n",
            "\n",
            "--- Batch Size = 4 ---\n",
            " ==> BS=4, latency=20.359s, throughput=127.71 tok/s, max_mem=8.71 GB\n",
            "\n",
            "--- Batch Size = 8 ---\n",
            " ==> BS=8, latency=22.041s, throughput=235.92 tok/s, max_mem=13.10 GB\n",
            "\n",
            "--- Batch Size = 16 ---\n",
            "[ERROR] BS=16 failed: CUDA out of memory. Tried to allocate 86.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 15.38 MiB is free. Process 594687 has 22.14 GiB memory in use. Of the allocated memory 21.10 GiB is allocated by PyTorch, and 831.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n",
            "--- Batch Size = 32 ---\n",
            "[ERROR] BS=32 failed: CUDA out of memory. Tried to allocate 298.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 91.38 MiB is free. Process 594687 has 22.07 GiB memory in use. Of the allocated memory 20.16 GiB is allocated by PyTorch, and 1.67 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n",
            "Method 2 Batch Results (INT4 + Channel-wise Prune + KV Quant):\n",
            "  BS=4: Latency=20.359s, Throughput=127.71 tok/s, Memory=8.71 GB\n",
            "  BS=8: Latency=22.041s, Throughput=235.92 tok/s, Memory=13.10 GB\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python int8_channel_prune_quant_batch.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LEvKrie0E-e2",
        "outputId": "1412c7ea-ee3d-47bf-f3b5-4ec0f20bf19a"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-29 20:23:58.864033: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-29 20:23:58.881592: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764447838.902902   41659 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764447838.909355   41659 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764447838.925952   41659 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764447838.925984   41659 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764447838.925987   41659 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764447838.925990   41659 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-29 20:23:58.930898: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\n",
            "Using unstructure for Pruning:\n",
            "================================================================================\n",
            "Method 2: INT8 + Channel-wise KV Pruning + KV Quantization (Batch Test)\n",
            "================================================================================\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "Loading checkpoint shards: 100% 3/3 [00:16<00:00,  5.63s/it]\n",
            "INT8 model loaded (bitsandbytes).\n",
            "\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "\n",
            "==============================\n",
            "  Batch Size Stress Test (channelwise)\n",
            "==============================\n",
            "\n",
            "--- Batch Size = 4 ---\n",
            " ==> BS=4, latency=17.437s, throughput=158.74 tok/s, max_mem=11.36 GB\n",
            "\n",
            "--- Batch Size = 8 ---\n",
            " ==> BS=8, latency=19.093s, throughput=289.94 tok/s, max_mem=15.86 GB\n",
            "\n",
            "--- Batch Size = 16 ---\n",
            "[ERROR] BS=16 failed: CUDA out of memory. Tried to allocate 76.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 27.38 MiB is free. Process 604800 has 22.13 GiB memory in use. Of the allocated memory 20.84 GiB is allocated by PyTorch, and 1.06 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n",
            "--- Batch Size = 32 ---\n",
            "[ERROR] BS=32 failed: CUDA out of memory. Tried to allocate 298.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 157.38 MiB is free. Process 604800 has 22.00 GiB memory in use. Of the allocated memory 19.17 GiB is allocated by PyTorch, and 2.59 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n",
            "Method 2 Batch Results (INT8 + Channel-wise Prune + KV Quant):\n",
            "  BS=4: Latency=17.437s, Throughput=158.74 tok/s, Memory=11.36 GB\n",
            "  BS=8: Latency=19.093s, Throughput=289.94 tok/s, Memory=15.86 GB\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python channel_prune_quant_batch.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UEiVgKw3FtwW",
        "outputId": "cb6121e9-027f-4e56-a0fd-468d32835b60"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-29 20:25:37.708798: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-29 20:25:37.727046: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764447937.750537   42178 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764447937.757233   42178 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764447937.773945   42178 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764447937.773976   42178 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764447937.773979   42178 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764447937.773982   42178 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-29 20:25:37.778858: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\n",
            "Using unstructure for Pruning:\n",
            "Loading sample image...\n",
            "Image size: (1000, 667)\n",
            "\n",
            "================================================================================\n",
            "Method 2: Channel-wise KV Pruning + KV Quantization (Batch Test)\n",
            "================================================================================\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Loading checkpoint shards: 100% 3/3 [00:04<00:00,  1.43s/it]\n",
            "LLava fp16 model loaded.\n",
            "\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "\n",
            "==============================\n",
            "  Batch Size Stress Test (channelwise)\n",
            "==============================\n",
            "\n",
            "--- Batch Size = 4 ---\n",
            " ==> BS=4, latency=7.660s, throughput=361.35 tok/s, max_mem=17.67 GB\n",
            "\n",
            "--- Batch Size = 8 ---\n",
            "[ERROR] BS=8 failed: CUDA out of memory. Tried to allocate 38.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 33.38 MiB is free. Process 612402 has 22.12 GiB memory in use. Of the allocated memory 21.60 GiB is allocated by PyTorch, and 303.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n",
            "--- Batch Size = 16 ---\n",
            "[ERROR] BS=16 failed: CUDA out of memory. Tried to allocate 150.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 125.38 MiB is free. Process 612402 has 22.03 GiB memory in use. Of the allocated memory 20.05 GiB is allocated by PyTorch, and 1.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n",
            "--- Batch Size = 32 ---\n",
            "[ERROR] BS=32 failed: CUDA out of memory. Tried to allocate 400.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 75.38 MiB is free. Process 612402 has 22.08 GiB memory in use. Of the allocated memory 19.63 GiB is allocated by PyTorch, and 2.23 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n",
            "Method 2 Batch Results (Channel-wise Prune + KV Quant):\n",
            "  BS=4: Latency=7.660s, Throughput=361.35 tok/s, Memory=17.67 GB\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python test_bitsandbytes_batch.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lK48zSaRS74Q",
        "outputId": "389d43cf-b97a-4325-9457-3a50be4eac77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-29 20:36:02.108930: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-29 20:36:02.127078: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764448562.149085   45122 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764448562.155670   45122 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764448562.172461   45122 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764448562.172516   45122 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764448562.172520   45122 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764448562.172523   45122 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-29 20:36:02.177401: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Loading sample image...\n",
            "Image size: (1000, 667)\n",
            "\n",
            "================================================================================\n",
            "8bit batch Test for LLaVA\n",
            "================================================================================\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "Loading checkpoint shards: 100% 3/3 [00:16<00:00,  5.64s/it]\n",
            "Model loaded.\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "Loaded: https://llava-vl.github.io/static/images/view.jpg\n",
            "\n",
            "==============================\n",
            "  Batch Size Stress Test (true batch forward)\n",
            "==============================\n",
            "\n",
            "--- Batch Size = 4 (1 forward pass with 4 images) ---\n",
            " ==> BS=4: latency=17.827s, throughput=155.72 tok/s, max mem=8.32 GB\n",
            "\n",
            "--- Batch Size = 8 (1 forward pass with 8 images) ---\n",
            " ==> BS=8: latency=20.289s, throughput=273.64 tok/s, max mem=9.78 GB\n",
            "\n",
            "--- Batch Size = 16 (1 forward pass with 16 images) ---\n",
            " ==> BS=16: latency=25.494s, throughput=435.56 tok/s, max mem=12.71 GB\n",
            "\n",
            "--- Batch Size = 32 (1 forward pass with 32 images) ---\n",
            " ==> BS=32: latency=36.260s, throughput=612.47 tok/s, max mem=18.56 GB\n",
            "\"8bit Batch Results:\n",
            "  BS=4: Latency=17.827s, Throughput=155.72 tok/s, Memory=8.32 GB\n",
            "  BS=8: Latency=20.289s, Throughput=273.64 tok/s, Memory=9.78 GB\n",
            "  BS=16: Latency=25.494s, Throughput=435.56 tok/s, Memory=12.71 GB\n",
            "  BS=32: Latency=36.260s, Throughput=612.47 tok/s, Memory=18.56 GB\n"
          ]
        }
      ]
    }
  ]
}