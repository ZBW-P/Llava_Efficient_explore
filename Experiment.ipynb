{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "runtime_attributes": {
        "runtime_version": "2025.10"
      }
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers accelerate pillow torch\n",
        "!pip install -q flash-attn --no-build-isolation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCg-P6EKcHuQ",
        "outputId": "4b76b510-7bc0-4c20-94f4-eabef18b1f65"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/8.4 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m7.3/8.4 MB\u001b[0m \u001b[31m233.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m139.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for flash-attn (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "qBUl_sgXxkaU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llmcompressor datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I6VrjHIVYFyZ",
        "outputId": "625b9b28-3ec1-4321-f2d0-95fb1675bcba"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llmcompressor\n",
            "  Downloading llmcompressor-0.8.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Collecting loguru<=0.7.3,>=0.7.2 (from llmcompressor)\n",
            "  Downloading loguru-0.7.3-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: pyyaml<=6.0.3,>=6.0.1 in /usr/local/lib/python3.12/dist-packages (from llmcompressor) (6.0.3)\n",
            "Requirement already satisfied: numpy<=2.3.3,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from llmcompressor) (2.0.2)\n",
            "Requirement already satisfied: requests<=2.32.5,>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from llmcompressor) (2.32.4)\n",
            "Requirement already satisfied: tqdm<=4.67.1,>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from llmcompressor) (4.67.1)\n",
            "Requirement already satisfied: torch<=2.8.0,>=2.7.0 in /usr/local/lib/python3.12/dist-packages (from llmcompressor) (2.8.0+cu126)\n",
            "Collecting transformers<=4.56.2,>=4.53.0 (from llmcompressor)\n",
            "  Downloading transformers-4.56.2-py3-none-any.whl.metadata (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.1/40.1 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: accelerate<=1.10.1,>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from llmcompressor) (1.10.1)\n",
            "Requirement already satisfied: nvidia-ml-py<=13.580.82,>=12.560.30 in /usr/local/lib/python3.12/dist-packages (from llmcompressor) (12.575.51)\n",
            "Requirement already satisfied: pillow<=11.3.0,>=10.4.0 in /usr/local/lib/python3.12/dist-packages (from llmcompressor) (11.3.0)\n",
            "Collecting compressed-tensors==0.12.2 (from llmcompressor)\n",
            "  Downloading compressed_tensors-0.12.2-py3-none-any.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.12/dist-packages (from compressed-tensors==0.12.2->llmcompressor) (2.11.10)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.35.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate<=1.10.1,>=1.6.0->llmcompressor) (5.9.5)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate<=1.10.1,>=1.6.0->llmcompressor) (0.6.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.10)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<=2.32.5,>=2.32.2->llmcompressor) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<=2.32.5,>=2.32.2->llmcompressor) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<=2.32.5,>=2.32.2->llmcompressor) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<=2.32.5,>=2.32.2->llmcompressor) (2025.10.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch<=2.8.0,>=2.7.0->llmcompressor) (3.4.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<=4.56.2,>=4.53.0->llmcompressor) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<=4.56.2,>=4.53.0->llmcompressor) (0.22.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0->compressed-tensors==0.12.2->llmcompressor) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0->compressed-tensors==0.12.2->llmcompressor) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0->compressed-tensors==0.12.2->llmcompressor) (0.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<=2.8.0,>=2.7.0->llmcompressor) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<=2.8.0,>=2.7.0->llmcompressor) (3.0.3)\n",
            "Downloading llmcompressor-0.8.1-py3-none-any.whl (273 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m273.2/273.2 kB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading compressed_tensors-0.12.2-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.0/183.0 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading loguru-0.7.3-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformers-4.56.2-py3-none-any.whl (11.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m113.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: loguru, transformers, compressed-tensors, llmcompressor\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.57.0\n",
            "    Uninstalling transformers-4.57.0:\n",
            "      Successfully uninstalled transformers-4.57.0\n",
            "Successfully installed compressed-tensors-0.12.2 llmcompressor-0.8.1 loguru-0.7.3 transformers-4.56.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U bitsandbytes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_tuG642YIRs",
        "outputId": "95a83522-10ff-4bde-a4d1-6a1b279cb8a1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: torch<3,>=2.3 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.8.0+cu126)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (25.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.3->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.3)\n",
            "Downloading bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl (59.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m44.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bitsandbytes\n",
            "Successfully installed bitsandbytes-0.48.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6RPnK-KkbI1D",
        "outputId": "b222150c-bf7b-4a48-a2c4-d0b5f561e621"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.12.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python test_baseline.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZV_YISLiWAr0",
        "outputId": "c67870cf-058b-459e-d84f-42f779d74fbe"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-28 04:45:36.594803: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-28 04:45:36.611840: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764305136.633080    1764 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764305136.639397    1764 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764305136.655493    1764 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764305136.655518    1764 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764305136.655521    1764 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764305136.655524    1764 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-28 04:45:36.660195: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Loading sample image...\n",
            "Image size: (1000, 667)\n",
            "\n",
            "================================================================================\n",
            "Baseline_with_kv_cache Test for LLaVA\n",
            "================================================================================\n",
            "processor_config.json: 100% 173/173 [00:00<00:00, 1.66MB/s]\n",
            "chat_template.json: 100% 701/701 [00:00<00:00, 7.28MB/s]\n",
            "chat_template.jinja: 100% 674/674 [00:00<00:00, 7.18MB/s]\n",
            "preprocessor_config.json: 100% 505/505 [00:00<00:00, 5.76MB/s]\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "tokenizer_config.json: 1.45kB [00:00, 7.58MB/s]\n",
            "tokenizer.model: 100% 500k/500k [00:01<00:00, 285kB/s]\n",
            "tokenizer.json: 3.62MB [00:00, 185MB/s]\n",
            "added_tokens.json: 100% 41.0/41.0 [00:00<00:00, 520kB/s]\n",
            "special_tokens_map.json: 100% 552/552 [00:00<00:00, 5.42MB/s]\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "config.json: 100% 950/950 [00:00<00:00, 11.1MB/s]\n",
            "model.safetensors.index.json: 70.1kB [00:00, 196MB/s]\n",
            "Fetching 3 files:   0% 0/3 [00:00<?, ?it/s]\n",
            "model-00001-of-00003.safetensors:   0% 0.00/4.99G [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:   0% 0.00/4.96G [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:   0% 0.00/4.18G [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:   0% 24.6k/4.99G [00:01<67:13:45, 20.6kB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:   0% 599k/4.96G [00:01<4:04:17, 338kB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:   0% 119k/4.99G [00:01<18:51:07, 73.6kB/s] \u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:   0% 546k/4.18G [00:02<4:21:57, 266kB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:   5% 239M/4.99G [00:02<00:24, 191MB/s]    \u001b[A\n",
            "model-00001-of-00003.safetensors:   6% 306M/4.99G [00:04<01:08, 68.1MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:   8% 412M/4.99G [00:04<00:44, 102MB/s] \u001b[A\n",
            "model-00001-of-00003.safetensors:  14% 677M/4.99G [00:05<00:21, 201MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:   1% 67.7M/4.96G [00:05<05:55, 13.8MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  16% 811M/4.99G [00:05<00:20, 204MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:   3% 135M/4.96G [00:08<04:41, 17.2MB/s] \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:   2% 67.6M/4.18G [00:12<12:27, 5.50MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:   3% 135M/4.18G [00:13<05:08, 13.1MB/s] \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:   5% 269M/4.96G [00:13<03:23, 23.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:   6% 269M/4.18G [00:13<01:57, 33.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:   8% 403M/4.96G [00:13<01:46, 42.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:   8% 336M/4.18G [00:13<01:21, 47.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:   9% 470M/4.96G [00:13<01:22, 54.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  13% 537M/4.18G [00:13<00:36, 101MB/s] \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  14% 604M/4.18G [00:14<00:34, 103MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  12% 604M/4.96G [00:14<00:57, 75.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  18% 738M/4.18G [00:14<00:25, 134MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  18% 880M/4.99G [00:14<01:52, 36.6MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  19% 805M/4.18G [00:15<00:24, 140MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  19% 941M/4.99G [00:15<01:35, 42.6MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  15% 738M/4.96G [00:15<00:44, 94.2MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  20% 1.01G/4.99G [00:16<01:33, 42.8MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  16% 805M/4.96G [00:19<01:26, 48.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  22% 940M/4.18G [00:19<00:51, 63.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  22% 1.08G/4.99G [00:19<01:45, 37.3MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  19% 940M/4.96G [00:19<00:53, 75.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  24% 1.01G/4.18G [00:19<00:40, 77.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  20% 1.01G/4.96G [00:19<00:42, 92.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  27% 1.14G/4.18G [00:19<00:25, 118MB/s] \u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  23% 1.14G/4.99G [00:19<01:25, 45.2MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  22% 1.07G/4.96G [00:19<00:38, 100MB/s] \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  29% 1.21G/4.18G [00:20<00:26, 111MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  23% 1.14G/4.96G [00:20<00:39, 96.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  32% 1.34G/4.18G [00:20<00:19, 145MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  34% 1.41G/4.18G [00:21<00:21, 127MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  26% 1.27G/4.96G [00:21<00:32, 113MB/s] \u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  26% 1.28G/4.99G [00:21<01:10, 53.0MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  27% 1.34G/4.96G [00:22<00:31, 115MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  27% 1.34G/4.99G [00:25<01:34, 38.5MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  30% 1.48G/4.99G [00:26<01:09, 50.6MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  28% 1.41G/4.96G [00:26<01:21, 43.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  31% 1.54G/4.96G [00:27<00:52, 65.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  35% 1.48G/4.18G [00:27<01:09, 38.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  39% 1.61G/4.18G [00:27<00:44, 57.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  34% 1.68G/4.96G [00:28<00:44, 73.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  37% 1.81G/4.96G [00:30<00:39, 79.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  43% 1.81G/4.18G [00:30<00:34, 68.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  31% 1.54G/4.99G [00:30<01:37, 35.2MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  38% 1.88G/4.96G [00:30<00:34, 88.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  45% 1.88G/4.18G [00:31<00:33, 69.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  33% 1.67G/4.99G [00:31<01:05, 50.7MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  35% 1.74G/4.99G [00:32<01:04, 50.9MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  39% 1.94G/4.99G [00:35<00:53, 57.5MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  39% 1.95G/4.96G [00:35<01:16, 39.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  48% 2.01G/4.18G [00:35<00:45, 47.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  51% 2.15G/4.18G [00:35<00:29, 68.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  42% 2.08G/4.99G [00:35<00:36, 79.2MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  42% 2.08G/4.96G [00:35<00:47, 61.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  45% 2.21G/4.96G [00:36<00:32, 83.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  53% 2.21G/4.18G [00:36<00:26, 73.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  43% 2.15G/4.99G [00:39<00:56, 50.3MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  46% 2.28G/4.96G [00:40<00:55, 48.1MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  44% 2.21G/4.99G [00:40<00:50, 55.0MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  56% 2.35G/4.18G [00:40<00:34, 53.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  48% 2.42G/4.99G [00:40<00:25, 101MB/s] \u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  47% 2.35G/4.96G [00:40<00:44, 59.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  58% 2.42G/4.18G [00:41<00:35, 50.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  50% 2.48G/4.99G [00:42<00:35, 71.6MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  49% 2.42G/4.96G [00:42<00:52, 48.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  61% 2.55G/4.18G [00:42<00:23, 68.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  50% 2.48G/4.96G [00:43<00:49, 50.5MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  51% 2.55G/4.99G [00:44<00:39, 61.2MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  51% 2.55G/4.96G [00:44<00:39, 60.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  63% 2.62G/4.18G [00:44<00:26, 59.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  54% 2.68G/4.99G [00:44<00:24, 93.4MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  66% 2.75G/4.18G [00:44<00:15, 89.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  56% 2.78G/4.99G [00:44<00:20, 110MB/s] \u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  53% 2.62G/4.96G [00:45<00:36, 64.1MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  57% 2.85G/4.99G [00:45<00:18, 117MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  67% 2.82G/4.18G [00:45<00:16, 80.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  54% 2.68G/4.96G [00:45<00:34, 66.4MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  58% 2.92G/4.99G [00:46<00:20, 99.0MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  57% 2.82G/4.96G [00:47<00:29, 72.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  71% 2.95G/4.18G [00:51<00:28, 43.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  74% 3.09G/4.18G [00:51<00:18, 57.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  60% 2.98G/4.99G [00:52<00:58, 34.3MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  60% 2.95G/4.96G [00:52<00:44, 45.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  77% 3.22G/4.18G [00:52<00:12, 78.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  61% 3.05G/4.99G [00:52<00:45, 42.8MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  64% 3.18G/4.99G [00:52<00:25, 71.9MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  61% 3.02G/4.96G [00:52<00:35, 53.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  64% 3.15G/4.96G [00:52<00:22, 81.9MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  65% 3.25G/4.99G [00:53<00:22, 78.5MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  65% 3.22G/4.96G [00:53<00:18, 94.3MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  68% 3.38G/4.99G [00:53<00:13, 117MB/s] \u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  79% 3.29G/4.18G [00:53<00:12, 73.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  66% 3.29G/4.96G [00:54<00:19, 85.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  80% 3.35G/4.18G [00:54<00:11, 69.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  69% 3.42G/4.96G [00:54<00:12, 120MB/s] \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  82% 3.42G/4.18G [00:54<00:08, 86.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  69% 3.45G/4.99G [00:55<00:21, 71.5MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  85% 3.55G/4.18G [00:59<00:13, 45.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  70% 3.52G/4.99G [01:00<00:41, 35.5MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  70% 3.49G/4.96G [01:01<00:40, 36.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  87% 3.62G/4.18G [01:01<00:11, 46.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  72% 3.59G/4.99G [01:01<00:33, 42.4MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  73% 3.62G/4.96G [01:01<00:24, 54.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  90% 3.76G/4.18G [01:01<00:06, 69.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  73% 3.65G/4.99G [01:01<00:26, 51.3MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  93% 3.89G/4.18G [01:01<00:02, 100MB/s] \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  76% 3.76G/4.96G [01:02<00:15, 78.5MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  75% 3.72G/4.99G [01:02<00:19, 66.0MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  76% 3.79G/4.99G [01:03<00:20, 60.2MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  77% 3.85G/4.99G [01:05<00:20, 54.5MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  95% 3.96G/4.18G [01:05<00:03, 56.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  77% 3.82G/4.96G [01:05<00:23, 48.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  96% 4.02G/4.18G [01:06<00:02, 59.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  79% 3.92G/4.99G [01:06<00:19, 55.2MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  78% 3.89G/4.96G [01:06<00:20, 51.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  80% 3.96G/4.96G [01:06<00:16, 61.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  97% 4.04G/4.18G [01:07<00:02, 48.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  81% 4.03G/4.96G [01:08<00:16, 55.7MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  81% 4.05G/4.99G [01:08<00:17, 52.2MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00003-of-00003.safetensors:  98% 4.11G/4.18G [01:09<00:01, 44.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00003-of-00003.safetensors: 100% 4.18G/4.18G [01:09<00:00, 60.4MB/s]\n",
            "\n",
            "\n",
            "model-00002-of-00003.safetensors:  83% 4.09G/4.96G [01:09<00:14, 61.5MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  85% 4.25G/4.99G [01:09<00:06, 107MB/s] \u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  85% 4.23G/4.96G [01:09<00:07, 99.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  87% 4.29G/4.96G [01:09<00:05, 122MB/s] \u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  88% 4.39G/4.99G [01:09<00:04, 136MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  88% 4.36G/4.96G [01:09<00:04, 136MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  89% 4.43G/4.96G [01:10<00:03, 152MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  89% 4.46G/4.99G [01:10<00:04, 132MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  91% 4.52G/4.99G [01:10<00:02, 158MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  91% 4.50G/4.96G [01:10<00:02, 156MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  92% 4.59G/4.99G [01:10<00:02, 157MB/s]\u001b[A\n",
            "model-00001-of-00003.safetensors:  93% 4.66G/4.99G [01:11<00:01, 189MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  92% 4.56G/4.96G [01:11<00:03, 118MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  95% 4.72G/4.99G [01:12<00:03, 87.3MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  93% 4.63G/4.96G [01:13<00:04, 77.9MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  96% 4.79G/4.99G [01:13<00:02, 94.5MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  95% 4.69G/4.96G [01:13<00:03, 88.7MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  97% 4.86G/4.99G [01:13<00:01, 109MB/s] \u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  96% 4.76G/4.96G [01:13<00:01, 107MB/s] \u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors:  99% 4.93G/4.99G [01:14<00:00, 128MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  97% 4.82G/4.96G [01:14<00:01, 127MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00003.safetensors:  99% 4.89G/4.96G [01:14<00:00, 143MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00003.safetensors: 100% 4.99G/4.99G [01:14<00:00, 67.0MB/s]\n",
            "Fetching 3 files:  33% 1/3 [01:15<02:30, 75.04s/it]\n",
            "\n",
            "model-00002-of-00003.safetensors: 100% 4.96G/4.96G [01:14<00:00, 66.2MB/s]\n",
            "Fetching 3 files: 100% 3/3 [01:15<00:00, 25.18s/it]\n",
            "Loading checkpoint shards: 100% 3/3 [00:04<00:00,  1.43s/it]\n",
            "generation_config.json: 100% 141/141 [00:00<00:00, 1.61MB/s]\n",
            "Model loaded.\n",
            "Running 3 inference passes...\n",
            " Run 1/3: 11.50 tokens/sec, 13.52 GB\n",
            " Run 2/3: 16.51 tokens/sec, 13.52 GB\n",
            " Run 3/3: 16.49 tokens/sec, 13.52 GB\n",
            "\n",
            "kv_cache_results Results:\n",
            "  Average Latency: 6.940 seconds\n",
            "  Average Throughput: 14.83 tokens/sec\n",
            "  Average Memory: 13.52 GB\n",
            "\n",
            "Sample Output:\n",
            "USER:  \n",
            "Describe this image in detail.\n",
            "ASSISTANT: The image features a pier extending out into a large body of water, likely a lake. The pier is made of wood and has a bench situated on it, providing ...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python test_kv_prune_quant.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H55M7Pi8yexw",
        "outputId": "62cdc10a-8021-443f-b413-d1435d11572a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-28 05:13:39.970014: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-28 05:13:39.987379: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764306820.009165    9258 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764306820.015633    9258 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764306820.031725    9258 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764306820.031754    9258 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764306820.031756    9258 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764306820.031759    9258 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-28 05:13:40.036494: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Loading sample image...\n",
            "Image size: (1000, 667)\n",
            "\n",
            "================================================================================\n",
            "Prun + Quantization KV_cache Test for LLaVA\n",
            "================================================================================\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Loading checkpoint shards: 100% 3/3 [00:04<00:00,  1.42s/it]\n",
            "Model loaded.\n",
            "Running 3 inference passes...\n",
            " Run 1/3: 202.79 tokens/sec, 14.26 GB\n",
            " Run 2/3: 205.27 tokens/sec, 14.26 GB\n",
            " Run 3/3: 204.80 tokens/sec, 14.26 GB\n",
            "\n",
            "Prun + Quant_kv_cache Results:\n",
            "  Average Latency: 3.148 seconds\n",
            "  Average Throughput: 204.29 tokens/sec\n",
            "  Average Memory: 14.26 GB\n",
            "\n",
            "Sample Output:\n",
            "USER:  \n",
            "Describe this image in detail.\n",
            "ASSISTANT:The image shows a pier extending out into the water. The pier is situated on the water's edge, and it is a long pier that extends out into the water. T...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python test_kv_prune.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YPBK2sapy0zb",
        "outputId": "462b4f20-9fa3-4fc6-e750-7bbf48fc2521"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-28 04:49:39.905786: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-28 04:49:39.923555: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764305379.945223    3001 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764305379.951672    3001 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764305379.968118    3001 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764305379.968152    3001 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764305379.968155    3001 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764305379.968158    3001 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-28 04:49:39.973023: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Loading sample image...\n",
            "Image size: (1000, 667)\n",
            "\n",
            "================================================================================\n",
            "Prun KV_cache Test for LLaVA\n",
            "================================================================================\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Loading checkpoint shards: 100% 3/3 [00:04<00:00,  1.42s/it]\n",
            "Model loaded.\n",
            "Running 3 inference passes...\n",
            " Run 1/3: 200.50 tokens/sec, 14.11 GB\n",
            " Run 2/3: 204.95 tokens/sec, 14.11 GB\n",
            " Run 3/3: 205.20 tokens/sec, 14.11 GB\n",
            "\n",
            "Prun_kv_cacheresults Results:\n",
            "  Average Latency: 3.159 seconds\n",
            "  Average Throughput: 203.55 tokens/sec\n",
            "  Average Memory: 14.11 GB\n",
            "\n",
            "Sample Output:\n",
            "USER:  \n",
            "Describe this image in detail.\n",
            "ASSISTANT:The image shows a pier extending out into the water. The pier is situated on the water's edge, and it is a long pier that extends out into the water. T...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python test_kv_quant.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afVotDivy6tQ",
        "outputId": "8e1953e9-379d-4ce7-a267-08b63efd8e4d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-28 05:15:54.006989: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-28 05:15:54.024089: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764306954.046074    9861 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764306954.052725    9861 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764306954.069526    9861 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764306954.069558    9861 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764306954.069563    9861 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764306954.069567    9861 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-28 05:15:54.074279: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Loading sample image...\n",
            "Image size: (1000, 667)\n",
            "\n",
            "================================================================================\n",
            "Quantization KV_cache Test for LLaVA\n",
            "================================================================================\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Loading checkpoint shards: 100% 3/3 [00:04<00:00,  1.41s/it]\n",
            "Model loaded.\n",
            "Running 3 inference passes...\n",
            " Run 1/3: 106.46 tokens/sec, 14.28 GB\n",
            " Run 2/3: 108.11 tokens/sec, 14.28 GB\n",
            " Run 3/3: 108.13 tokens/sec, 14.28 GB\n",
            "\n",
            "Quant_kv_cache Results:\n",
            "  Average Latency: 6.452 seconds\n",
            "  Average Throughput: 107.56 tokens/sec\n",
            "  Average Memory: 14.28 GB\n",
            "\n",
            "Sample Output:\n",
            "USER:  \n",
            "Describe this image in detail.\n",
            "ASSISTANT: The image features a pier with a wooden walkway extending out into the water. The pier is situated in front of a beautiful mountain range, creating a ...\n",
            "\n"
          ]
        }
      ]
    }
  ]
}